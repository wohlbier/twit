===============================================================================
Title
-------------------------------------------------------------------------------
Identifying Actor Characteristics in State-Linked Information
Operations Using Twitter Data and Graph Based Neural Networks
===============================================================================
Keywords (at least three)
-------------------------------------------------------------------------------
Information operations,
Graph neural networks,
GraphSAINT,
GraphSAGE,
SIGN,
Twitter
===============================================================================
About your team. * Please, describe your research group and its availability
to work collaboratively with the Neocortex team during the 30-day Early User
Program (EUP). Please, include who would be the main point of contact for this
collaboration (name, email address).
-------------------------------------------------------------------------------

Point of contact: Dr. John G. Wohlbier, jgwohlbier@sei.cmu.edu

The Advanced Computing group in the Carnegie Mellon University, Software
Engineering Institute (SEI), Emerging Technology Center has extensive
experience at the intersection of data intensive AI/ML computing and
emerging hardware. The team serves as trusted government advisors for the DARPA
Software Defined Hardware (SDH) and Domain Specific System on Chip (DSSoC)
programs. Team members understand, and can exploit very low level performance
characteristics, such as those that will be required to have success on
Neocortex.

https://www.linkedin.com/in/john-wohlbier-52a6b5a1/
Dr. John Wohlbier is a Senior Research Scientist at the SEI. He has over 15
years of experience in HPC and is the technical lead for the SEI's support to
DARPA SDH and DSSoC.

https://www.linkedin.com/in/scott-m-3785751/
Dr. Scott McMillan is a Principal Member of the Research Staff at the SEI.
He has over two decades of experience in HPC, and is an active member of the
GraphBLAS API specification committee.

https://www.linkedin.com/in/annika-horgan-40b935162/
Annika Horgan is an Associate Software Developer at the SEI. She has
considerable experience working with schedulers for advanced hardware, and
provides data analytics expertise.

The team will be available to work with the Neocortex team during the EUP.
Wohlbier has an existing working relationship with Dr. Natalia Vassilieva
at Cerebras.

===============================================================================
About your research. * Please, provide a concise description of your ML/AI
use case, its scientific motivation, and impact. What do you hope to
accomplish during the 30-day EUP? See the Neocortex EUP webpage for examples
(https://www.cmu.edu/psc/aibd/neocortex/early-user-program.html).
-------------------------------------------------------------------------------
Since 2018 Twitter has been releasing data comprised of tweets and user
profiles from accounts identified as being associated with state-backed
information operations
[https://transparency.twitter.com/en/reports/information-operations.html].
The data is released to the public in hopes that the research community can
use it to "build the necessary societal defenses and capacities to protect
public conversation." Graph Neural Networks (GNNs) have proven powerful in
recent years in combining AI/ML techniques applied to node attributes with
underlying graph structure to perform node classification, link prediction,
and community detection. We will study state actor characteristics using a
myriad of GNNs as applied to retweet graphs constructed from twitter data.
Our characterizations will be shared with the community to advance the
state of the art in understanding of information operations.

We have assembled a collection of models using some of the most popular
GNNs and applied it to the June 2020 data release which includes over
32,000 users, 160,000 retweets, and 120,000,000 tweets. We will use Neocortex
to apply our models to ALL of the existing Twitter information operations
datasets, and expect to run the models in a timely manner. Our models include
both those that require multiple instances of the full dense graph adjacency
matrix resident in memory, such as Scalable Inception Graph Neural Networks
(SIGN), as well as those that follow a graph sampling paradigm, such as
GraphSAGE and GraphSAINT.

===============================================================================
Related publications. * Please, list any recent publications that motivate or
describe the work you plan to execute on Neocortex.
-------------------------------------------------------------------------------
Data:
https://transparency.twitter.com/en/reports/information-operations.html

Graph Neural Networks:

SIGN
Rossi, E., Frasca, F., Chamberlain, B., Eynard, D., Bronstein, M., & Monti, F. (2020). SIGN: Scalable Inception Graph Neural Networks. arXiv preprint arXiv:2004.11198.
https://arxiv.org/pdf/2004.11198

GraphSAGE
Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning on large graphs. In Advances in neural information processing systems (pp. 1024-1034).
https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf

GraphSAINT
Zeng, H., Zhou, H., Srivastava, A., Kannan, R., & Prasanna, V. (2019). Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931.
https://arxiv.org/pdf/1907.04931

===============================================================================
About your ML model. * Please, provide a technical description of your neural
network architecture and optimization algorithm. If you are using a published
model+algorithm, please indicate the model, citation, and repo (if available).
If not a published one, then list the #parameters, #layers, layer types,
numerical precision, etc. If possible, please provide a reference to a known
proxy/representative model, code (e.g. TensorFlow code), or repo link
(e.g. Github).
-------------------------------------------------------------------------------
The primary models we will use are SIGN, GraphSAGE, and GraphSAINT, as
mentioned above. Our work to date has been implemented using PyTorch
Geometric https://github.com/rusty1s/pytorch_geometric. Our implementations
are available at https://github.com/wohlbier/twit, which includes code for
raw data processing and feature extraction.

The number of layers and parameters depends on the GNN. Typical GNNs have
only two or three layers, which can be confirmed in our code. The way GNNs
exploit graph structure is through a single or multi-hop neighbor aggregation
mechanism. Sampling based GNNs such as GraphSAGE and GraphSAINT use randomly
sampled subsets of neighbors, and neighbors of neighbors, to avoid oftentimes
exponential explosion of attribute aggregation. It is suspected that inclusion
of additional neighbors during sampling can improve model fidelity, but most
often it is not done for reasons of computational cost. On Neocortex we will
study how large we can make neighbor samples, and the impact it has on
fidelity and runtime.

The nature of the SIGN architecture is such that the memory requirements
grow rapidly with a modest increase in an individual parameter value, while
implementations appearing in the literature use a small a small value for
this parameter. For example, in the inception like formulation of SIGN
the parameter $r$ appears as an exponent on the adjacency matrix. An adjacency
matrix raised to several powers will quickly become dense and multiple copies
of the dense  adjacency matrix would need to be stored in memory. On Neocortex
we will explore the practical limits of the SIGN architecture and how model
fidelity will benefit from increasing parameters.

===============================================================================
About your data. * Please, describe your data by indicating type (images,
video, sensor, text, etc), dimensions of a single training sample (e.g. image
dimensions and number of channels for images, sequence length for sequence
data), and dataset volume (number of training samples, volume in MBs/GBs/TBs).
Please indicate if your data requires time-consuming preprocessing and mention
any other consideration(s) you consider relevant.
-------------------------------------------------------------------------------
Our data consists of user information and tweets released by Twitter over the
course of two years, as described here.
https://transparency.twitter.com/en/reports/information-operations.html
The releases contain users, tweets, and the media associated with the tweets.
Tweets and users are stored in ASCII csv files. User and tweet data are used
to build features and to construct graphs. For the purposes of this work we
will focus only on the user and tweet data, and not use the media data. The
most recent data release included:
China: 23,750 accounts,     427,903 tweets
Russia: 1,152 accounts,   4,373,218 tweets
Turkey: 7,340 accounts, 120,253,807 tweets
We constructed a retweet graph from the combined data and it results in
161,076 edges. This is a directed graph as users need not retweet each other.
While this graph is of a modest size, it is expected to grow significantly
when we include data from all previous releases. Moreover, it is expected
that information operations are accelerating, and that Twitter will continue to
release new data every few months. For details of the sizes on past data
releases see https://github.com/wohlbier/twit/blob/master/README.md.

Because we are excluding the media files, data volumes are modest and do
not add up to much more than tens of gigabytes. In future work if we were
to include media into our models, then data volumes would quickly rise to
tens of terabytes.

Extracting features from the data can take considerable time, but we expect
to have most of the feature extraction work done ahead of time.

===============================================================================
Performance. * Please, share the training or inference performance you are
currently seeing. What infrastructure are you using now (e.g. # GPU nodes and
type; on-premise or cloud)? Have you scaled your model to multiple GPUs,
processors, and/or nodes? What are the limitations or bottlenecks you would
like to address with Neocortex, and is there a specific level of performance
you would like to achieve?
-------------------------------------------------------------------------------
With our current model parameters, training and inference for the models we
plan to run on Neocortex can take up to several hours on an older model, high
end dual socket Xeon Broadwell node. However, on Neocortex we plan to explore
parameter spaces with deeper neighbor sampling that would cause CPU
implementations to grind to a halt. The performance characteristics of GNNs
are a mixture of sparse and dense operations. The fully connected
layers of the neural network are dense, structured, single precision floating
point operations with predictable behavior. On conventional architectures
the sparse operations typically dominate the runtime due to their unstructured
memory access patterns. In particular, accesses to main memory cost hundreds
of times more than floating point calculations, and walking neighbor lists
essentially becomes pointer chasing. For this reason, neighbor intensive
algorithms become memory latency bound on cache-based CPU systems. We expect
that the high speed fabric on the Cerebras Wafer Scale Engine (WSE) will be
extremely well suited for processing deep neighbor sampling on GNNs. For our
initial work we do not expect that memory requirements will exceed the
18GB of SRAM on the WSE, and therefore we do not expect to be limited by the
1.2Tbps ethernet links.

Our models natively run with the intra-node parallelism provided by PyTorch's
OpenMP backend. In particular, with no modifications to run invocation PyTorch
consumes all 96 hyper-threads on our two socket box. In the past we have
demonstrated incremental performance improvements for workloads like GraphSAGE
by taking great care with thread placement, especially with regard to
placing forward and backwards threads on the same core. We have not studied
this for our current workloads and expect that it will not be an issue for
Neocortex.









- GraphSAGE - would like to test K > 2 which becomes prohibitively expensive


GraphSAINT
(twit_env) jgwohlbier@sdh01:~/.../twit$ time ./gs.py
already dl
num_classes: 3
Epoch: 00, Loss: 6152.9744, Train: 0.1067, Val: 0.1082, Test: 0.0995
Epoch: 01, Loss: 5849.7098, Train: 0.7799, Val: 0.7764, Test: 0.7714
Epoch: 02, Loss: 2924.7435, Train: 0.8754, Val: 0.8804, Test: 0.8792
Epoch: 03, Loss: 2201.9630, Train: 0.8703, Val: 0.8810, Test: 0.8738
Epoch: 04, Loss: 2616.2829, Train: 0.8713, Val: 0.8801, Test: 0.8760
Epoch: 05, Loss: 1943.6431, Train: 0.8804, Val: 0.8881, Test: 0.8852
Epoch: 06, Loss: 617.6883, Train: 0.8821, Val: 0.8878, Test: 0.8856
Epoch: 07, Loss: 620.2298, Train: 0.8824, Val: 0.8890, Test: 0.8875
Epoch: 08, Loss: 743.5296, Train: 0.8841, Val: 0.8896, Test: 0.8887
Epoch: 09, Loss: 376.1414, Train: 0.8859, Val: 0.8914, Test: 0.8894
Epoch: 10, Loss: 299.8327, Train: 0.8860, Val: 0.8905, Test: 0.8887
Epoch: 11, Loss: 299.5615, Train: 0.8833, Val: 0.8896, Test: 0.8881
Epoch: 12, Loss: 287.8665, Train: 0.8870, Val: 0.8933, Test: 0.8919
Epoch: 13, Loss: 240.9572, Train: 0.8949, Val: 0.8997, Test: 0.8993
Epoch: 14, Loss: 149.8815, Train: 0.8903, Val: 0.8967, Test: 0.8967
Epoch: 15, Loss: 192.3049, Train: 0.8832, Val: 0.8893, Test: 0.8887
Epoch: 16, Loss: 225.4192, Train: 0.8869, Val: 0.8930, Test: 0.8929
Epoch: 17, Loss: 129.8509, Train: 0.9107, Val: 0.9080, Test: 0.9203
Epoch: 18, Loss: 74.5670, Train: 0.9039, Val: 0.9040, Test: 0.9085
Epoch: 19, Loss: 213.2434, Train: 0.8929, Val: 0.8994, Test: 0.8967
Epoch: 20, Loss: 126.7781, Train: 0.8869, Val: 0.8930, Test: 0.8897
Epoch: 21, Loss: 97.7633, Train: 0.8874, Val: 0.8933, Test: 0.8903
Epoch: 22, Loss: 167.8456, Train: 0.8947, Val: 0.9003, Test: 0.8974
Epoch: 23, Loss: 153.3535, Train: 0.8989, Val: 0.9000, Test: 0.9044
Epoch: 24, Loss: 83.4825, Train: 0.8989, Val: 0.9025, Test: 0.9056
Epoch: 25, Loss: 123.1785, Train: 0.8956, Val: 0.8997, Test: 0.8993
Epoch: 26, Loss: 80.7431, Train: 0.8992, Val: 0.9016, Test: 0.9037
Epoch: 27, Loss: 82.1456, Train: 0.9009, Val: 0.9043, Test: 0.9053
Epoch: 28, Loss: 90.7044, Train: 0.8988, Val: 0.9019, Test: 0.9031
Epoch: 29, Loss: 72.0267, Train: 0.8980, Val: 0.9019, Test: 0.9037
Epoch: 30, Loss: 101.0051, Train: 0.9003, Val: 0.9028, Test: 0.9060
Epoch: 31, Loss: 87.6565, Train: 0.9016, Val: 0.9034, Test: 0.9069
Epoch: 32, Loss: 75.7141, Train: 0.9018, Val: 0.9034, Test: 0.9072
Epoch: 33, Loss: 77.5357, Train: 0.9025, Val: 0.9037, Test: 0.9056
Epoch: 34, Loss: 88.1981, Train: 0.9018, Val: 0.9031, Test: 0.9053
Epoch: 35, Loss: 75.4968, Train: 0.9054, Val: 0.9040, Test: 0.9079
Epoch: 36, Loss: 37.4750, Train: 0.9069, Val: 0.9043, Test: 0.9098
Epoch: 37, Loss: 113.2350, Train: 0.9052, Val: 0.9040, Test: 0.9079
Epoch: 38, Loss: 67.9553, Train: 0.9065, Val: 0.9040, Test: 0.9098
Epoch: 39, Loss: 69.5618, Train: 0.9071, Val: 0.9059, Test: 0.9127
Epoch: 40, Loss: 43.1211, Train: 0.9057, Val: 0.9046, Test: 0.9091
Epoch: 41, Loss: 96.1745, Train: 0.9063, Val: 0.9056, Test: 0.9098
Epoch: 42, Loss: 54.5213, Train: 0.9091, Val: 0.9071, Test: 0.9146
Epoch: 43, Loss: 57.0595, Train: 0.9088, Val: 0.9080, Test: 0.9165
Epoch: 44, Loss: 91.6374, Train: 0.9082, Val: 0.9068, Test: 0.9133
Epoch: 45, Loss: 95.3209, Train: 0.9132, Val: 0.9102, Test: 0.9203
Epoch: 46, Loss: 91.0889, Train: 0.9129, Val: 0.9102, Test: 0.9203
Epoch: 47, Loss: 62.0237, Train: 0.9078, Val: 0.9043, Test: 0.9127
Epoch: 48, Loss: 73.1241, Train: 0.9053, Val: 0.9049, Test: 0.9098
Epoch: 49, Loss: 71.8207, Train: 0.9076, Val: 0.9046, Test: 0.9133
Epoch: 50, Loss: 50.3067, Train: 0.9114, Val: 0.9083, Test: 0.9178
Epoch: 51, Loss: 63.4144, Train: 0.9110, Val: 0.9080, Test: 0.9178
Epoch: 52, Loss: 65.2065, Train: 0.9067, Val: 0.9046, Test: 0.9079
Epoch: 53, Loss: 62.8003, Train: 0.9055, Val: 0.9046, Test: 0.9101
Epoch: 54, Loss: 43.1601, Train: 0.9091, Val: 0.9059, Test: 0.9155
Epoch: 55, Loss: 70.8405, Train: 0.9107, Val: 0.9086, Test: 0.9178
Epoch: 56, Loss: 72.2491, Train: 0.9071, Val: 0.9049, Test: 0.9104
Epoch: 57, Loss: 51.9161, Train: 0.9074, Val: 0.9056, Test: 0.9107
Epoch: 58, Loss: 40.3683, Train: 0.9100, Val: 0.9074, Test: 0.9171
Epoch: 59, Loss: 46.3795, Train: 0.9113, Val: 0.9086, Test: 0.9168
Epoch: 60, Loss: 54.4328, Train: 0.9128, Val: 0.9080, Test: 0.9158
Epoch: 61, Loss: 35.2957, Train: 0.9091, Val: 0.9071, Test: 0.9171
Epoch: 62, Loss: 43.8879, Train: 0.9123, Val: 0.9117, Test: 0.9197
Epoch: 63, Loss: 43.2893, Train: 0.9129, Val: 0.9114, Test: 0.9200
Epoch: 64, Loss: 21.7307, Train: 0.9154, Val: 0.9114, Test: 0.9225
Epoch: 65, Loss: 62.9612, Train: 0.9140, Val: 0.9111, Test: 0.9178
Epoch: 66, Loss: 74.3256, Train: 0.9129, Val: 0.9092, Test: 0.9155
Epoch: 67, Loss: 42.0416, Train: 0.9145, Val: 0.9105, Test: 0.9216
Epoch: 68, Loss: 46.6265, Train: 0.9149, Val: 0.9117, Test: 0.9203
Epoch: 69, Loss: 51.7324, Train: 0.9083, Val: 0.9077, Test: 0.9114
Epoch: 70, Loss: 102.8084, Train: 0.9048, Val: 0.9056, Test: 0.9082
Epoch: 71, Loss: 60.8171, Train: 0.9117, Val: 0.9086, Test: 0.9165
Epoch: 72, Loss: 61.8582, Train: 0.9152, Val: 0.9126, Test: 0.9225
Epoch: 73, Loss: 65.4072, Train: 0.9084, Val: 0.9062, Test: 0.9101
Epoch: 74, Loss: 38.0276, Train: 0.9040, Val: 0.9049, Test: 0.9098
Epoch: 75, Loss: 30.3875, Train: 0.9125, Val: 0.9089, Test: 0.9152
Epoch: 76, Loss: 47.9553, Train: 0.9169, Val: 0.9151, Test: 0.9245
Epoch: 77, Loss: 37.9037, Train: 0.9157, Val: 0.9126, Test: 0.9222
Epoch: 78, Loss: 54.1659, Train: 0.9100, Val: 0.9074, Test: 0.9107
Epoch: 79, Loss: 63.5994, Train: 0.9114, Val: 0.9095, Test: 0.9123
Epoch: 80, Loss: 44.6015, Train: 0.9159, Val: 0.9117, Test: 0.9187
Epoch: 81, Loss: 44.2496, Train: 0.9165, Val: 0.9135, Test: 0.9225
Epoch: 82, Loss: 16.6150, Train: 0.9148, Val: 0.9126, Test: 0.9209
Epoch: 83, Loss: 26.9345, Train: 0.9091, Val: 0.9074, Test: 0.9098
Epoch: 84, Loss: 13.8865, Train: 0.9119, Val: 0.9089, Test: 0.9155
Epoch: 85, Loss: 29.2247, Train: 0.9177, Val: 0.9151, Test: 0.9245
Epoch: 86, Loss: 27.5849, Train: 0.9179, Val: 0.9148, Test: 0.9232
Epoch: 87, Loss: 33.2912, Train: 0.9181, Val: 0.9138, Test: 0.9235
Epoch: 88, Loss: 40.1010, Train: 0.9153, Val: 0.9120, Test: 0.9206
Epoch: 89, Loss: 24.4981, Train: 0.9161, Val: 0.9126, Test: 0.9216
Epoch: 90, Loss: 30.0685, Train: 0.9183, Val: 0.9160, Test: 0.9238
Epoch: 91, Loss: 38.9167, Train: 0.9218, Val: 0.9172, Test: 0.9257
Epoch: 92, Loss: 42.6400, Train: 0.9191, Val: 0.9151, Test: 0.9248
Epoch: 93, Loss: 26.2917, Train: 0.9175, Val: 0.9144, Test: 0.9241
Epoch: 94, Loss: 38.4658, Train: 0.9119, Val: 0.9098, Test: 0.9184
Epoch: 95, Loss: 31.9538, Train: 0.9102, Val: 0.9095, Test: 0.9117
Epoch: 96, Loss: 41.5685, Train: 0.9108, Val: 0.9102, Test: 0.9139
Epoch: 97, Loss: 28.2606, Train: 0.9159, Val: 0.9138, Test: 0.9216
Epoch: 98, Loss: 25.5384, Train: 0.9155, Val: 0.9138, Test: 0.9222
Epoch: 99, Loss: 24.2836, Train: 0.9116, Val: 0.9102, Test: 0.9165
Epoch: 100, Loss: 24.7018, Train: 0.9175, Val: 0.9138, Test: 0.9203
Epoch: 101, Loss: 25.7313, Train: 0.9245, Val: 0.9206, Test: 0.9289
Epoch: 102, Loss: 20.8281, Train: 0.9187, Val: 0.9160, Test: 0.9251
Epoch: 103, Loss: 17.2633, Train: 0.9142, Val: 0.9138, Test: 0.9193
Epoch: 104, Loss: 33.1495, Train: 0.9117, Val: 0.9105, Test: 0.9142
Epoch: 105, Loss: 35.1639, Train: 0.9148, Val: 0.9132, Test: 0.9200
Epoch: 106, Loss: 32.5242, Train: 0.9188, Val: 0.9166, Test: 0.9238
Epoch: 107, Loss: 29.7929, Train: 0.9182, Val: 0.9175, Test: 0.9248
Epoch: 108, Loss: 24.7523, Train: 0.9143, Val: 0.9141, Test: 0.9193
Epoch: 109, Loss: 33.3794, Train: 0.9168, Val: 0.9163, Test: 0.9206
Epoch: 110, Loss: 18.3871, Train: 0.9213, Val: 0.9200, Test: 0.9260
Epoch: 111, Loss: 10.0313, Train: 0.9202, Val: 0.9175, Test: 0.9238
Epoch: 112, Loss: 29.5538, Train: 0.9189, Val: 0.9154, Test: 0.9235
Epoch: 113, Loss: 20.5242, Train: 0.9174, Val: 0.9154, Test: 0.9229
Epoch: 114, Loss: 22.4150, Train: 0.9175, Val: 0.9157, Test: 0.9235
Epoch: 115, Loss: 25.2596, Train: 0.9167, Val: 0.9151, Test: 0.9225
Epoch: 116, Loss: 21.2470, Train: 0.9191, Val: 0.9160, Test: 0.9232
Epoch: 117, Loss: 31.3492, Train: 0.9228, Val: 0.9178, Test: 0.9248
Epoch: 118, Loss: 34.6205, Train: 0.9174, Val: 0.9166, Test: 0.9222
Epoch: 119, Loss: 20.8133, Train: 0.9166, Val: 0.9151, Test: 0.9229
Epoch: 120, Loss: 32.1324, Train: 0.9147, Val: 0.9141, Test: 0.9171
Epoch: 121, Loss: 17.2116, Train: 0.9166, Val: 0.9151, Test: 0.9222
Epoch: 122, Loss: 14.8822, Train: 0.9175, Val: 0.9148, Test: 0.9213
Epoch: 123, Loss: 19.0484, Train: 0.9172, Val: 0.9138, Test: 0.9222
Epoch: 124, Loss: 40.9435, Train: 0.9118, Val: 0.9126, Test: 0.9184
Epoch: 125, Loss: 24.0119, Train: 0.9140, Val: 0.9114, Test: 0.9181
Epoch: 126, Loss: 25.4207, Train: 0.9176, Val: 0.9138, Test: 0.9213
Epoch: 127, Loss: 31.6674, Train: 0.9172, Val: 0.9163, Test: 0.9225
Epoch: 128, Loss: 39.3910, Train: 0.9199, Val: 0.9154, Test: 0.9241
Epoch: 129, Loss: 17.2587, Train: 0.9228, Val: 0.9163, Test: 0.9283
Epoch: 130, Loss: 25.2369, Train: 0.9244, Val: 0.9193, Test: 0.9292
Epoch: 131, Loss: 33.9533, Train: 0.9212, Val: 0.9175, Test: 0.9257
Epoch: 132, Loss: 20.9923, Train: 0.9190, Val: 0.9166, Test: 0.9241
Epoch: 133, Loss: 16.3543, Train: 0.9150, Val: 0.9120, Test: 0.9193
Epoch: 134, Loss: 24.5229, Train: 0.9127, Val: 0.9132, Test: 0.9181
Epoch: 135, Loss: 20.8190, Train: 0.9195, Val: 0.9178, Test: 0.9241
Epoch: 136, Loss: 15.3990, Train: 0.9177, Val: 0.9163, Test: 0.9238
Epoch: 137, Loss: 14.6150, Train: 0.9172, Val: 0.9154, Test: 0.9213
Epoch: 138, Loss: 25.3938, Train: 0.9165, Val: 0.9148, Test: 0.9197
Epoch: 139, Loss: 20.9503, Train: 0.9176, Val: 0.9123, Test: 0.9241
Epoch: 140, Loss: 14.7233, Train: 0.9197, Val: 0.9178, Test: 0.9248
Epoch: 141, Loss: 12.7073, Train: 0.9191, Val: 0.9169, Test: 0.9251
Epoch: 142, Loss: 18.7380, Train: 0.9164, Val: 0.9141, Test: 0.9222
Epoch: 143, Loss: 8.5804, Train: 0.9179, Val: 0.9154, Test: 0.9251
Epoch: 144, Loss: 24.7015, Train: 0.9182, Val: 0.9172, Test: 0.9232
Epoch: 145, Loss: 21.8371, Train: 0.9183, Val: 0.9163, Test: 0.9219
Epoch: 146, Loss: 25.0706, Train: 0.9218, Val: 0.9193, Test: 0.9260
Epoch: 147, Loss: 25.1566, Train: 0.9220, Val: 0.9175, Test: 0.9264
Epoch: 148, Loss: 29.4475, Train: 0.9129, Val: 0.9126, Test: 0.9206
Epoch: 149, Loss: 23.7964, Train: 0.9120, Val: 0.9120, Test: 0.9181
Epoch: 150, Loss: 18.5737, Train: 0.9162, Val: 0.9126, Test: 0.9213
Epoch: 151, Loss: 20.6997, Train: 0.9227, Val: 0.9200, Test: 0.9283
Epoch: 152, Loss: 35.0313, Train: 0.9165, Val: 0.9163, Test: 0.9206
Epoch: 153, Loss: 34.4330, Train: 0.9159, Val: 0.9144, Test: 0.9216
Epoch: 154, Loss: 32.9417, Train: 0.9175, Val: 0.9157, Test: 0.9219
Epoch: 155, Loss: 34.0486, Train: 0.9044, Val: 0.9059, Test: 0.9123
Epoch: 156, Loss: 25.0675, Train: 0.9151, Val: 0.9148, Test: 0.9190
Epoch: 157, Loss: 41.3952, Train: 0.9170, Val: 0.9144, Test: 0.9209
Epoch: 158, Loss: 29.6668, Train: 0.9106, Val: 0.9117, Test: 0.9162
Epoch: 159, Loss: 23.4954, Train: 0.9088, Val: 0.9083, Test: 0.9114
Epoch: 160, Loss: 8.2189, Train: 0.9165, Val: 0.9138, Test: 0.9232
Epoch: 161, Loss: 24.0030, Train: 0.9232, Val: 0.9206, Test: 0.9273
Epoch: 162, Loss: 21.1875, Train: 0.9222, Val: 0.9190, Test: 0.9276
Epoch: 163, Loss: 25.8295, Train: 0.9181, Val: 0.9163, Test: 0.9241
Epoch: 164, Loss: 16.1199, Train: 0.9181, Val: 0.9178, Test: 0.9241
Epoch: 165, Loss: 13.9893, Train: 0.9208, Val: 0.9193, Test: 0.9270
Epoch: 166, Loss: 19.0797, Train: 0.9185, Val: 0.9178, Test: 0.9241
Epoch: 167, Loss: 22.1657, Train: 0.9221, Val: 0.9200, Test: 0.9286
Epoch: 168, Loss: 18.6282, Train: 0.9171, Val: 0.9172, Test: 0.9241
Epoch: 169, Loss: 12.6754, Train: 0.9191, Val: 0.9190, Test: 0.9276
Epoch: 170, Loss: 11.4981, Train: 0.9188, Val: 0.9187, Test: 0.9260
Epoch: 171, Loss: 13.6265, Train: 0.9189, Val: 0.9169, Test: 0.9251
Epoch: 172, Loss: 21.1724, Train: 0.9187, Val: 0.9160, Test: 0.9251
Epoch: 173, Loss: 14.3551, Train: 0.9251, Val: 0.9221, Test: 0.9321
Epoch: 174, Loss: 27.0363, Train: 0.9246, Val: 0.9218, Test: 0.9283
Epoch: 175, Loss: 17.2806, Train: 0.9206, Val: 0.9184, Test: 0.9238
Epoch: 176, Loss: 21.4222, Train: 0.9193, Val: 0.9175, Test: 0.9235
Epoch: 177, Loss: 28.1890, Train: 0.9228, Val: 0.9215, Test: 0.9296
Epoch: 178, Loss: 18.0498, Train: 0.9228, Val: 0.9224, Test: 0.9289
Epoch: 179, Loss: 18.5037, Train: 0.9192, Val: 0.9157, Test: 0.9257
Epoch: 180, Loss: 27.5394, Train: 0.9216, Val: 0.9203, Test: 0.9286
Epoch: 181, Loss: 19.3975, Train: 0.9156, Val: 0.9144, Test: 0.9232
Epoch: 182, Loss: 13.9634, Train: 0.9189, Val: 0.9169, Test: 0.9260
Epoch: 183, Loss: 28.5592, Train: 0.9215, Val: 0.9175, Test: 0.9273
Epoch: 184, Loss: 24.2978, Train: 0.9287, Val: 0.9239, Test: 0.9356
Epoch: 185, Loss: 9.3759, Train: 0.9236, Val: 0.9190, Test: 0.9276
Epoch: 186, Loss: 21.5781, Train: 0.9219, Val: 0.9200, Test: 0.9276
Epoch: 187, Loss: 21.2927, Train: 0.9218, Val: 0.9187, Test: 0.9264
Epoch: 188, Loss: 20.5797, Train: 0.9265, Val: 0.9243, Test: 0.9324
Epoch: 189, Loss: 20.9233, Train: 0.9272, Val: 0.9243, Test: 0.9343
Epoch: 190, Loss: 14.4878, Train: 0.9247, Val: 0.9227, Test: 0.9305
Epoch: 191, Loss: 23.5189, Train: 0.9218, Val: 0.9224, Test: 0.9286
Epoch: 192, Loss: 29.1323, Train: 0.9195, Val: 0.9187, Test: 0.9251
Epoch: 193, Loss: 37.7024, Train: 0.9243, Val: 0.9230, Test: 0.9302
Epoch: 194, Loss: 13.7675, Train: 0.9185, Val: 0.9187, Test: 0.9248
Epoch: 195, Loss: 20.5915, Train: 0.9163, Val: 0.9151, Test: 0.9229
Epoch: 196, Loss: 36.3199, Train: 0.9147, Val: 0.9132, Test: 0.9193
Epoch: 197, Loss: 7.0784, Train: 0.9221, Val: 0.9203, Test: 0.9289
Epoch: 198, Loss: 34.5632, Train: 0.9212, Val: 0.9203, Test: 0.9270
Epoch: 199, Loss: 33.6305, Train: 0.9212, Val: 0.9200, Test: 0.9276
Epoch: 200, Loss: 20.1478, Train: 0.9283, Val: 0.9218, Test: 0.9343
Epoch: 201, Loss: 36.5633, Train: 0.9065, Val: 0.9046, Test: 0.9111
Epoch: 202, Loss: 35.0013, Train: 0.9171, Val: 0.9123, Test: 0.9222
Epoch: 203, Loss: 35.5217, Train: 0.9093, Val: 0.9034, Test: 0.9139
Epoch: 204, Loss: 30.4119, Train: 0.9141, Val: 0.9132, Test: 0.9174
Epoch: 205, Loss: 42.8894, Train: 0.9134, Val: 0.9138, Test: 0.9152
Epoch: 206, Loss: 14.4087, Train: 0.9211, Val: 0.9187, Test: 0.9267
Epoch: 207, Loss: 37.3590, Train: 0.9254, Val: 0.9227, Test: 0.9299
Epoch: 208, Loss: 22.8356, Train: 0.9258, Val: 0.9215, Test: 0.9292
Epoch: 209, Loss: 13.8252, Train: 0.9249, Val: 0.9230, Test: 0.9276
Epoch: 210, Loss: 24.7943, Train: 0.9261, Val: 0.9221, Test: 0.9302
Epoch: 211, Loss: 25.9551, Train: 0.9227, Val: 0.9209, Test: 0.9292
Epoch: 212, Loss: 8.5868, Train: 0.9230, Val: 0.9203, Test: 0.9276
Epoch: 213, Loss: 23.3470, Train: 0.9215, Val: 0.9175, Test: 0.9270
Epoch: 214, Loss: 11.5452, Train: 0.9218, Val: 0.9215, Test: 0.9264
Epoch: 215, Loss: 12.3655, Train: 0.9204, Val: 0.9200, Test: 0.9257
Epoch: 216, Loss: 15.8874, Train: 0.9198, Val: 0.9172, Test: 0.9248
Epoch: 217, Loss: 6.9739, Train: 0.9251, Val: 0.9246, Test: 0.9311
Epoch: 218, Loss: 15.1930, Train: 0.9232, Val: 0.9197, Test: 0.9280
Epoch: 219, Loss: 20.4351, Train: 0.9314, Val: 0.9310, Test: 0.9369
Epoch: 220, Loss: 15.3994, Train: 0.9298, Val: 0.9258, Test: 0.9347
Epoch: 221, Loss: 22.0105, Train: 0.9273, Val: 0.9255, Test: 0.9315
Epoch: 222, Loss: 13.3998, Train: 0.9256, Val: 0.9215, Test: 0.9311
Epoch: 223, Loss: 12.7604, Train: 0.9237, Val: 0.9221, Test: 0.9296
Epoch: 224, Loss: 13.5998, Train: 0.9224, Val: 0.9221, Test: 0.9308
Epoch: 225, Loss: 25.4535, Train: 0.9237, Val: 0.9224, Test: 0.9311
Epoch: 226, Loss: 12.4421, Train: 0.9290, Val: 0.9264, Test: 0.9340
Epoch: 227, Loss: 11.4691, Train: 0.9307, Val: 0.9270, Test: 0.9353
Epoch: 228, Loss: 23.6826, Train: 0.9281, Val: 0.9236, Test: 0.9337
Epoch: 229, Loss: 23.0538, Train: 0.9085, Val: 0.9114, Test: 0.9158
Epoch: 230, Loss: 36.3948, Train: 0.9114, Val: 0.9105, Test: 0.9184
Epoch: 231, Loss: 19.8688, Train: 0.9065, Val: 0.9080, Test: 0.9111
Epoch: 232, Loss: 9.0966, Train: 0.9083, Val: 0.9102, Test: 0.9158
Epoch: 233, Loss: 13.0405, Train: 0.9165, Val: 0.9132, Test: 0.9200
Epoch: 234, Loss: 15.8490, Train: 0.9267, Val: 0.9246, Test: 0.9327

Epoch: 235, Loss: 11.3157, Train: 0.9215, Val: 0.9209, Test: 0.9296
Epoch: 236, Loss: 11.0374, Train: 0.9171, Val: 0.9160, Test: 0.9229
Epoch: 237, Loss: 14.9429, Train: 0.9187, Val: 0.9163, Test: 0.9238
Epoch: 238, Loss: 16.6493, Train: 0.9247, Val: 0.9227, Test: 0.9308
Epoch: 239, Loss: 13.5081, Train: 0.9271, Val: 0.9246, Test: 0.9334
Epoch: 240, Loss: 22.3959, Train: 0.9231, Val: 0.9236, Test: 0.9283
Epoch: 241, Loss: 22.7363, Train: 0.9225, Val: 0.9200, Test: 0.9273
Epoch: 242, Loss: 12.6171, Train: 0.9271, Val: 0.9230, Test: 0.9337
Epoch: 243, Loss: 9.0133, Train: 0.9218, Val: 0.9212, Test: 0.9276
Epoch: 244, Loss: 10.3485, Train: 0.9210, Val: 0.9206, Test: 0.9286
Epoch: 245, Loss: 11.8978, Train: 0.9210, Val: 0.9193, Test: 0.9276
Epoch: 246, Loss: 12.6055, Train: 0.9285, Val: 0.9246, Test: 0.9356
Epoch: 247, Loss: 9.9954, Train: 0.9288, Val: 0.9261, Test: 0.9372
Epoch: 248, Loss: 13.3060, Train: 0.9228, Val: 0.9224, Test: 0.9299
Epoch: 249, Loss: 6.2546, Train: 0.9199, Val: 0.9197, Test: 0.9257
Epoch: 250, Loss: 8.2962, Train: 0.9228, Val: 0.9221, Test: 0.9292
Epoch: 251, Loss: 9.7987, Train: 0.9261, Val: 0.9239, Test: 0.9337
Epoch: 252, Loss: 12.1559, Train: 0.9228, Val: 0.9218, Test: 0.9308
Epoch: 253, Loss: 12.3303, Train: 0.9211, Val: 0.9184, Test: 0.9264
Epoch: 254, Loss: 9.0998, Train: 0.9276, Val: 0.9246, Test: 0.9334
Epoch: 255, Loss: 11.8000, Train: 0.9324, Val: 0.9310, Test: 0.9401
Epoch: 256, Loss: 13.2272, Train: 0.9275, Val: 0.9255, Test: 0.9327
Epoch: 257, Loss: 18.6293, Train: 0.9203, Val: 0.9184, Test: 0.9276
Epoch: 258, Loss: 7.1767, Train: 0.9276, Val: 0.9289, Test: 0.9340
Epoch: 259, Loss: 7.0314, Train: 0.9281, Val: 0.9279, Test: 0.9343
Epoch: 260, Loss: 10.8939, Train: 0.9248, Val: 0.9255, Test: 0.9296
Epoch: 261, Loss: 7.5628, Train: 0.9260, Val: 0.9258, Test: 0.9318
Epoch: 262, Loss: 4.4346, Train: 0.9298, Val: 0.9301, Test: 0.9343
Epoch: 263, Loss: 6.3339, Train: 0.9312, Val: 0.9301, Test: 0.9353
Epoch: 264, Loss: 38.3344, Train: 0.9276, Val: 0.9264, Test: 0.9331
Epoch: 265, Loss: 15.9322, Train: 0.9241, Val: 0.9243, Test: 0.9299
Epoch: 266, Loss: 8.6754, Train: 0.9202, Val: 0.9175, Test: 0.9264
Epoch: 267, Loss: 18.6772, Train: 0.9146, Val: 0.9144, Test: 0.9187
Epoch: 268, Loss: 14.1550, Train: 0.9141, Val: 0.9138, Test: 0.9184
Epoch: 269, Loss: 15.1522, Train: 0.9323, Val: 0.9298, Test: 0.9382
Epoch: 270, Loss: 12.7958, Train: 0.9260, Val: 0.9243, Test: 0.9318
Epoch: 271, Loss: 9.5278, Train: 0.9203, Val: 0.9197, Test: 0.9241
Epoch: 272, Loss: 10.3335, Train: 0.9256, Val: 0.9252, Test: 0.9305
Epoch: 273, Loss: 4.0103, Train: 0.9305, Val: 0.9304, Test: 0.9369
Epoch: 274, Loss: 11.8475, Train: 0.9260, Val: 0.9255, Test: 0.9334
Epoch: 275, Loss: 7.5658, Train: 0.9252, Val: 0.9233, Test: 0.9286
Epoch: 276, Loss: 9.2356, Train: 0.9314, Val: 0.9304, Test: 0.9353
Epoch: 277, Loss: 5.9925, Train: 0.9317, Val: 0.9313, Test: 0.9372
Epoch: 278, Loss: 13.3810, Train: 0.9287, Val: 0.9276, Test: 0.9350
Epoch: 279, Loss: 12.9408, Train: 0.9283, Val: 0.9264, Test: 0.9337
Epoch: 280, Loss: 9.5477, Train: 0.9294, Val: 0.9282, Test: 0.9362
Epoch: 281, Loss: 6.9200, Train: 0.9271, Val: 0.9249, Test: 0.9324
Epoch: 282, Loss: 16.5118, Train: 0.9259, Val: 0.9246, Test: 0.9292
Epoch: 283, Loss: 7.0093, Train: 0.9269, Val: 0.9264, Test: 0.9308
Epoch: 284, Loss: 6.9040, Train: 0.9244, Val: 0.9233, Test: 0.9289
Epoch: 285, Loss: 5.8401, Train: 0.9283, Val: 0.9267, Test: 0.9337
Epoch: 286, Loss: 8.8999, Train: 0.9320, Val: 0.9310, Test: 0.9353
Epoch: 287, Loss: 6.9867, Train: 0.9273, Val: 0.9264, Test: 0.9334
Epoch: 288, Loss: 5.8154, Train: 0.9259, Val: 0.9239, Test: 0.9308
Epoch: 289, Loss: 7.9326, Train: 0.9301, Val: 0.9298, Test: 0.9353
Epoch: 290, Loss: 6.2391, Train: 0.9304, Val: 0.9307, Test: 0.9362
Epoch: 291, Loss: 26.2283, Train: 0.9252, Val: 0.9212, Test: 0.9280
Epoch: 292, Loss: 19.1484, Train: 0.9264, Val: 0.9267, Test: 0.9334
Epoch: 293, Loss: 24.9480, Train: 0.9268, Val: 0.9255, Test: 0.9331
Epoch: 294, Loss: 16.3649, Train: 0.9263, Val: 0.9246, Test: 0.9305
Epoch: 295, Loss: 13.5803, Train: 0.9319, Val: 0.9304, Test: 0.9350
Epoch: 296, Loss: 14.8107, Train: 0.9231, Val: 0.9224, Test: 0.9260
Epoch: 297, Loss: 12.4926, Train: 0.9224, Val: 0.9209, Test: 0.9264
Epoch: 298, Loss: 10.6645, Train: 0.9271, Val: 0.9246, Test: 0.9302
Epoch: 299, Loss: 15.3599, Train: 0.9272, Val: 0.9264, Test: 0.9334
Epoch: 300, Loss: 11.0727, Train: 0.9306, Val: 0.9279, Test: 0.9340
Epoch: 301, Loss: 7.6177, Train: 0.9272, Val: 0.9258, Test: 0.9311
Epoch: 302, Loss: 18.3474, Train: 0.9240, Val: 0.9221, Test: 0.9273
Epoch: 303, Loss: 7.2511, Train: 0.9275, Val: 0.9273, Test: 0.9318
Epoch: 304, Loss: 10.6152, Train: 0.9288, Val: 0.9279, Test: 0.9343
Epoch: 305, Loss: 14.6335, Train: 0.9275, Val: 0.9258, Test: 0.9321
Epoch: 306, Loss: 17.1904, Train: 0.9256, Val: 0.9252, Test: 0.9280
Epoch: 307, Loss: 13.2667, Train: 0.9313, Val: 0.9310, Test: 0.9350
Epoch: 308, Loss: 9.9787, Train: 0.9292, Val: 0.9304, Test: 0.9343
Epoch: 309, Loss: 34.1177, Train: 0.9259, Val: 0.9230, Test: 0.9302
Epoch: 310, Loss: 10.3757, Train: 0.9285, Val: 0.9285, Test: 0.9353
Epoch: 311, Loss: 9.9604, Train: 0.9283, Val: 0.9282, Test: 0.9347
Epoch: 312, Loss: 9.3260, Train: 0.9320, Val: 0.9304, Test: 0.9359
Epoch: 313, Loss: 8.2714, Train: 0.9230, Val: 0.9209, Test: 0.9315
Epoch: 314, Loss: 12.7569, Train: 0.9208, Val: 0.9175, Test: 0.9270
Epoch: 315, Loss: 10.2401, Train: 0.9225, Val: 0.9190, Test: 0.9286
Epoch: 316, Loss: 7.0700, Train: 0.9255, Val: 0.9224, Test: 0.9318
Epoch: 317, Loss: 12.1055, Train: 0.9277, Val: 0.9249, Test: 0.9356
Epoch: 318, Loss: 5.6357, Train: 0.9256, Val: 0.9243, Test: 0.9299
Epoch: 319, Loss: 4.9561, Train: 0.9243, Val: 0.9246, Test: 0.9305
Epoch: 320, Loss: 7.6442, Train: 0.9273, Val: 0.9252, Test: 0.9337
Epoch: 321, Loss: 11.3715, Train: 0.9273, Val: 0.9267, Test: 0.9353
Epoch: 322, Loss: 14.7866, Train: 0.9252, Val: 0.9221, Test: 0.9343
Epoch: 323, Loss: 7.6874, Train: 0.9222, Val: 0.9200, Test: 0.9264
Epoch: 324, Loss: 10.2283, Train: 0.9289, Val: 0.9279, Test: 0.9353
Epoch: 325, Loss: 11.3756, Train: 0.9276, Val: 0.9273, Test: 0.9378
Epoch: 326, Loss: 8.4519, Train: 0.9216, Val: 0.9190, Test: 0.9299
Epoch: 327, Loss: 7.3319, Train: 0.9261, Val: 0.9224, Test: 0.9321
Epoch: 328, Loss: 8.2358, Train: 0.9262, Val: 0.9258, Test: 0.9327
Epoch: 329, Loss: 11.9761, Train: 0.9252, Val: 0.9224, Test: 0.9337
Epoch: 330, Loss: 2.9056, Train: 0.9261, Val: 0.9252, Test: 0.9331
Epoch: 331, Loss: 11.3896, Train: 0.9324, Val: 0.9313, Test: 0.9410
Epoch: 332, Loss: 7.8439, Train: 0.9292, Val: 0.9273, Test: 0.9359
Epoch: 333, Loss: 7.7573, Train: 0.9241, Val: 0.9218, Test: 0.9292
Epoch: 334, Loss: 5.4376, Train: 0.9291, Val: 0.9292, Test: 0.9347
Epoch: 335, Loss: 5.1428, Train: 0.9343, Val: 0.9344, Test: 0.9407
Epoch: 336, Loss: 7.1032, Train: 0.9305, Val: 0.9292, Test: 0.9362
Epoch: 337, Loss: 6.6410, Train: 0.9288, Val: 0.9279, Test: 0.9359
Epoch: 338, Loss: 6.5617, Train: 0.9297, Val: 0.9285, Test: 0.9356
Epoch: 339, Loss: 9.7083, Train: 0.9273, Val: 0.9273, Test: 0.9324
Epoch: 340, Loss: 8.8254, Train: 0.9285, Val: 0.9270, Test: 0.9347
Epoch: 341, Loss: 3.7582, Train: 0.9273, Val: 0.9264, Test: 0.9356
Epoch: 342, Loss: 13.0337, Train: 0.9250, Val: 0.9215, Test: 0.9321
Epoch: 343, Loss: 6.4384, Train: 0.9259, Val: 0.9243, Test: 0.9324
Epoch: 344, Loss: 12.7442, Train: 0.9305, Val: 0.9295, Test: 0.9369
Epoch: 345, Loss: 5.1243, Train: 0.9279, Val: 0.9264, Test: 0.9337
Epoch: 346, Loss: 6.0483, Train: 0.9269, Val: 0.9255, Test: 0.9324
Epoch: 347, Loss: 8.5772, Train: 0.9319, Val: 0.9313, Test: 0.9391
Epoch: 348, Loss: 4.9142, Train: 0.9314, Val: 0.9301, Test: 0.9372
Epoch: 349, Loss: 7.5418, Train: 0.9268, Val: 0.9252, Test: 0.9331
Epoch: 350, Loss: 7.7056, Train: 0.9297, Val: 0.9295, Test: 0.9353
Epoch: 351, Loss: 9.6949, Train: 0.9324, Val: 0.9313, Test: 0.9382
Epoch: 352, Loss: 4.3672, Train: 0.9276, Val: 0.9255, Test: 0.9347
Epoch: 353, Loss: 9.0822, Train: 0.9302, Val: 0.9273, Test: 0.9359
Epoch: 354, Loss: 7.1616, Train: 0.9375, Val: 0.9359, Test: 0.9436
Epoch: 355, Loss: 3.8528, Train: 0.9290, Val: 0.9289, Test: 0.9362
Epoch: 356, Loss: 5.0207, Train: 0.9271, Val: 0.9249, Test: 0.9286
Epoch: 357, Loss: 5.8221, Train: 0.9295, Val: 0.9282, Test: 0.9340
Epoch: 358, Loss: 12.5427, Train: 0.9327, Val: 0.9322, Test: 0.9359
Epoch: 359, Loss: 6.0859, Train: 0.9201, Val: 0.9197, Test: 0.9267
Epoch: 360, Loss: 11.5007, Train: 0.9216, Val: 0.9221, Test: 0.9280
Epoch: 361, Loss: 6.5287, Train: 0.9284, Val: 0.9285, Test: 0.9362
Epoch: 362, Loss: 8.8238, Train: 0.9264, Val: 0.9258, Test: 0.9315
Epoch: 363, Loss: 5.8687, Train: 0.9305, Val: 0.9298, Test: 0.9350
Epoch: 364, Loss: 2.7504, Train: 0.9335, Val: 0.9331, Test: 0.9378
Epoch: 365, Loss: 4.9514, Train: 0.9327, Val: 0.9319, Test: 0.9378
Epoch: 366, Loss: 7.6638, Train: 0.9320, Val: 0.9310, Test: 0.9372
Epoch: 367, Loss: 6.9268, Train: 0.9333, Val: 0.9335, Test: 0.9375
Epoch: 368, Loss: 5.0194, Train: 0.9324, Val: 0.9313, Test: 0.9378
Epoch: 369, Loss: 5.3894, Train: 0.9317, Val: 0.9307, Test: 0.9366
Epoch: 370, Loss: 5.0689, Train: 0.9315, Val: 0.9301, Test: 0.9366
Epoch: 371, Loss: 13.6552, Train: 0.9316, Val: 0.9316, Test: 0.9369
Epoch: 372, Loss: 2.6344, Train: 0.9280, Val: 0.9273, Test: 0.9366
Epoch: 373, Loss: 6.7206, Train: 0.9287, Val: 0.9261, Test: 0.9340
Epoch: 374, Loss: 4.7947, Train: 0.9330, Val: 0.9341, Test: 0.9378
Epoch: 375, Loss: 3.5950, Train: 0.9313, Val: 0.9313, Test: 0.9369
Epoch: 376, Loss: 3.0249, Train: 0.9275, Val: 0.9270, Test: 0.9359
Epoch: 377, Loss: 2.5567, Train: 0.9241, Val: 0.9236, Test: 0.9308
Epoch: 378, Loss: 4.1129, Train: 0.9244, Val: 0.9236, Test: 0.9315
Epoch: 379, Loss: 3.4955, Train: 0.9300, Val: 0.9285, Test: 0.9366
Epoch: 380, Loss: 3.1884, Train: 0.9319, Val: 0.9304, Test: 0.9382
Epoch: 381, Loss: 3.3369, Train: 0.9318, Val: 0.9310, Test: 0.9366
Epoch: 382, Loss: 4.7483, Train: 0.9301, Val: 0.9289, Test: 0.9372
Epoch: 383, Loss: 4.0170, Train: 0.9280, Val: 0.9273, Test: 0.9353
Epoch: 384, Loss: 5.0471, Train: 0.9286, Val: 0.9273, Test: 0.9362
Epoch: 385, Loss: 2.1502, Train: 0.9286, Val: 0.9285, Test: 0.9366
Epoch: 386, Loss: 5.9099, Train: 0.9302, Val: 0.9304, Test: 0.9382
Epoch: 387, Loss: 3.7647, Train: 0.9308, Val: 0.9301, Test: 0.9378
Epoch: 388, Loss: 2.4975, Train: 0.9324, Val: 0.9322, Test: 0.9394
Epoch: 389, Loss: 11.3523, Train: 0.9317, Val: 0.9307, Test: 0.9385
Epoch: 390, Loss: 17.8730, Train: 0.9282, Val: 0.9273, Test: 0.9378
Epoch: 391, Loss: 11.2935, Train: 0.9220, Val: 0.9197, Test: 0.9296
Epoch: 392, Loss: 5.8266, Train: 0.9193, Val: 0.9169, Test: 0.9241
Epoch: 393, Loss: 9.3047, Train: 0.9315, Val: 0.9316, Test: 0.9394
Epoch: 394, Loss: 13.7220, Train: 0.9242, Val: 0.9243, Test: 0.9299
Epoch: 395, Loss: 12.5734, Train: 0.9195, Val: 0.9172, Test: 0.9235
Epoch: 396, Loss: 3.5654, Train: 0.9269, Val: 0.9243, Test: 0.9302
Epoch: 397, Loss: 4.1817, Train: 0.9358, Val: 0.9365, Test: 0.9436
Epoch: 398, Loss: 5.5915, Train: 0.9279, Val: 0.9252, Test: 0.9353
Epoch: 399, Loss: 17.7465, Train: 0.9230, Val: 0.9200, Test: 0.9260
Epoch: 400, Loss: 11.9496, Train: 0.9308, Val: 0.9289, Test: 0.9394
Epoch: 401, Loss: 5.8713, Train: 0.9365, Val: 0.9356, Test: 0.9439
Epoch: 402, Loss: 6.7520, Train: 0.9325, Val: 0.9313, Test: 0.9385
Epoch: 403, Loss: 8.0864, Train: 0.9333, Val: 0.9322, Test: 0.9398
Epoch: 404, Loss: 4.4446, Train: 0.9337, Val: 0.9328, Test: 0.9382
Epoch: 405, Loss: 4.2882, Train: 0.9331, Val: 0.9316, Test: 0.9413
Epoch: 406, Loss: 4.8097, Train: 0.9346, Val: 0.9325, Test: 0.9413
Epoch: 407, Loss: 4.3282, Train: 0.9320, Val: 0.9310, Test: 0.9385
Epoch: 408, Loss: 4.8470, Train: 0.9285, Val: 0.9261, Test: 0.9356
Epoch: 409, Loss: 3.5339, Train: 0.9291, Val: 0.9273, Test: 0.9343
Epoch: 410, Loss: 2.9113, Train: 0.9338, Val: 0.9319, Test: 0.9410
Epoch: 411, Loss: 4.3689, Train: 0.9334, Val: 0.9313, Test: 0.9378
Epoch: 412, Loss: 4.6468, Train: 0.9272, Val: 0.9258, Test: 0.9337
Epoch: 413, Loss: 11.5155, Train: 0.9276, Val: 0.9270, Test: 0.9359
Epoch: 414, Loss: 6.9444, Train: 0.9253, Val: 0.9218, Test: 0.9318
Epoch: 415, Loss: 7.3329, Train: 0.9250, Val: 0.9230, Test: 0.9283
Epoch: 416, Loss: 8.6334, Train: 0.9236, Val: 0.9212, Test: 0.9318
Epoch: 417, Loss: 16.0875, Train: 0.9441, Val: 0.9430, Test: 0.9461
Epoch: 418, Loss: 10.9280, Train: 0.9278, Val: 0.9267, Test: 0.9353
Epoch: 419, Loss: 7.9483, Train: 0.9297, Val: 0.9261, Test: 0.9315
Epoch: 420, Loss: 4.3815, Train: 0.9375, Val: 0.9368, Test: 0.9442
Epoch: 421, Loss: 12.4282, Train: 0.9336, Val: 0.9353, Test: 0.9417
Epoch: 422, Loss: 7.5163, Train: 0.9230, Val: 0.9227, Test: 0.9264
Epoch: 423, Loss: 5.7556, Train: 0.9310, Val: 0.9298, Test: 0.9359
Epoch: 424, Loss: 8.6709, Train: 0.9378, Val: 0.9396, Test: 0.9417
Epoch: 425, Loss: 4.7705, Train: 0.9320, Val: 0.9307, Test: 0.9362
Epoch: 426, Loss: 6.9273, Train: 0.9351, Val: 0.9331, Test: 0.9426
Epoch: 427, Loss: 5.7672, Train: 0.9345, Val: 0.9344, Test: 0.9407
Epoch: 428, Loss: 3.1203, Train: 0.9302, Val: 0.9292, Test: 0.9378
Epoch: 429, Loss: 3.0835, Train: 0.9341, Val: 0.9319, Test: 0.9439
Epoch: 430, Loss: 4.9888, Train: 0.9366, Val: 0.9365, Test: 0.9404
Epoch: 431, Loss: 2.3162, Train: 0.9309, Val: 0.9285, Test: 0.9362
Epoch: 432, Loss: 3.0797, Train: 0.9297, Val: 0.9276, Test: 0.9356
Epoch: 433, Loss: 4.9866, Train: 0.9345, Val: 0.9338, Test: 0.9398
Epoch: 434, Loss: 6.8047, Train: 0.9313, Val: 0.9301, Test: 0.9391
Epoch: 435, Loss: 5.8637, Train: 0.9268, Val: 0.9239, Test: 0.9334
Epoch: 436, Loss: 4.4204, Train: 0.9336, Val: 0.9313, Test: 0.9394
Epoch: 437, Loss: 3.0999, Train: 0.9382, Val: 0.9387, Test: 0.9439
Epoch: 438, Loss: 40.3512, Train: 0.9288, Val: 0.9264, Test: 0.9340
Epoch: 439, Loss: 9.3897, Train: 0.9343, Val: 0.9319, Test: 0.9404
Epoch: 440, Loss: 5.8533, Train: 0.9365, Val: 0.9377, Test: 0.9410
Epoch: 441, Loss: 5.0022, Train: 0.9290, Val: 0.9270, Test: 0.9340
Epoch: 442, Loss: 13.1438, Train: 0.9307, Val: 0.9292, Test: 0.9362
Epoch: 443, Loss: 4.4857, Train: 0.9337, Val: 0.9316, Test: 0.9398
Epoch: 444, Loss: 7.2525, Train: 0.9337, Val: 0.9350, Test: 0.9401
Epoch: 445, Loss: 5.9247, Train: 0.9332, Val: 0.9328, Test: 0.9388
Epoch: 446, Loss: 2.8327, Train: 0.9314, Val: 0.9301, Test: 0.9372
Epoch: 447, Loss: 6.2790, Train: 0.9331, Val: 0.9310, Test: 0.9439
Epoch: 448, Loss: 4.6983, Train: 0.9381, Val: 0.9381, Test: 0.9385
Epoch: 449, Loss: 8.2435, Train: 0.9233, Val: 0.9209, Test: 0.9296
Epoch: 450, Loss: 3.7408, Train: 0.9217, Val: 0.9197, Test: 0.9260
Epoch: 451, Loss: 4.1331, Train: 0.9331, Val: 0.9331, Test: 0.9394
Epoch: 452, Loss: 3.4887, Train: 0.9339, Val: 0.9328, Test: 0.9394
Epoch: 453, Loss: 4.9445, Train: 0.9312, Val: 0.9307, Test: 0.9340
Epoch: 454, Loss: 8.2155, Train: 0.9350, Val: 0.9341, Test: 0.9417
Epoch: 455, Loss: 5.2556, Train: 0.9400, Val: 0.9390, Test: 0.9461
Epoch: 456, Loss: 2.2640, Train: 0.9326, Val: 0.9285, Test: 0.9378
Epoch: 457, Loss: 3.5135, Train: 0.9312, Val: 0.9298, Test: 0.9366
Epoch: 458, Loss: 3.8172, Train: 0.9400, Val: 0.9368, Test: 0.9471
Epoch: 459, Loss: 3.8020, Train: 0.9384, Val: 0.9384, Test: 0.9455
Epoch: 460, Loss: 3.6643, Train: 0.9330, Val: 0.9304, Test: 0.9382
Epoch: 461, Loss: 3.8993, Train: 0.9329, Val: 0.9307, Test: 0.9388
Epoch: 462, Loss: 2.7148, Train: 0.9346, Val: 0.9347, Test: 0.9407
Epoch: 463, Loss: 2.9036, Train: 0.9360, Val: 0.9350, Test: 0.9420
Epoch: 464, Loss: 2.9482, Train: 0.9349, Val: 0.9322, Test: 0.9420
Epoch: 465, Loss: 4.0579, Train: 0.9322, Val: 0.9301, Test: 0.9404
Epoch: 466, Loss: 4.0575, Train: 0.9296, Val: 0.9279, Test: 0.9356
Epoch: 467, Loss: 3.8668, Train: 0.9324, Val: 0.9307, Test: 0.9372
Epoch: 468, Loss: 5.5337, Train: 0.9343, Val: 0.9319, Test: 0.9417
Epoch: 469, Loss: 1.9245, Train: 0.9320, Val: 0.9292, Test: 0.9385
Epoch: 470, Loss: 4.7040, Train: 0.9315, Val: 0.9304, Test: 0.9369
Epoch: 471, Loss: 4.5630, Train: 0.9354, Val: 0.9365, Test: 0.9407
Epoch: 472, Loss: 3.6754, Train: 0.9345, Val: 0.9316, Test: 0.9407
Epoch: 473, Loss: 3.5433, Train: 0.9339, Val: 0.9310, Test: 0.9404
Epoch: 474, Loss: 6.3667, Train: 0.9367, Val: 0.9347, Test: 0.9433
Epoch: 475, Loss: 2.9357, Train: 0.9337, Val: 0.9316, Test: 0.9398
Epoch: 476, Loss: 2.8000, Train: 0.9346, Val: 0.9319, Test: 0.9413
Epoch: 477, Loss: 2.9117, Train: 0.9269, Val: 0.9258, Test: 0.9340
Epoch: 478, Loss: 3.7009, Train: 0.9257, Val: 0.9218, Test: 0.9296
Epoch: 479, Loss: 2.3853, Train: 0.9344, Val: 0.9316, Test: 0.9388
Epoch: 480, Loss: 4.3563, Train: 0.9345, Val: 0.9331, Test: 0.9372
Epoch: 481, Loss: 3.0100, Train: 0.9261, Val: 0.9221, Test: 0.9305
Epoch: 482, Loss: 3.0118, Train: 0.9292, Val: 0.9276, Test: 0.9334
Epoch: 483, Loss: 2.6141, Train: 0.9392, Val: 0.9405, Test: 0.9436
Epoch: 484, Loss: 2.9857, Train: 0.9310, Val: 0.9276, Test: 0.9362
Epoch: 485, Loss: 4.4152, Train: 0.9291, Val: 0.9270, Test: 0.9340
Epoch: 486, Loss: 2.2331, Train: 0.9379, Val: 0.9353, Test: 0.9439
Epoch: 487, Loss: 4.4147, Train: 0.9389, Val: 0.9387, Test: 0.9439
Epoch: 488, Loss: 6.0278, Train: 0.9298, Val: 0.9276, Test: 0.9353
Epoch: 489, Loss: 6.3422, Train: 0.9314, Val: 0.9279, Test: 0.9388
Epoch: 490, Loss: 5.6495, Train: 0.9349, Val: 0.9325, Test: 0.9394
Epoch: 491, Loss: 3.6880, Train: 0.9366, Val: 0.9359, Test: 0.9407
Epoch: 492, Loss: 2.9486, Train: 0.9332, Val: 0.9316, Test: 0.9391
Epoch: 493, Loss: 4.1553, Train: 0.9298, Val: 0.9279, Test: 0.9343
Epoch: 494, Loss: 2.0423, Train: 0.9341, Val: 0.9319, Test: 0.9394
Epoch: 495, Loss: 4.4833, Train: 0.9339, Val: 0.9319, Test: 0.9398
Epoch: 496, Loss: 2.0026, Train: 0.9317, Val: 0.9285, Test: 0.9362
Epoch: 497, Loss: 2.1199, Train: 0.9350, Val: 0.9335, Test: 0.9407
Epoch: 498, Loss: 2.5012, Train: 0.9355, Val: 0.9344, Test: 0.9382
Epoch: 499, Loss: 1.5292, Train: 0.9334, Val: 0.9310, Test: 0.9385
Epoch: 500, Loss: 2.7030, Train: 0.9358, Val: 0.9322, Test: 0.9404
Epoch: 501, Loss: 3.0381, Train: 0.9371, Val: 0.9353, Test: 0.9433
Epoch: 502, Loss: 1.5708, Train: 0.9354, Val: 0.9335, Test: 0.9407
Epoch: 503, Loss: 1.4767, Train: 0.9346, Val: 0.9328, Test: 0.9401
Epoch: 504, Loss: 11.9040, Train: 0.9319, Val: 0.9289, Test: 0.9353
Epoch: 505, Loss: 19.6759, Train: 0.9496, Val: 0.9482, Test: 0.9525
Epoch: 506, Loss: 69.5573, Train: 0.9160, Val: 0.9154, Test: 0.9190
Epoch: 507, Loss: 420.1362, Train: 0.9224, Val: 0.9209, Test: 0.9292
Epoch: 508, Loss: 185.9859, Train: 0.9135, Val: 0.9092, Test: 0.9152
Epoch: 509, Loss: 217.5991, Train: 0.9177, Val: 0.9144, Test: 0.9222
Epoch: 510, Loss: 61.7806, Train: 0.9259, Val: 0.9224, Test: 0.9315
Epoch: 511, Loss: 76.6703, Train: 0.9283, Val: 0.9243, Test: 0.9343
Epoch: 512, Loss: 52.0573, Train: 0.9287, Val: 0.9261, Test: 0.9337
Epoch: 513, Loss: 13.8260, Train: 0.9336, Val: 0.9310, Test: 0.9353
Epoch: 514, Loss: 61.3785, Train: 0.9290, Val: 0.9252, Test: 0.9324
Epoch: 515, Loss: 78.5026, Train: 0.9281, Val: 0.9252, Test: 0.9331
Epoch: 516, Loss: 79.6903, Train: 0.9225, Val: 0.9200, Test: 0.9267
Epoch: 517, Loss: 35.9303, Train: 0.9292, Val: 0.9267, Test: 0.9350
Epoch: 518, Loss: 28.9152, Train: 0.9285, Val: 0.9264, Test: 0.9340
Epoch: 519, Loss: 45.3679, Train: 0.9349, Val: 0.9313, Test: 0.9388
Epoch: 520, Loss: 18.0287, Train: 0.9397, Val: 0.9365, Test: 0.9458
Epoch: 521, Loss: 20.7736, Train: 0.9355, Val: 0.9328, Test: 0.9407
Epoch: 522, Loss: 24.6316, Train: 0.9314, Val: 0.9307, Test: 0.9372
Epoch: 523, Loss: 23.0372, Train: 0.9278, Val: 0.9267, Test: 0.9321
Epoch: 524, Loss: 12.0524, Train: 0.9249, Val: 0.9243, Test: 0.9311
Epoch: 525, Loss: 16.4778, Train: 0.9526, Val: 0.9500, Test: 0.9560
Epoch: 526, Loss: 6.9547, Train: 0.9330, Val: 0.9307, Test: 0.9375
Epoch: 527, Loss: 16.2029, Train: 0.9337, Val: 0.9319, Test: 0.9372
Epoch: 528, Loss: 13.2672, Train: 0.9367, Val: 0.9331, Test: 0.9417
Epoch: 529, Loss: 16.3005, Train: 0.9377, Val: 0.9353, Test: 0.9436
Epoch: 530, Loss: 15.8511, Train: 0.9286, Val: 0.9267, Test: 0.9359
Epoch: 531, Loss: 5.5223, Train: 0.9280, Val: 0.9273, Test: 0.9327
Epoch: 532, Loss: 6.3572, Train: 0.9358, Val: 0.9341, Test: 0.9410
Epoch: 533, Loss: 7.9624, Train: 0.9206, Val: 0.9181, Test: 0.9206
Epoch: 534, Loss: 5.8280, Train: 0.9141, Val: 0.9141, Test: 0.9158
Epoch: 535, Loss: 3.7128, Train: 0.9115, Val: 0.9141, Test: 0.9158
Epoch: 536, Loss: 14.7306, Train: 0.9230, Val: 0.9172, Test: 0.9270
Epoch: 537, Loss: 10.0542, Train: 0.9318, Val: 0.9276, Test: 0.9398
Epoch: 538, Loss: 7.6659, Train: 0.9213, Val: 0.9203, Test: 0.9292
Epoch: 539, Loss: 5.7300, Train: 0.9192, Val: 0.9184, Test: 0.9254
Epoch: 540, Loss: 10.3633, Train: 0.9244, Val: 0.9221, Test: 0.9331
Epoch: 541, Loss: 4.7549, Train: 0.9276, Val: 0.9261, Test: 0.9359
Epoch: 542, Loss: 5.7904, Train: 0.9288, Val: 0.9267, Test: 0.9343
Epoch: 543, Loss: 3.5028, Train: 0.9271, Val: 0.9255, Test: 0.9318
Epoch: 544, Loss: 6.8777, Train: 0.9345, Val: 0.9292, Test: 0.9404
Epoch: 545, Loss: 8.3834, Train: 0.9369, Val: 0.9353, Test: 0.9423
Epoch: 546, Loss: 4.4796, Train: 0.9342, Val: 0.9301, Test: 0.9423
Epoch: 547, Loss: 6.7481, Train: 0.9251, Val: 0.9233, Test: 0.9305
Epoch: 548, Loss: 5.1772, Train: 0.9291, Val: 0.9279, Test: 0.9382
Epoch: 549, Loss: 7.9758, Train: 0.9374, Val: 0.9359, Test: 0.9442
Epoch: 550, Loss: 5.4233, Train: 0.9367, Val: 0.9338, Test: 0.9442
Epoch: 551, Loss: 9.5489, Train: 0.9328, Val: 0.9319, Test: 0.9391
Epoch: 552, Loss: 7.8058, Train: 0.9284, Val: 0.9258, Test: 0.9296
Epoch: 553, Loss: 5.1430, Train: 0.9253, Val: 0.9227, Test: 0.9305
Epoch: 554, Loss: 11.5375, Train: 0.9137, Val: 0.9132, Test: 0.9197
Epoch: 555, Loss: 23.1222, Train: 0.9330, Val: 0.9344, Test: 0.9372
Epoch: 556, Loss: 18.3640, Train: 0.9385, Val: 0.9359, Test: 0.9426
Epoch: 557, Loss: 7.3415, Train: 0.9377, Val: 0.9338, Test: 0.9413
Epoch: 558, Loss: 10.2142, Train: 0.9409, Val: 0.9393, Test: 0.9461
Epoch: 559, Loss: 4.4297, Train: 0.9402, Val: 0.9368, Test: 0.9449
Epoch: 560, Loss: 4.0659, Train: 0.9367, Val: 0.9319, Test: 0.9388
Epoch: 561, Loss: 7.7309, Train: 0.9379, Val: 0.9347, Test: 0.9423
Epoch: 562, Loss: 7.6412, Train: 0.9396, Val: 0.9356, Test: 0.9458
Epoch: 563, Loss: 11.3657, Train: 0.9362, Val: 0.9350, Test: 0.9404
Epoch: 564, Loss: 2.4905, Train: 0.9345, Val: 0.9319, Test: 0.9391
Epoch: 565, Loss: 21.2968, Train: 0.9293, Val: 0.9258, Test: 0.9353
Epoch: 566, Loss: 13.2338, Train: 0.9384, Val: 0.9387, Test: 0.9426
Epoch: 567, Loss: 11.8612, Train: 0.9333, Val: 0.9298, Test: 0.9391
Epoch: 568, Loss: 4.4698, Train: 0.9313, Val: 0.9282, Test: 0.9356
Epoch: 569, Loss: 6.2684, Train: 0.9342, Val: 0.9319, Test: 0.9410
Epoch: 570, Loss: 9.6438, Train: 0.9369, Val: 0.9387, Test: 0.9452
Epoch: 571, Loss: 7.0411, Train: 0.9404, Val: 0.9368, Test: 0.9480
Epoch: 572, Loss: 4.6983, Train: 0.9354, Val: 0.9335, Test: 0.9401
Epoch: 573, Loss: 3.6057, Train: 0.9324, Val: 0.9298, Test: 0.9375
Epoch: 574, Loss: 2.5373, Train: 0.9328, Val: 0.9289, Test: 0.9375
Epoch: 575, Loss: 4.0671, Train: 0.9391, Val: 0.9356, Test: 0.9439
Epoch: 576, Loss: 8.4890, Train: 0.9341, Val: 0.9298, Test: 0.9404
Epoch: 577, Loss: 4.8497, Train: 0.9277, Val: 0.9261, Test: 0.9356
Epoch: 578, Loss: 4.9731, Train: 0.9382, Val: 0.9377, Test: 0.9413
Epoch: 579, Loss: 8.2264, Train: 0.9284, Val: 0.9273, Test: 0.9337
Epoch: 580, Loss: 3.9405, Train: 0.9306, Val: 0.9279, Test: 0.9366
Epoch: 581, Loss: 7.2493, Train: 0.9432, Val: 0.9439, Test: 0.9474
Epoch: 582, Loss: 8.0507, Train: 0.9379, Val: 0.9353, Test: 0.9442
Epoch: 583, Loss: 5.0533, Train: 0.9304, Val: 0.9249, Test: 0.9343
Epoch: 584, Loss: 2.3394, Train: 0.9391, Val: 0.9368, Test: 0.9423
Epoch: 585, Loss: 4.6020, Train: 0.9387, Val: 0.9371, Test: 0.9442
Epoch: 586, Loss: 4.7066, Train: 0.9336, Val: 0.9310, Test: 0.9372
Epoch: 587, Loss: 5.3997, Train: 0.9379, Val: 0.9350, Test: 0.9442
Epoch: 588, Loss: 4.5232, Train: 0.9425, Val: 0.9396, Test: 0.9471
Epoch: 589, Loss: 3.1550, Train: 0.9362, Val: 0.9341, Test: 0.9420
Epoch: 590, Loss: 4.1021, Train: 0.9347, Val: 0.9325, Test: 0.9410
Epoch: 591, Loss: 3.3167, Train: 0.9418, Val: 0.9387, Test: 0.9484
Epoch: 592, Loss: 4.7155, Train: 0.9428, Val: 0.9445, Test: 0.9464
Epoch: 593, Loss: 9.2604, Train: 0.9343, Val: 0.9328, Test: 0.9398
Epoch: 594, Loss: 2.6448, Train: 0.9312, Val: 0.9292, Test: 0.9353
Epoch: 595, Loss: 9.7809, Train: 0.9354, Val: 0.9325, Test: 0.9394
Epoch: 596, Loss: 4.0713, Train: 0.9382, Val: 0.9356, Test: 0.9455
Epoch: 597, Loss: 2.9158, Train: 0.9365, Val: 0.9344, Test: 0.9407
Epoch: 598, Loss: 2.4975, Train: 0.9338, Val: 0.9304, Test: 0.9394
Epoch: 599, Loss: 3.3521, Train: 0.9370, Val: 0.9338, Test: 0.9429
Epoch: 600, Loss: 2.2016, Train: 0.9405, Val: 0.9390, Test: 0.9458
Epoch: 601, Loss: 4.2111, Train: 0.9375, Val: 0.9362, Test: 0.9442
Epoch: 602, Loss: 1.7982, Train: 0.9362, Val: 0.9335, Test: 0.9445
Epoch: 603, Loss: 6.7093, Train: 0.9392, Val: 0.9362, Test: 0.9468
Epoch: 604, Loss: 3.2315, Train: 0.9386, Val: 0.9381, Test: 0.9426
Epoch: 605, Loss: 5.6620, Train: 0.9325, Val: 0.9316, Test: 0.9394
Epoch: 606, Loss: 1.7625, Train: 0.9289, Val: 0.9267, Test: 0.9347
Epoch: 607, Loss: 2.3309, Train: 0.9340, Val: 0.9316, Test: 0.9391
Epoch: 608, Loss: 4.3948, Train: 0.9350, Val: 0.9328, Test: 0.9413
Epoch: 609, Loss: 2.3561, Train: 0.9380, Val: 0.9356, Test: 0.9436
Epoch: 610, Loss: 1.8173, Train: 0.9357, Val: 0.9331, Test: 0.9391
Epoch: 611, Loss: 2.4710, Train: 0.9341, Val: 0.9322, Test: 0.9382
Epoch: 612, Loss: 6.3142, Train: 0.9375, Val: 0.9353, Test: 0.9429
Epoch: 613, Loss: 3.7096, Train: 0.9390, Val: 0.9356, Test: 0.9452
Epoch: 614, Loss: 3.0555, Train: 0.9379, Val: 0.9347, Test: 0.9442
Epoch: 615, Loss: 2.8400, Train: 0.9355, Val: 0.9328, Test: 0.9420
Epoch: 616, Loss: 2.0180, Train: 0.9379, Val: 0.9353, Test: 0.9423
Epoch: 617, Loss: 2.9478, Train: 0.9387, Val: 0.9371, Test: 0.9442
Epoch: 618, Loss: 5.9686, Train: 0.9381, Val: 0.9356, Test: 0.9439
Epoch: 619, Loss: 3.6260, Train: 0.9360, Val: 0.9344, Test: 0.9398
Epoch: 620, Loss: 5.0197, Train: 0.9305, Val: 0.9273, Test: 0.9350
Epoch: 621, Loss: 7.0246, Train: 0.9341, Val: 0.9307, Test: 0.9391
Epoch: 622, Loss: 10.4551, Train: 0.9585, Val: 0.9565, Test: 0.9624
Epoch: 623, Loss: 3.4033, Train: 0.9315, Val: 0.9276, Test: 0.9356
Epoch: 624, Loss: 8.1318, Train: 0.9268, Val: 0.9236, Test: 0.9311
Epoch: 625, Loss: 3.6326, Train: 0.9345, Val: 0.9304, Test: 0.9398
Epoch: 626, Loss: 5.4700, Train: 0.9350, Val: 0.9298, Test: 0.9382
Epoch: 627, Loss: 5.9824, Train: 0.9375, Val: 0.9328, Test: 0.9436
Epoch: 628, Loss: 6.1368, Train: 0.9401, Val: 0.9390, Test: 0.9452
Epoch: 629, Loss: 3.3866, Train: 0.9362, Val: 0.9328, Test: 0.9417
Epoch: 630, Loss: 2.3965, Train: 0.9360, Val: 0.9316, Test: 0.9398
Epoch: 631, Loss: 1.5101, Train: 0.9413, Val: 0.9365, Test: 0.9464
Epoch: 632, Loss: 3.6408, Train: 0.9409, Val: 0.9381, Test: 0.9464
Epoch: 633, Loss: 3.2011, Train: 0.9367, Val: 0.9335, Test: 0.9420
Epoch: 634, Loss: 3.1571, Train: 0.9343, Val: 0.9310, Test: 0.9407
Epoch: 635, Loss: 1.9933, Train: 0.9359, Val: 0.9353, Test: 0.9401
Epoch: 636, Loss: 1.4494, Train: 0.9354, Val: 0.9322, Test: 0.9426
Epoch: 637, Loss: 2.2250, Train: 0.9381, Val: 0.9338, Test: 0.9442
Epoch: 638, Loss: 2.9446, Train: 0.9428, Val: 0.9402, Test: 0.9484
Epoch: 639, Loss: 2.8876, Train: 0.9394, Val: 0.9353, Test: 0.9455
Epoch: 640, Loss: 10.2845, Train: 0.9371, Val: 0.9331, Test: 0.9445
Epoch: 641, Loss: 21.8019, Train: 0.9529, Val: 0.9509, Test: 0.9573
Epoch: 642, Loss: 7.7336, Train: 0.9309, Val: 0.9279, Test: 0.9356
Epoch: 643, Loss: 5.0517, Train: 0.9295, Val: 0.9255, Test: 0.9308
Epoch: 644, Loss: 7.6597, Train: 0.9037, Val: 0.9028, Test: 0.9085
Epoch: 645, Loss: 4.5943, Train: 0.9370, Val: 0.9365, Test: 0.9413
Epoch: 646, Loss: 5.8149, Train: 0.9274, Val: 0.9255, Test: 0.9340
Epoch: 647, Loss: 4.4552, Train: 0.9251, Val: 0.9209, Test: 0.9321
Epoch: 648, Loss: 4.6034, Train: 0.9365, Val: 0.9350, Test: 0.9410
Epoch: 649, Loss: 5.5661, Train: 0.9436, Val: 0.9408, Test: 0.9484
Epoch: 650, Loss: 4.0468, Train: 0.9417, Val: 0.9374, Test: 0.9474
Epoch: 651, Loss: 12.5045, Train: 0.9378, Val: 0.9335, Test: 0.9436
Epoch: 652, Loss: 3.9257, Train: 0.9370, Val: 0.9377, Test: 0.9413
Epoch: 653, Loss: 3.7711, Train: 0.9312, Val: 0.9279, Test: 0.9366
Epoch: 654, Loss: 1.0999, Train: 0.9369, Val: 0.9359, Test: 0.9420
Epoch: 655, Loss: 4.1816, Train: 0.9389, Val: 0.9390, Test: 0.9426
Epoch: 656, Loss: 8.3770, Train: 0.9370, Val: 0.9341, Test: 0.9433
Epoch: 657, Loss: 1.9168, Train: 0.9417, Val: 0.9384, Test: 0.9487
Epoch: 658, Loss: 2.8391, Train: 0.9383, Val: 0.9402, Test: 0.9423
Epoch: 659, Loss: 1.6626, Train: 0.9299, Val: 0.9261, Test: 0.9337
Epoch: 660, Loss: 2.6946, Train: 0.9381, Val: 0.9347, Test: 0.9426
Epoch: 661, Loss: 2.1328, Train: 0.9384, Val: 0.9374, Test: 0.9442
Epoch: 662, Loss: 1.7944, Train: 0.9389, Val: 0.9356, Test: 0.9455
Epoch: 663, Loss: 1.4138, Train: 0.9352, Val: 0.9350, Test: 0.9401
Epoch: 664, Loss: 1.7664, Train: 0.9328, Val: 0.9316, Test: 0.9385
Epoch: 665, Loss: 0.7768, Train: 0.9357, Val: 0.9335, Test: 0.9417
Epoch: 666, Loss: 2.2416, Train: 0.9411, Val: 0.9393, Test: 0.9471
Epoch: 667, Loss: 1.6593, Train: 0.9404, Val: 0.9390, Test: 0.9480
Epoch: 668, Loss: 2.0874, Train: 0.9387, Val: 0.9353, Test: 0.9449
Epoch: 669, Loss: 2.7683, Train: 0.9413, Val: 0.9384, Test: 0.9477
Epoch: 670, Loss: 2.7152, Train: 0.9435, Val: 0.9423, Test: 0.9493
Epoch: 671, Loss: 4.0134, Train: 0.9383, Val: 0.9356, Test: 0.9445
Epoch: 672, Loss: 2.2490, Train: 0.9375, Val: 0.9328, Test: 0.9413
Epoch: 673, Loss: 7.1900, Train: 0.9391, Val: 0.9347, Test: 0.9439
Epoch: 674, Loss: 6.6914, Train: 0.9381, Val: 0.9371, Test: 0.9420
Epoch: 675, Loss: 2.8274, Train: 0.9311, Val: 0.9270, Test: 0.9366
Epoch: 676, Loss: 1.1882, Train: 0.9349, Val: 0.9307, Test: 0.9391
Epoch: 677, Loss: 2.6246, Train: 0.9393, Val: 0.9381, Test: 0.9426
Epoch: 678, Loss: 8.3305, Train: 0.9321, Val: 0.9313, Test: 0.9366
Epoch: 679, Loss: 15.4851, Train: 0.9334, Val: 0.9313, Test: 0.9359
Epoch: 680, Loss: 4.1935, Train: 0.9258, Val: 0.9224, Test: 0.9292
Epoch: 681, Loss: 5.8088, Train: 0.9323, Val: 0.9301, Test: 0.9372
Epoch: 682, Loss: 6.5416, Train: 0.9583, Val: 0.9571, Test: 0.9627
Epoch: 683, Loss: 4.0107, Train: 0.9392, Val: 0.9368, Test: 0.9426
Epoch: 684, Loss: 11.5974, Train: 0.9362, Val: 0.9335, Test: 0.9401
Epoch: 685, Loss: 4.5843, Train: 0.9377, Val: 0.9347, Test: 0.9410
Epoch: 686, Loss: 14.9297, Train: 0.9306, Val: 0.9221, Test: 0.9334
Epoch: 687, Loss: 11.6584, Train: 0.9515, Val: 0.9519, Test: 0.9544
Epoch: 688, Loss: 13.2275, Train: 0.9410, Val: 0.9399, Test: 0.9433
Epoch: 689, Loss: 14.7954, Train: 0.9251, Val: 0.9206, Test: 0.9296
Epoch: 690, Loss: 4.5770, Train: 0.9270, Val: 0.9203, Test: 0.9251
Epoch: 691, Loss: 5.0435, Train: 0.9200, Val: 0.9193, Test: 0.9251
Epoch: 692, Loss: 6.4940, Train: 0.9288, Val: 0.9255, Test: 0.9324
Epoch: 693, Loss: 4.9684, Train: 0.9379, Val: 0.9344, Test: 0.9401
Epoch: 694, Loss: 4.3193, Train: 0.9343, Val: 0.9310, Test: 0.9353
Epoch: 695, Loss: 4.3365, Train: 0.9350, Val: 0.9316, Test: 0.9385
Epoch: 696, Loss: 5.1162, Train: 0.9397, Val: 0.9338, Test: 0.9423
Epoch: 697, Loss: 2.8509, Train: 0.9595, Val: 0.9558, Test: 0.9614
Epoch: 698, Loss: 2.5398, Train: 0.9333, Val: 0.9301, Test: 0.9369
Epoch: 699, Loss: 1.6710, Train: 0.9327, Val: 0.9289, Test: 0.9350
Epoch: 700, Loss: 2.0148, Train: 0.9423, Val: 0.9411, Test: 0.9449
Epoch: 701, Loss: 3.0431, Train: 0.9406, Val: 0.9402, Test: 0.9452
Epoch: 702, Loss: 4.2105, Train: 0.9322, Val: 0.9292, Test: 0.9356
Epoch: 703, Loss: 3.8119, Train: 0.9351, Val: 0.9319, Test: 0.9410
Epoch: 704, Loss: 3.0475, Train: 0.9445, Val: 0.9420, Test: 0.9477
Epoch: 705, Loss: 3.5407, Train: 0.9417, Val: 0.9390, Test: 0.9458
Epoch: 706, Loss: 2.0998, Train: 0.9368, Val: 0.9319, Test: 0.9407
Epoch: 707, Loss: 1.5649, Train: 0.9392, Val: 0.9393, Test: 0.9420
Epoch: 708, Loss: 2.8991, Train: 0.9384, Val: 0.9374, Test: 0.9413
Epoch: 709, Loss: 3.0795, Train: 0.9386, Val: 0.9341, Test: 0.9436
Epoch: 710, Loss: 1.9055, Train: 0.9428, Val: 0.9417, Test: 0.9484
Epoch: 711, Loss: 4.1239, Train: 0.9428, Val: 0.9405, Test: 0.9480
Epoch: 712, Loss: 4.9067, Train: 0.9415, Val: 0.9387, Test: 0.9480
Epoch: 713, Loss: 2.6450, Train: 0.9418, Val: 0.9374, Test: 0.9487
Epoch: 714, Loss: 4.6993, Train: 0.9448, Val: 0.9451, Test: 0.9522
Epoch: 715, Loss: 3.0303, Train: 0.9373, Val: 0.9359, Test: 0.9436
Epoch: 716, Loss: 2.9805, Train: 0.9382, Val: 0.9356, Test: 0.9439
Epoch: 717, Loss: 3.4992, Train: 0.9436, Val: 0.9420, Test: 0.9493
Epoch: 718, Loss: 2.3837, Train: 0.9409, Val: 0.9387, Test: 0.9464
Epoch: 719, Loss: 2.5654, Train: 0.9358, Val: 0.9328, Test: 0.9404
Epoch: 720, Loss: 5.6703, Train: 0.9365, Val: 0.9313, Test: 0.9429
Epoch: 721, Loss: 3.4934, Train: 0.9600, Val: 0.9561, Test: 0.9665
Epoch: 722, Loss: 6.6898, Train: 0.9320, Val: 0.9313, Test: 0.9378
Epoch: 723, Loss: 2.8394, Train: 0.9271, Val: 0.9230, Test: 0.9324
Epoch: 724, Loss: 3.6606, Train: 0.9320, Val: 0.9261, Test: 0.9407
Epoch: 725, Loss: 5.8121, Train: 0.9551, Val: 0.9525, Test: 0.9595
Epoch: 726, Loss: 1.6097, Train: 0.9352, Val: 0.9341, Test: 0.9426
Epoch: 727, Loss: 2.8114, Train: 0.9350, Val: 0.9319, Test: 0.9401
Epoch: 728, Loss: 1.5611, Train: 0.9395, Val: 0.9374, Test: 0.9452
Epoch: 729, Loss: 1.9295, Train: 0.9415, Val: 0.9405, Test: 0.9480
Epoch: 730, Loss: 2.0409, Train: 0.9396, Val: 0.9362, Test: 0.9461
Epoch: 731, Loss: 1.5474, Train: 0.9389, Val: 0.9368, Test: 0.9445
Epoch: 732, Loss: 3.1103, Train: 0.9371, Val: 0.9316, Test: 0.9439
Epoch: 733, Loss: 2.2438, Train: 0.9389, Val: 0.9350, Test: 0.9445
Epoch: 734, Loss: 2.8801, Train: 0.9410, Val: 0.9371, Test: 0.9449
Epoch: 735, Loss: 2.5756, Train: 0.9389, Val: 0.9362, Test: 0.9442
Epoch: 736, Loss: 1.4391, Train: 0.9375, Val: 0.9350, Test: 0.9442
Epoch: 737, Loss: 1.6169, Train: 0.9403, Val: 0.9384, Test: 0.9455
Epoch: 738, Loss: 4.1089, Train: 0.9413, Val: 0.9387, Test: 0.9480
Epoch: 739, Loss: 1.5285, Train: 0.9401, Val: 0.9359, Test: 0.9455
Epoch: 740, Loss: 4.4835, Train: 0.9455, Val: 0.9460, Test: 0.9506
Epoch: 741, Loss: 2.2872, Train: 0.9361, Val: 0.9338, Test: 0.9420
Epoch: 742, Loss: 1.2487, Train: 0.9371, Val: 0.9325, Test: 0.9455
Epoch: 743, Loss: 1.4220, Train: 0.9449, Val: 0.9427, Test: 0.9515
Epoch: 744, Loss: 2.4122, Train: 0.9568, Val: 0.9558, Test: 0.9586
Epoch: 745, Loss: 1.1923, Train: 0.9367, Val: 0.9347, Test: 0.9429
Epoch: 746, Loss: 1.6353, Train: 0.9386, Val: 0.9353, Test: 0.9449
Epoch: 747, Loss: 1.0592, Train: 0.9430, Val: 0.9423, Test: 0.9474
Epoch: 748, Loss: 2.4182, Train: 0.9420, Val: 0.9393, Test: 0.9477
Epoch: 749, Loss: 2.0490, Train: 0.9406, Val: 0.9368, Test: 0.9461
Epoch: 750, Loss: 1.6510, Train: 0.9422, Val: 0.9405, Test: 0.9471
Epoch: 751, Loss: 2.7790, Train: 0.9408, Val: 0.9377, Test: 0.9458
Epoch: 752, Loss: 2.4124, Train: 0.9399, Val: 0.9374, Test: 0.9436
Epoch: 753, Loss: 1.5171, Train: 0.9380, Val: 0.9374, Test: 0.9436
Epoch: 754, Loss: 0.9375, Train: 0.9369, Val: 0.9350, Test: 0.9410
Epoch: 755, Loss: 1.3555, Train: 0.9393, Val: 0.9384, Test: 0.9452
Epoch: 756, Loss: 3.4379, Train: 0.9414, Val: 0.9381, Test: 0.9484
Epoch: 757, Loss: 3.1621, Train: 0.9403, Val: 0.9353, Test: 0.9464
Epoch: 758, Loss: 1.5354, Train: 0.9403, Val: 0.9396, Test: 0.9461
Epoch: 759, Loss: 3.3373, Train: 0.9335, Val: 0.9310, Test: 0.9401
Epoch: 760, Loss: 6.3730, Train: 0.9347, Val: 0.9310, Test: 0.9420
Epoch: 761, Loss: 1.8797, Train: 0.9444, Val: 0.9451, Test: 0.9484
Epoch: 762, Loss: 1.7905, Train: 0.9368, Val: 0.9356, Test: 0.9404
Epoch: 763, Loss: 1.5558, Train: 0.9363, Val: 0.9325, Test: 0.9410
Epoch: 764, Loss: 2.0203, Train: 0.9391, Val: 0.9387, Test: 0.9417
Epoch: 765, Loss: 2.2584, Train: 0.9379, Val: 0.9377, Test: 0.9410
Epoch: 766, Loss: 2.6297, Train: 0.9319, Val: 0.9295, Test: 0.9343
Epoch: 767, Loss: 1.5541, Train: 0.9396, Val: 0.9356, Test: 0.9464
Epoch: 768, Loss: 1.0241, Train: 0.9574, Val: 0.9574, Test: 0.9576
Epoch: 769, Loss: 2.0717, Train: 0.9371, Val: 0.9335, Test: 0.9407
Epoch: 770, Loss: 2.7686, Train: 0.9377, Val: 0.9344, Test: 0.9426
Epoch: 771, Loss: 1.7993, Train: 0.9404, Val: 0.9414, Test: 0.9426
Epoch: 772, Loss: 1.0022, Train: 0.9377, Val: 0.9328, Test: 0.9404
Epoch: 773, Loss: 1.0898, Train: 0.9393, Val: 0.9353, Test: 0.9439
Epoch: 774, Loss: 1.2947, Train: 0.9395, Val: 0.9371, Test: 0.9436
Epoch: 775, Loss: 1.2531, Train: 0.9340, Val: 0.9310, Test: 0.9394
Epoch: 776, Loss: 1.8942, Train: 0.9516, Val: 0.9497, Test: 0.9557
Epoch: 777, Loss: 1.9810, Train: 0.9292, Val: 0.9267, Test: 0.9324
Epoch: 778, Loss: 2.2210, Train: 0.9299, Val: 0.9270, Test: 0.9362
Epoch: 779, Loss: 3.0080, Train: 0.9413, Val: 0.9387, Test: 0.9458
Epoch: 780, Loss: 2.2730, Train: 0.9403, Val: 0.9381, Test: 0.9442
Epoch: 781, Loss: 1.8734, Train: 0.9290, Val: 0.9246, Test: 0.9350
Epoch: 782, Loss: 1.9335, Train: 0.9379, Val: 0.9331, Test: 0.9417
Epoch: 783, Loss: 1.9711, Train: 0.9423, Val: 0.9396, Test: 0.9471
Epoch: 784, Loss: 2.2105, Train: 0.9378, Val: 0.9325, Test: 0.9436
Epoch: 785, Loss: 1.8555, Train: 0.9387, Val: 0.9371, Test: 0.9442
Epoch: 786, Loss: 2.9470, Train: 0.9379, Val: 0.9359, Test: 0.9413
Epoch: 787, Loss: 2.4362, Train: 0.9371, Val: 0.9347, Test: 0.9410
Epoch: 788, Loss: 5.5162, Train: 0.9391, Val: 0.9335, Test: 0.9449
Epoch: 789, Loss: 2.6330, Train: 0.9421, Val: 0.9405, Test: 0.9449
Epoch: 790, Loss: 1.9241, Train: 0.9365, Val: 0.9350, Test: 0.9413
Epoch: 791, Loss: 1.5476, Train: 0.9341, Val: 0.9307, Test: 0.9385
Epoch: 792, Loss: 1.3781, Train: 0.9393, Val: 0.9371, Test: 0.9445
Epoch: 793, Loss: 2.2468, Train: 0.9385, Val: 0.9365, Test: 0.9436
Epoch: 794, Loss: 1.0224, Train: 0.9331, Val: 0.9313, Test: 0.9413
Epoch: 795, Loss: 1.0160, Train: 0.9360, Val: 0.9316, Test: 0.9404
Epoch: 796, Loss: 1.1490, Train: 0.9410, Val: 0.9384, Test: 0.9464
Epoch: 797, Loss: 1.1950, Train: 0.9342, Val: 0.9328, Test: 0.9401
Epoch: 798, Loss: 2.0912, Train: 0.9315, Val: 0.9292, Test: 0.9353
Epoch: 799, Loss: 2.8504, Train: 0.9366, Val: 0.9350, Test: 0.9413
Epoch: 800, Loss: 1.6985, Train: 0.9423, Val: 0.9402, Test: 0.9455
Epoch: 801, Loss: 1.7672, Train: 0.9338, Val: 0.9310, Test: 0.9391
Epoch: 802, Loss: 1.9656, Train: 0.9377, Val: 0.9325, Test: 0.9417
Epoch: 803, Loss: 1.6546, Train: 0.9458, Val: 0.9442, Test: 0.9506
Epoch: 804, Loss: 2.5398, Train: 0.9392, Val: 0.9371, Test: 0.9445
Epoch: 805, Loss: 1.7222, Train: 0.9367, Val: 0.9344, Test: 0.9398
Epoch: 806, Loss: 1.1499, Train: 0.9374, Val: 0.9350, Test: 0.9410
Epoch: 807, Loss: 0.9155, Train: 0.9348, Val: 0.9279, Test: 0.9404
Epoch: 808, Loss: 3.8596, Train: 0.9376, Val: 0.9341, Test: 0.9429
Epoch: 809, Loss: 0.9675, Train: 0.9464, Val: 0.9476, Test: 0.9512
Epoch: 810, Loss: 0.8306, Train: 0.9393, Val: 0.9365, Test: 0.9429
Epoch: 811, Loss: 3.3765, Train: 0.9408, Val: 0.9368, Test: 0.9468
Epoch: 812, Loss: 1.5929, Train: 0.9454, Val: 0.9442, Test: 0.9503
Epoch: 813, Loss: 1.1501, Train: 0.9392, Val: 0.9381, Test: 0.9429
Epoch: 814, Loss: 2.3789, Train: 0.9332, Val: 0.9316, Test: 0.9388
Epoch: 815, Loss: 1.0674, Train: 0.9300, Val: 0.9264, Test: 0.9366
Epoch: 816, Loss: 2.7321, Train: 0.9333, Val: 0.9289, Test: 0.9394
Epoch: 817, Loss: 1.4852, Train: 0.9444, Val: 0.9433, Test: 0.9464
Epoch: 818, Loss: 1.8327, Train: 0.9355, Val: 0.9328, Test: 0.9378
Epoch: 819, Loss: 1.7133, Train: 0.9343, Val: 0.9307, Test: 0.9401
Epoch: 820, Loss: 5.9509, Train: 0.9399, Val: 0.9362, Test: 0.9445
Epoch: 821, Loss: 0.8563, Train: 0.9419, Val: 0.9402, Test: 0.9464
Epoch: 822, Loss: 2.4388, Train: 0.9310, Val: 0.9270, Test: 0.9385
Epoch: 823, Loss: 1.0847, Train: 0.9327, Val: 0.9304, Test: 0.9375
Epoch: 824, Loss: 2.4699, Train: 0.9347, Val: 0.9313, Test: 0.9382
Epoch: 825, Loss: 1.9415, Train: 0.9350, Val: 0.9310, Test: 0.9433
Epoch: 826, Loss: 1.4503, Train: 0.9371, Val: 0.9344, Test: 0.9458
Epoch: 827, Loss: 3.6141, Train: 0.9394, Val: 0.9402, Test: 0.9426
Epoch: 828, Loss: 2.3324, Train: 0.9326, Val: 0.9279, Test: 0.9372
Epoch: 829, Loss: 1.5150, Train: 0.9351, Val: 0.9301, Test: 0.9382
Epoch: 830, Loss: 3.8150, Train: 0.9598, Val: 0.9555, Test: 0.9640
Epoch: 831, Loss: 2.3465, Train: 0.9423, Val: 0.9402, Test: 0.9455
Epoch: 832, Loss: 1.2743, Train: 0.9351, Val: 0.9325, Test: 0.9398
Epoch: 833, Loss: 1.2225, Train: 0.9326, Val: 0.9298, Test: 0.9375
Epoch: 834, Loss: 1.6057, Train: 0.9352, Val: 0.9325, Test: 0.9404
Epoch: 835, Loss: 1.7628, Train: 0.9370, Val: 0.9335, Test: 0.9423
Epoch: 836, Loss: 1.1020, Train: 0.9427, Val: 0.9405, Test: 0.9455
Epoch: 837, Loss: 1.3217, Train: 0.9389, Val: 0.9359, Test: 0.9429
Epoch: 838, Loss: 1.8212, Train: 0.9371, Val: 0.9338, Test: 0.9410
Epoch: 839, Loss: 1.1907, Train: 0.9401, Val: 0.9368, Test: 0.9436
Epoch: 840, Loss: 0.6046, Train: 0.9403, Val: 0.9353, Test: 0.9436
Epoch: 841, Loss: 2.1688, Train: 0.9384, Val: 0.9338, Test: 0.9423
Epoch: 842, Loss: 0.7857, Train: 0.9410, Val: 0.9384, Test: 0.9449
Epoch: 843, Loss: 2.9033, Train: 0.9393, Val: 0.9365, Test: 0.9433
Epoch: 844, Loss: 1.1158, Train: 0.9393, Val: 0.9350, Test: 0.9439
Epoch: 845, Loss: 0.9206, Train: 0.9431, Val: 0.9384, Test: 0.9493
Epoch: 846, Loss: 1.1756, Train: 0.9420, Val: 0.9377, Test: 0.9477
Epoch: 847, Loss: 1.8775, Train: 0.9409, Val: 0.9359, Test: 0.9471
Epoch: 848, Loss: 0.9750, Train: 0.9414, Val: 0.9381, Test: 0.9468
Epoch: 849, Loss: 1.2103, Train: 0.9443, Val: 0.9408, Test: 0.9500
Epoch: 850, Loss: 1.0714, Train: 0.9412, Val: 0.9368, Test: 0.9468
Epoch: 851, Loss: 0.9495, Train: 0.9275, Val: 0.9236, Test: 0.9334
Epoch: 852, Loss: 2.5223, Train: 0.9177, Val: 0.9138, Test: 0.9197
Epoch: 853, Loss: 1.4920, Train: 0.9206, Val: 0.9163, Test: 0.9225
Epoch: 854, Loss: 1.6348, Train: 0.9531, Val: 0.9506, Test: 0.9566
Epoch: 855, Loss: 1.4020, Train: 0.9305, Val: 0.9258, Test: 0.9353
Epoch: 856, Loss: 2.0715, Train: 0.9245, Val: 0.9190, Test: 0.9273
Epoch: 857, Loss: 1.7522, Train: 0.9331, Val: 0.9273, Test: 0.9394
Epoch: 858, Loss: 3.5125, Train: 0.9368, Val: 0.9328, Test: 0.9420
Epoch: 859, Loss: 1.7939, Train: 0.9328, Val: 0.9292, Test: 0.9359
Epoch: 860, Loss: 2.4501, Train: 0.9348, Val: 0.9310, Test: 0.9391
Epoch: 861, Loss: 1.4196, Train: 0.9349, Val: 0.9313, Test: 0.9420
Epoch: 862, Loss: 1.6556, Train: 0.9333, Val: 0.9264, Test: 0.9385
Epoch: 863, Loss: 1.7323, Train: 0.9373, Val: 0.9331, Test: 0.9458
Epoch: 864, Loss: 1.2708, Train: 0.9326, Val: 0.9289, Test: 0.9394
Epoch: 865, Loss: 2.1083, Train: 0.9300, Val: 0.9252, Test: 0.9366
Epoch: 866, Loss: 1.0975, Train: 0.9413, Val: 0.9377, Test: 0.9468
Epoch: 867, Loss: 1.5588, Train: 0.9426, Val: 0.9390, Test: 0.9480
Epoch: 868, Loss: 1.1868, Train: 0.9394, Val: 0.9353, Test: 0.9468
Epoch: 869, Loss: 1.0493, Train: 0.9385, Val: 0.9356, Test: 0.9436
Epoch: 870, Loss: 2.4111, Train: 0.9413, Val: 0.9384, Test: 0.9477
Epoch: 871, Loss: 0.7344, Train: 0.9438, Val: 0.9411, Test: 0.9484
Epoch: 872, Loss: 0.7228, Train: 0.9422, Val: 0.9393, Test: 0.9468
Epoch: 873, Loss: 2.5610, Train: 0.9383, Val: 0.9313, Test: 0.9452
Epoch: 874, Loss: 1.5607, Train: 0.9408, Val: 0.9381, Test: 0.9471
Epoch: 875, Loss: 1.9447, Train: 0.9401, Val: 0.9377, Test: 0.9452
Epoch: 876, Loss: 0.8197, Train: 0.9327, Val: 0.9292, Test: 0.9388
Epoch: 877, Loss: 0.7813, Train: 0.9354, Val: 0.9310, Test: 0.9407
Epoch: 878, Loss: 1.3248, Train: 0.9434, Val: 0.9408, Test: 0.9484
Epoch: 879, Loss: 0.9832, Train: 0.9418, Val: 0.9362, Test: 0.9461
Epoch: 880, Loss: 2.2952, Train: 0.9393, Val: 0.9356, Test: 0.9452
Epoch: 881, Loss: 2.3674, Train: 0.9396, Val: 0.9350, Test: 0.9445
Epoch: 882, Loss: 1.7463, Train: 0.9383, Val: 0.9353, Test: 0.9442
Epoch: 883, Loss: 2.0647, Train: 0.9388, Val: 0.9356, Test: 0.9433
Epoch: 884, Loss: 1.6844, Train: 0.9387, Val: 0.9362, Test: 0.9426
Epoch: 885, Loss: 2.8944, Train: 0.9327, Val: 0.9307, Test: 0.9388
Epoch: 886, Loss: 1.1328, Train: 0.9338, Val: 0.9307, Test: 0.9382
Epoch: 887, Loss: 4.3457, Train: 0.9411, Val: 0.9365, Test: 0.9464
Epoch: 888, Loss: 1.6543, Train: 0.9437, Val: 0.9411, Test: 0.9484
Epoch: 889, Loss: 1.3112, Train: 0.9422, Val: 0.9390, Test: 0.9477
Epoch: 890, Loss: 1.5889, Train: 0.9370, Val: 0.9341, Test: 0.9401
Epoch: 891, Loss: 1.5785, Train: 0.9355, Val: 0.9307, Test: 0.9391
Epoch: 892, Loss: 0.9868, Train: 0.9357, Val: 0.9319, Test: 0.9385
Epoch: 893, Loss: 1.0365, Train: 0.9369, Val: 0.9328, Test: 0.9417
Epoch: 894, Loss: 0.8655, Train: 0.9382, Val: 0.9341, Test: 0.9420
Epoch: 895, Loss: 1.5851, Train: 0.9370, Val: 0.9328, Test: 0.9420
Epoch: 896, Loss: 1.1744, Train: 0.9374, Val: 0.9335, Test: 0.9423
Epoch: 897, Loss: 1.6050, Train: 0.9431, Val: 0.9377, Test: 0.9468
Epoch: 898, Loss: 0.9439, Train: 0.9462, Val: 0.9445, Test: 0.9500
Epoch: 899, Loss: 1.5772, Train: 0.9418, Val: 0.9377, Test: 0.9464
Epoch: 900, Loss: 1.4100, Train: 0.9393, Val: 0.9347, Test: 0.9445
Epoch: 901, Loss: 1.3424, Train: 0.9374, Val: 0.9365, Test: 0.9407
Epoch: 902, Loss: 1.8043, Train: 0.9367, Val: 0.9310, Test: 0.9429
Epoch: 903, Loss: 1.1995, Train: 0.9393, Val: 0.9362, Test: 0.9461
Epoch: 904, Loss: 1.3325, Train: 0.9467, Val: 0.9448, Test: 0.9500
Epoch: 905, Loss: 2.1093, Train: 0.9369, Val: 0.9335, Test: 0.9439
Epoch: 906, Loss: 1.9711, Train: 0.9328, Val: 0.9313, Test: 0.9398
Epoch: 907, Loss: 2.1153, Train: 0.9396, Val: 0.9362, Test: 0.9449
Epoch: 908, Loss: 2.1222, Train: 0.9448, Val: 0.9427, Test: 0.9484
Epoch: 909, Loss: 1.2055, Train: 0.9355, Val: 0.9331, Test: 0.9398
Epoch: 910, Loss: 1.4591, Train: 0.9394, Val: 0.9335, Test: 0.9429
Epoch: 911, Loss: 1.4426, Train: 0.9447, Val: 0.9408, Test: 0.9490
Epoch: 912, Loss: 1.5560, Train: 0.9437, Val: 0.9408, Test: 0.9480
Epoch: 913, Loss: 1.5230, Train: 0.9394, Val: 0.9335, Test: 0.9442
Epoch: 914, Loss: 0.7735, Train: 0.9382, Val: 0.9368, Test: 0.9407
Epoch: 915, Loss: 2.7376, Train: 0.9377, Val: 0.9362, Test: 0.9404
Epoch: 916, Loss: 1.0899, Train: 0.9357, Val: 0.9304, Test: 0.9391
Epoch: 917, Loss: 1.6322, Train: 0.9337, Val: 0.9319, Test: 0.9388
Epoch: 918, Loss: 1.9729, Train: 0.9582, Val: 0.9583, Test: 0.9611
Epoch: 919, Loss: 1.5734, Train: 0.9328, Val: 0.9295, Test: 0.9378
Epoch: 920, Loss: 4.8332, Train: 0.9271, Val: 0.9203, Test: 0.9331
Epoch: 921, Loss: 1.5873, Train: 0.9466, Val: 0.9454, Test: 0.9512
Epoch: 922, Loss: 2.0270, Train: 0.9534, Val: 0.9515, Test: 0.9566
Epoch: 923, Loss: 0.7980, Train: 0.9283, Val: 0.9243, Test: 0.9299
Epoch: 924, Loss: 2.4950, Train: 0.9331, Val: 0.9285, Test: 0.9356
Epoch: 925, Loss: 1.3828, Train: 0.9601, Val: 0.9592, Test: 0.9637
Epoch: 926, Loss: 1.5738, Train: 0.9362, Val: 0.9359, Test: 0.9404
Epoch: 927, Loss: 2.4242, Train: 0.9010, Val: 0.9056, Test: 0.9063
Epoch: 928, Loss: 28.1333, Train: 0.9508, Val: 0.9512, Test: 0.9563
Epoch: 929, Loss: 7.4127, Train: 0.9268, Val: 0.9227, Test: 0.9308
Epoch: 930, Loss: 5.9302, Train: 0.9126, Val: 0.9102, Test: 0.9162
Epoch: 931, Loss: 3.5876, Train: 0.9206, Val: 0.9160, Test: 0.9235
Epoch: 932, Loss: 4.4643, Train: 0.9449, Val: 0.9420, Test: 0.9522
Epoch: 933, Loss: 9.3311, Train: 0.9413, Val: 0.9377, Test: 0.9480
Epoch: 934, Loss: 4.5407, Train: 0.9286, Val: 0.9252, Test: 0.9337
Epoch: 935, Loss: 3.7576, Train: 0.9314, Val: 0.9289, Test: 0.9353
Epoch: 936, Loss: 2.1183, Train: 0.9403, Val: 0.9387, Test: 0.9461
Epoch: 937, Loss: 1.8060, Train: 0.9402, Val: 0.9374, Test: 0.9455
Epoch: 938, Loss: 1.8727, Train: 0.9376, Val: 0.9347, Test: 0.9433
Epoch: 939, Loss: 1.5537, Train: 0.9373, Val: 0.9338, Test: 0.9413
Epoch: 940, Loss: 3.0688, Train: 0.9389, Val: 0.9347, Test: 0.9433
Epoch: 941, Loss: 1.5518, Train: 0.9377, Val: 0.9356, Test: 0.9436
Epoch: 942, Loss: 3.0328, Train: 0.9400, Val: 0.9384, Test: 0.9458
Epoch: 943, Loss: 2.1068, Train: 0.9398, Val: 0.9374, Test: 0.9436
Epoch: 944, Loss: 2.7315, Train: 0.9396, Val: 0.9344, Test: 0.9449
Epoch: 945, Loss: 1.2843, Train: 0.9381, Val: 0.9338, Test: 0.9436
Epoch: 946, Loss: 1.4981, Train: 0.9453, Val: 0.9445, Test: 0.9503
Epoch: 947, Loss: 1.4922, Train: 0.9392, Val: 0.9359, Test: 0.9445
Epoch: 948, Loss: 2.4396, Train: 0.9372, Val: 0.9310, Test: 0.9413
Epoch: 949, Loss: 2.9579, Train: 0.9382, Val: 0.9374, Test: 0.9439
Epoch: 950, Loss: 2.3799, Train: 0.9440, Val: 0.9445, Test: 0.9503
Epoch: 951, Loss: 2.7398, Train: 0.9370, Val: 0.9353, Test: 0.9407
Epoch: 952, Loss: 5.7936, Train: 0.9374, Val: 0.9331, Test: 0.9433
Epoch: 953, Loss: 1.5137, Train: 0.9433, Val: 0.9408, Test: 0.9487
Epoch: 954, Loss: 1.4899, Train: 0.9436, Val: 0.9420, Test: 0.9464
Epoch: 955, Loss: 1.7804, Train: 0.9341, Val: 0.9313, Test: 0.9420
Epoch: 956, Loss: 4.1308, Train: 0.9329, Val: 0.9255, Test: 0.9385
Epoch: 957, Loss: 2.6607, Train: 0.9439, Val: 0.9414, Test: 0.9484
Epoch: 958, Loss: 2.5305, Train: 0.9617, Val: 0.9635, Test: 0.9633
Epoch: 959, Loss: 3.3841, Train: 0.9379, Val: 0.9356, Test: 0.9429
Epoch: 960, Loss: 4.3538, Train: 0.9354, Val: 0.9328, Test: 0.9394
Epoch: 961, Loss: 6.4605, Train: 0.9386, Val: 0.9374, Test: 0.9429
Epoch: 962, Loss: 2.0415, Train: 0.9610, Val: 0.9601, Test: 0.9656
Epoch: 963, Loss: 3.4437, Train: 0.9363, Val: 0.9310, Test: 0.9413
Epoch: 964, Loss: 1.3298, Train: 0.9379, Val: 0.9338, Test: 0.9413
Epoch: 965, Loss: 1.7578, Train: 0.9563, Val: 0.9555, Test: 0.9586
Epoch: 966, Loss: 0.8104, Train: 0.9426, Val: 0.9399, Test: 0.9484
Epoch: 967, Loss: 1.3907, Train: 0.9391, Val: 0.9347, Test: 0.9452
Epoch: 968, Loss: 2.2860, Train: 0.9413, Val: 0.9381, Test: 0.9464
Epoch: 969, Loss: 1.9653, Train: 0.9468, Val: 0.9451, Test: 0.9512
Epoch: 970, Loss: 1.5794, Train: 0.9413, Val: 0.9414, Test: 0.9464
Epoch: 971, Loss: 1.2271, Train: 0.9403, Val: 0.9384, Test: 0.9436
Epoch: 972, Loss: 1.6393, Train: 0.9432, Val: 0.9390, Test: 0.9480
Epoch: 973, Loss: 1.6233, Train: 0.9631, Val: 0.9611, Test: 0.9665
Epoch: 974, Loss: 2.2140, Train: 0.9408, Val: 0.9374, Test: 0.9455
Epoch: 975, Loss: 6.4428, Train: 0.9401, Val: 0.9356, Test: 0.9468
Epoch: 976, Loss: 2.0049, Train: 0.9450, Val: 0.9427, Test: 0.9496
Epoch: 977, Loss: 0.9795, Train: 0.9393, Val: 0.9393, Test: 0.9423
Epoch: 978, Loss: 2.0776, Train: 0.9329, Val: 0.9307, Test: 0.9394
Epoch: 979, Loss: 1.4229, Train: 0.9371, Val: 0.9344, Test: 0.9442
Epoch: 980, Loss: 1.0539, Train: 0.9583, Val: 0.9568, Test: 0.9624
Epoch: 981, Loss: 4.5661, Train: 0.9416, Val: 0.9396, Test: 0.9468
Epoch: 982, Loss: 1.2637, Train: 0.9348, Val: 0.9316, Test: 0.9413
Epoch: 983, Loss: 3.2270, Train: 0.9333, Val: 0.9292, Test: 0.9398
Epoch: 984, Loss: 1.4738, Train: 0.9274, Val: 0.9236, Test: 0.9324
Epoch: 985, Loss: 2.2666, Train: 0.9548, Val: 0.9546, Test: 0.9570
Epoch: 986, Loss: 3.5594, Train: 0.9328, Val: 0.9292, Test: 0.9359
Epoch: 987, Loss: 6.8638, Train: 0.9357, Val: 0.9313, Test: 0.9417
Epoch: 988, Loss: 1.1752, Train: 0.9431, Val: 0.9405, Test: 0.9484
Epoch: 989, Loss: 1.7544, Train: 0.9426, Val: 0.9408, Test: 0.9458
Epoch: 990, Loss: 2.9445, Train: 0.9367, Val: 0.9316, Test: 0.9417
Epoch: 991, Loss: 2.6041, Train: 0.9406, Val: 0.9365, Test: 0.9436
Epoch: 992, Loss: 1.3977, Train: 0.9436, Val: 0.9414, Test: 0.9468
Epoch: 993, Loss: 1.8518, Train: 0.9435, Val: 0.9423, Test: 0.9468
Epoch: 994, Loss: 1.1669, Train: 0.9416, Val: 0.9393, Test: 0.9461
Epoch: 995, Loss: 2.3499, Train: 0.9385, Val: 0.9338, Test: 0.9423
Epoch: 996, Loss: 2.0105, Train: 0.9419, Val: 0.9396, Test: 0.9461
Epoch: 997, Loss: 1.7352, Train: 0.9425, Val: 0.9433, Test: 0.9442
Epoch: 998, Loss: 1.5089, Train: 0.9366, Val: 0.9338, Test: 0.9391
Epoch: 999, Loss: 1.6907, Train: 0.9404, Val: 0.9384, Test: 0.9439

real	116m56.879s
user	556m8.639s
sys	270m44.731s


GraphSAGE supervised
Epoch: 482, Loss: 2.3207
Epoch: 483, Loss: 2.0362
Epoch: 484, Loss: 1.8770
Epoch: 485, Loss: 1.7397
Epoch: 486, Loss: 1.5728
Epoch: 487, Loss: 1.3965
Epoch: 488, Loss: 1.1703
Epoch: 489, Loss: 2.9595
Epoch: 490, Loss: 2.0883
Epoch: 491, Loss: 1.4526
Epoch: 492, Loss: 1.5539
Epoch: 493, Loss: 2.8374
Epoch: 494, Loss: 2.8336
Epoch: 495, Loss: 1.9544
Epoch: 496, Loss: 2.0721
Epoch: 497, Loss: 2.8908
Epoch: 498, Loss: 2.1071
Epoch: 499, Loss: 2.2458
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36376.36it/s]
Epoch: 500, Loss: 2.2315, Train: 0.8932, Val: 0.8880, test: 0.8788
Epoch: 501, Loss: 2.0703
Epoch: 502, Loss: 2.1036
Epoch: 503, Loss: 1.8379
Epoch: 504, Loss: 3.1818
Epoch: 505, Loss: 2.5691
Epoch: 506, Loss: 1.7565
Epoch: 507, Loss: 2.9537
Epoch: 508, Loss: 2.7230
Epoch: 509, Loss: 2.8785
Epoch: 510, Loss: 2.5391
Epoch: 511, Loss: 2.0647
Epoch: 512, Loss: 2.4166
Epoch: 513, Loss: 2.5725
Epoch: 514, Loss: 2.1028
Epoch: 515, Loss: 2.1666
Epoch: 516, Loss: 1.6434
Epoch: 517, Loss: 2.1993
Epoch: 518, Loss: 1.6297
Epoch: 519, Loss: 1.8585
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34077.67it/s]
Epoch: 520, Loss: 2.0331, Train: 0.9117, Val: 0.9094, test: 0.9058
Epoch: 521, Loss: 1.9618
Epoch: 522, Loss: 2.2269
Epoch: 523, Loss: 1.5004
Epoch: 524, Loss: 1.8502
Epoch: 525, Loss: 1.9288
Epoch: 526, Loss: 2.1898
Epoch: 527, Loss: 1.6836
Epoch: 528, Loss: 2.6844
Epoch: 529, Loss: 2.0688
Epoch: 530, Loss: 1.1821
Epoch: 531, Loss: 2.3815
Epoch: 532, Loss: 1.9677
Epoch: 533, Loss: 2.7604
Epoch: 534, Loss: 2.1285
Epoch: 535, Loss: 2.5122
Epoch: 536, Loss: 5.9080
Epoch: 537, Loss: 3.6764
Epoch: 538, Loss: 6.3893
Epoch: 539, Loss: 2.9526
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35408.07it/s]
Epoch: 540, Loss: 5.4179, Train: 0.9136, Val: 0.9109, test: 0.9062
Epoch: 541, Loss: 11.4149
Epoch: 542, Loss: 4.6010
Epoch: 543, Loss: 6.9543
Epoch: 544, Loss: 6.7373
Epoch: 545, Loss: 4.5159
Epoch: 546, Loss: 4.3247
Epoch: 547, Loss: 4.7057
Epoch: 548, Loss: 7.0509
Epoch: 549, Loss: 4.4709
Epoch: 550, Loss: 4.3951
Epoch: 551, Loss: 3.6375
Epoch: 552, Loss: 4.0802
Epoch: 553, Loss: 4.1695
Epoch: 554, Loss: 5.9349
Epoch: 555, Loss: 4.4487
Epoch: 556, Loss: 3.2373
Epoch: 557, Loss: 3.1925
Epoch: 558, Loss: 2.4073
Epoch: 559, Loss: 3.3539
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29658.26it/s]
Epoch: 560, Loss: 4.9678, Train: 0.9160, Val: 0.9140, test: 0.9153
Epoch: 561, Loss: 2.7473
Epoch: 562, Loss: 3.8193
Epoch: 563, Loss: 2.5900
Epoch: 564, Loss: 3.0941
Epoch: 565, Loss: 2.3262
Epoch: 566, Loss: 3.7383
Epoch: 567, Loss: 2.1915
Epoch: 568, Loss: 2.7304
Epoch: 569, Loss: 2.9261
Epoch: 570, Loss: 2.7723
Epoch: 571, Loss: 1.8707
Epoch: 572, Loss: 2.2477
Epoch: 573, Loss: 2.0065
Epoch: 574, Loss: 1.7018
Epoch: 575, Loss: 2.2319
Epoch: 576, Loss: 1.9297
Epoch: 577, Loss: 2.1655
Epoch: 578, Loss: 1.7415
Epoch: 579, Loss: 2.1743
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30429.31it/s]
Epoch: 580, Loss: 1.5257, Train: 0.9160, Val: 0.9122, test: 0.9087
Epoch: 581, Loss: 1.9469
Epoch: 582, Loss: 1.6724
Epoch: 583, Loss: 1.7492
Epoch: 584, Loss: 2.0817
Epoch: 585, Loss: 1.9182
Epoch: 586, Loss: 1.7500
Epoch: 587, Loss: 1.3507
Epoch: 588, Loss: 1.6725
Epoch: 589, Loss: 2.4703
Epoch: 590, Loss: 1.9505
Epoch: 591, Loss: 1.9672
Epoch: 592, Loss: 3.7428
Epoch: 593, Loss: 2.8772
Epoch: 594, Loss: 2.7522
Epoch: 595, Loss: 1.9750
Epoch: 596, Loss: 3.2905
Epoch: 597, Loss: 2.1722
Epoch: 598, Loss: 2.1184
Epoch: 599, Loss: 2.9967
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30127.58it/s]
Epoch: 600, Loss: 1.9306, Train: 0.9016, Val: 0.9007, test: 0.8942
Epoch: 601, Loss: 2.4302
Epoch: 602, Loss: 2.4596
Epoch: 603, Loss: 2.2147
Epoch: 604, Loss: 2.8014
Epoch: 605, Loss: 2.0108
Epoch: 606, Loss: 1.8834
Epoch: 607, Loss: 2.0805
Epoch: 608, Loss: 2.1495
Epoch: 609, Loss: 2.5041
Epoch: 610, Loss: 2.8036
Epoch: 611, Loss: 2.1085
Epoch: 612, Loss: 2.6892
Epoch: 613, Loss: 1.8449
Epoch: 614, Loss: 2.8982
Epoch: 615, Loss: 2.2250
Epoch: 616, Loss: 2.3581
Epoch: 617, Loss: 2.9618
Epoch: 618, Loss: 2.2418
Epoch: 619, Loss: 1.8585
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29709.78it/s]
Epoch: 620, Loss: 2.0568, Train: 0.9164, Val: 0.9131, test: 0.9137
Epoch: 621, Loss: 1.6007
Epoch: 622, Loss: 2.4810
Epoch: 623, Loss: 2.1279
Epoch: 624, Loss: 1.7007
Epoch: 625, Loss: 2.1268
Epoch: 626, Loss: 1.6662
Epoch: 627, Loss: 1.7484
Epoch: 628, Loss: 2.0052
Epoch: 629, Loss: 1.5000
Epoch: 630, Loss: 1.7416
Epoch: 631, Loss: 1.4765
Epoch: 632, Loss: 1.6035
Epoch: 633, Loss: 2.0764
Epoch: 634, Loss: 1.6841
Epoch: 635, Loss: 1.5589
Epoch: 636, Loss: 1.4588
Epoch: 637, Loss: 1.2664
Epoch: 638, Loss: 1.5710
Epoch: 639, Loss: 1.6725
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30008.02it/s]
Epoch: 640, Loss: 1.3528, Train: 0.9137, Val: 0.9109, test: 0.9074
Epoch: 641, Loss: 1.4072
Epoch: 642, Loss: 2.1868
Epoch: 643, Loss: 1.9994
Epoch: 644, Loss: 1.0464
Epoch: 645, Loss: 1.7191
Epoch: 646, Loss: 1.8161
Epoch: 647, Loss: 1.6892
Epoch: 648, Loss: 1.4686
Epoch: 649, Loss: 1.6487
Epoch: 650, Loss: 1.2062
Epoch: 651, Loss: 1.4245
Epoch: 652, Loss: 1.4854
Epoch: 653, Loss: 1.7604
Epoch: 654, Loss: 1.7680
Epoch: 655, Loss: 1.2522
Epoch: 656, Loss: 1.1558
Epoch: 657, Loss: 1.6231
Epoch: 658, Loss: 1.7337
Epoch: 659, Loss: 1.2498
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35880.33it/s]
Epoch: 660, Loss: 1.4271, Train: 0.9155, Val: 0.9134, test: 0.9074
Epoch: 661, Loss: 1.9072
Epoch: 662, Loss: 1.4263
Epoch: 663, Loss: 1.4313
Epoch: 664, Loss: 1.4032
Epoch: 665, Loss: 2.1140
Epoch: 666, Loss: 1.3799
Epoch: 667, Loss: 1.7645
Epoch: 668, Loss: 1.1634
Epoch: 669, Loss: 2.4196
Epoch: 670, Loss: 1.3302
Epoch: 671, Loss: 2.2899
Epoch: 672, Loss: 1.7222
Epoch: 673, Loss: 2.0484
Epoch: 674, Loss: 1.4832
Epoch: 675, Loss: 1.8684
Epoch: 676, Loss: 1.5779
Epoch: 677, Loss: 1.8051
Epoch: 678, Loss: 2.0840
Epoch: 679, Loss: 1.4928
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35509.67it/s]
Epoch: 680, Loss: 1.9609, Train: 0.9127, Val: 0.9112, test: 0.9058
Epoch: 681, Loss: 1.4566
Epoch: 682, Loss: 1.3600
Epoch: 683, Loss: 1.1857
Epoch: 684, Loss: 1.9588
Epoch: 685, Loss: 1.4512
Epoch: 686, Loss: 1.6827
Epoch: 687, Loss: 1.2753
Epoch: 688, Loss: 1.2945
Epoch: 689, Loss: 1.9244
Epoch: 690, Loss: 1.7081
Epoch: 691, Loss: 1.1892
Epoch: 692, Loss: 1.5541
Epoch: 693, Loss: 2.0250
Epoch: 694, Loss: 1.8689
Epoch: 695, Loss: 1.4395
Epoch: 696, Loss: 1.1743
Epoch: 697, Loss: 1.4262
Epoch: 698, Loss: 1.6160
Epoch: 699, Loss: 2.0287
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36104.54it/s]
Epoch: 700, Loss: 1.2810, Train: 0.9169, Val: 0.9115, test: 0.9105
Epoch: 701, Loss: 1.3567
Epoch: 702, Loss: 2.1273
Epoch: 703, Loss: 2.8250
Epoch: 704, Loss: 2.1288
Epoch: 705, Loss: 1.3864
Epoch: 706, Loss: 2.0185
Epoch: 707, Loss: 2.5772
Epoch: 708, Loss: 1.7442
Epoch: 709, Loss: 2.5945
Epoch: 710, Loss: 2.4985
Epoch: 711, Loss: 1.9188
Epoch: 712, Loss: 2.0237
Epoch: 713, Loss: 2.6295
Epoch: 714, Loss: 2.2160
Epoch: 715, Loss: 2.0621
Epoch: 716, Loss: 2.9771
Epoch: 717, Loss: 1.7085
Epoch: 718, Loss: 2.0556
Epoch: 719, Loss: 2.6148
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37751.72it/s]
Epoch: 720, Loss: 2.2078, Train: 0.8870, Val: 0.8873, test: 0.8782
Epoch: 721, Loss: 1.6850
Epoch: 722, Loss: 3.1627
Epoch: 723, Loss: 1.9876
Epoch: 724, Loss: 2.5606
Epoch: 725, Loss: 2.3373
Epoch: 726, Loss: 1.9061
Epoch: 727, Loss: 1.7077
Epoch: 728, Loss: 2.3810
Epoch: 729, Loss: 2.0027
Epoch: 730, Loss: 1.8719
Epoch: 731, Loss: 1.6866
Epoch: 732, Loss: 2.9096
Epoch: 733, Loss: 1.6585
Epoch: 734, Loss: 2.2272
Epoch: 735, Loss: 1.6659
Epoch: 736, Loss: 1.6920
Epoch: 737, Loss: 1.7692
Epoch: 738, Loss: 2.5968
Epoch: 739, Loss: 1.9086
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37891.32it/s]
Epoch: 740, Loss: 1.9822, Train: 0.9148, Val: 0.9125, test: 0.9074
Epoch: 741, Loss: 1.5437
Epoch: 742, Loss: 1.8715
Epoch: 743, Loss: 2.5965
Epoch: 744, Loss: 1.3054
Epoch: 745, Loss: 2.2631
Epoch: 746, Loss: 1.2573
Epoch: 747, Loss: 3.7157
Epoch: 748, Loss: 1.5559
Epoch: 749, Loss: 1.5295
Epoch: 750, Loss: 1.3319
Epoch: 751, Loss: 1.2540
Epoch: 752, Loss: 2.0398
Epoch: 753, Loss: 2.1144
Epoch: 754, Loss: 1.6008
Epoch: 755, Loss: 2.0337
Epoch: 756, Loss: 1.6456
Epoch: 757, Loss: 2.5818
Epoch: 758, Loss: 1.5037
Epoch: 759, Loss: 1.9890
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35881.36it/s]
Epoch: 760, Loss: 2.1618, Train: 0.9169, Val: 0.9140, test: 0.9099
Epoch: 761, Loss: 1.5315
Epoch: 762, Loss: 1.8954
Epoch: 763, Loss: 2.8760
Epoch: 764, Loss: 2.0663
Epoch: 765, Loss: 1.8053
Epoch: 766, Loss: 3.2994
Epoch: 767, Loss: 1.7995
Epoch: 768, Loss: 1.3658
Epoch: 769, Loss: 1.9551
Epoch: 770, Loss: 1.9097
Epoch: 771, Loss: 1.9160
Epoch: 772, Loss: 1.8849
Epoch: 773, Loss: 1.9194
Epoch: 774, Loss: 2.8172
Epoch: 775, Loss: 1.5224
Epoch: 776, Loss: 2.1805
Epoch: 777, Loss: 1.8000
Epoch: 778, Loss: 1.6627
Epoch: 779, Loss: 1.6601
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35020.17it/s]
Epoch: 780, Loss: 1.7208, Train: 0.9142, Val: 0.9125, test: 0.9068
Epoch: 781, Loss: 1.6205
Epoch: 782, Loss: 2.0467
Epoch: 783, Loss: 3.2147
Epoch: 784, Loss: 1.5923
Epoch: 785, Loss: 1.7880
Epoch: 786, Loss: 2.3600
Epoch: 787, Loss: 1.8341
Epoch: 788, Loss: 1.7855
Epoch: 789, Loss: 2.3983
Epoch: 790, Loss: 1.7252
Epoch: 791, Loss: 1.5168
Epoch: 792, Loss: 2.1453
Epoch: 793, Loss: 1.8534
Epoch: 794, Loss: 1.8065
Epoch: 795, Loss: 1.9756
Epoch: 796, Loss: 2.4827
Epoch: 797, Loss: 1.9461
Epoch: 798, Loss: 1.6515
Epoch: 799, Loss: 1.4472
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37063.36it/s]
Epoch: 800, Loss: 1.4361, Train: 0.9162, Val: 0.9134, test: 0.9071
Epoch: 801, Loss: 1.5599
Epoch: 802, Loss: 1.3858
Epoch: 803, Loss: 1.3978
Epoch: 804, Loss: 0.9600
Epoch: 805, Loss: 1.3012
Epoch: 806, Loss: 1.2730
Epoch: 807, Loss: 2.4055
Epoch: 808, Loss: 1.5114
Epoch: 809, Loss: 1.2026
Epoch: 810, Loss: 1.3644
Epoch: 811, Loss: 1.5545
Epoch: 812, Loss: 1.9654
Epoch: 813, Loss: 1.4788
Epoch: 814, Loss: 1.4374
Epoch: 815, Loss: 1.6007
Epoch: 816, Loss: 1.3491
Epoch: 817, Loss: 1.6261
Epoch: 818, Loss: 1.1392
Epoch: 819, Loss: 1.0640
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30141.63it/s]
Epoch: 820, Loss: 2.3060, Train: 0.8700, Val: 0.8734, test: 0.8597
Epoch: 821, Loss: 2.4348
Epoch: 822, Loss: 1.1120
Epoch: 823, Loss: 1.7565
Epoch: 824, Loss: 1.9102
Epoch: 825, Loss: 1.7429
Epoch: 826, Loss: 2.0001
Epoch: 827, Loss: 1.8249
Epoch: 828, Loss: 1.6075
Epoch: 829, Loss: 1.7554
Epoch: 830, Loss: 1.6556
Epoch: 831, Loss: 1.1803
Epoch: 832, Loss: 1.5248
Epoch: 833, Loss: 2.1114
Epoch: 834, Loss: 1.1902
Epoch: 835, Loss: 1.4698
Epoch: 836, Loss: 1.2640
Epoch: 837, Loss: 1.4718
Epoch: 838, Loss: 1.2182
Epoch: 839, Loss: 1.3161
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29819.40it/s]
Epoch: 840, Loss: 1.2361, Train: 0.9154, Val: 0.9134, test: 0.9065
Epoch: 841, Loss: 1.0948
Epoch: 842, Loss: 1.1645
Epoch: 843, Loss: 1.6557
Epoch: 844, Loss: 1.9219
Epoch: 845, Loss: 1.3907
Epoch: 846, Loss: 1.3141
Epoch: 847, Loss: 0.9924
Epoch: 848, Loss: 1.0549
Epoch: 849, Loss: 1.2209
Epoch: 850, Loss: 1.1394
Epoch: 851, Loss: 1.1450
Epoch: 852, Loss: 1.3059
Epoch: 853, Loss: 1.2802
Epoch: 854, Loss: 1.2154
Epoch: 855, Loss: 1.1600
Epoch: 856, Loss: 1.0485
Epoch: 857, Loss: 1.5463
Epoch: 858, Loss: 1.0134
Epoch: 859, Loss: 1.3568
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37451.16it/s]
Epoch: 860, Loss: 0.9887, Train: 0.9160, Val: 0.9137, test: 0.9083
Epoch: 861, Loss: 1.1437
Epoch: 862, Loss: 1.1273
Epoch: 863, Loss: 1.0702
Epoch: 864, Loss: 1.0422
Epoch: 865, Loss: 2.3651
Epoch: 866, Loss: 1.7220
Epoch: 867, Loss: 1.2264
Epoch: 868, Loss: 1.6547
Epoch: 869, Loss: 1.2025
Epoch: 870, Loss: 1.4846
Epoch: 871, Loss: 1.1111
Epoch: 872, Loss: 1.1913
Epoch: 873, Loss: 1.1669
Epoch: 874, Loss: 1.5788
Epoch: 875, Loss: 1.4726
Epoch: 876, Loss: 1.3132
Epoch: 877, Loss: 1.4951
Epoch: 878, Loss: 1.0536
Epoch: 879, Loss: 1.0305
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37005.60it/s]
Epoch: 880, Loss: 2.2777, Train: 0.9140, Val: 0.9128, test: 0.9052
Epoch: 881, Loss: 1.4147
Epoch: 882, Loss: 1.9675
Epoch: 883, Loss: 1.6982
Epoch: 884, Loss: 1.3841
Epoch: 885, Loss: 2.1723
Epoch: 886, Loss: 1.4323
Epoch: 887, Loss: 3.4820
Epoch: 888, Loss: 1.2870
Epoch: 889, Loss: 3.5777
Epoch: 890, Loss: 2.0231
Epoch: 891, Loss: 1.5606
Epoch: 892, Loss: 1.7774
Epoch: 893, Loss: 2.5717
Epoch: 894, Loss: 1.6057
Epoch: 895, Loss: 1.7903
Epoch: 896, Loss: 2.2685
Epoch: 897, Loss: 2.0264
Epoch: 898, Loss: 1.6222
Epoch: 899, Loss: 2.6563
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37120.34it/s]
Epoch: 900, Loss: 1.7850, Train: 0.9164, Val: 0.9122, test: 0.9146
Epoch: 901, Loss: 1.4034
Epoch: 902, Loss: 2.0343
Epoch: 903, Loss: 1.7091
Epoch: 904, Loss: 2.5599
Epoch: 905, Loss: 1.9939
Epoch: 906, Loss: 1.7388
Epoch: 907, Loss: 1.6115
Epoch: 908, Loss: 1.8772
Epoch: 909, Loss: 1.7528
Epoch: 910, Loss: 2.9440
Epoch: 911, Loss: 1.2167
Epoch: 912, Loss: 1.5353
Epoch: 913, Loss: 1.2612
Epoch: 914, Loss: 1.1246
Epoch: 915, Loss: 1.0839
Epoch: 916, Loss: 1.2487
Epoch: 917, Loss: 2.5674
Epoch: 918, Loss: 1.3620
Epoch: 919, Loss: 1.1380
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36622.30it/s]
Epoch: 920, Loss: 1.2463, Train: 0.9161, Val: 0.9128, test: 0.9087
Epoch: 921, Loss: 2.2999
Epoch: 922, Loss: 1.4443
Epoch: 923, Loss: 1.3707
Epoch: 924, Loss: 1.8630
Epoch: 925, Loss: 1.1806
Epoch: 926, Loss: 1.7547
Epoch: 927, Loss: 1.5302
Epoch: 928, Loss: 1.7515
Epoch: 929, Loss: 1.6162
Epoch: 930, Loss: 1.0324
Epoch: 931, Loss: 1.4321
Epoch: 932, Loss: 1.4047
Epoch: 933, Loss: 1.3755
Epoch: 934, Loss: 1.3535
Epoch: 935, Loss: 1.3167
Epoch: 936, Loss: 1.0935
Epoch: 937, Loss: 1.5340
Epoch: 938, Loss: 1.8395
Epoch: 939, Loss: 1.1806
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30379.21it/s]
Epoch: 940, Loss: 1.5664, Train: 0.9196, Val: 0.9146, test: 0.9187
Epoch: 941, Loss: 1.3056
Epoch: 942, Loss: 2.3222
Epoch: 943, Loss: 1.1168
Epoch: 944, Loss: 1.3469
Epoch: 945, Loss: 1.5963
Epoch: 946, Loss: 0.9845
Epoch: 947, Loss: 1.0709
Epoch: 948, Loss: 1.2378
Epoch: 949, Loss: 1.3492
Epoch: 950, Loss: 1.2564
Epoch: 951, Loss: 1.0951
Epoch: 952, Loss: 1.1343
Epoch: 953, Loss: 1.5820
Epoch: 954, Loss: 1.0088
Epoch: 955, Loss: 1.1611
Epoch: 956, Loss: 1.0756
Epoch: 957, Loss: 1.0819
Epoch: 958, Loss: 2.3161
Epoch: 959, Loss: 1.1295
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37183.80it/s]
Epoch: 960, Loss: 0.9547, Train: 0.9156, Val: 0.9134, test: 0.9080
Epoch: 961, Loss: 1.1348
Epoch: 962, Loss: 1.1759
Epoch: 963, Loss: 1.1826
Epoch: 964, Loss: 1.2295
Epoch: 965, Loss: 1.2324
Epoch: 966, Loss: 1.4655
Epoch: 967, Loss: 1.6555
Epoch: 968, Loss: 1.1612
Epoch: 969, Loss: 1.3843
Epoch: 970, Loss: 1.5399
Epoch: 971, Loss: 1.1662
Epoch: 972, Loss: 1.1384
Epoch: 973, Loss: 0.9577
Epoch: 974, Loss: 1.7077
Epoch: 975, Loss: 0.8210
Epoch: 976, Loss: 0.9742
Epoch: 977, Loss: 1.1062
Epoch: 978, Loss: 1.5800
Epoch: 979, Loss: 1.2023
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37760.89it/s]
Epoch: 980, Loss: 1.0351, Train: 0.9165, Val: 0.9128, test: 0.9090
Epoch: 981, Loss: 0.9147
Epoch: 982, Loss: 1.9526
Epoch: 983, Loss: 1.0867
Epoch: 984, Loss: 0.9023
Epoch: 985, Loss: 0.9936
Epoch: 986, Loss: 0.7923
Epoch: 987, Loss: 0.9627
Epoch: 988, Loss: 1.0328
Epoch: 989, Loss: 0.8069
Epoch: 990, Loss: 1.3825
Epoch: 991, Loss: 0.8736
Epoch: 992, Loss: 0.9714
Epoch: 993, Loss: 1.2923
Epoch: 994, Loss: 0.9035
Epoch: 995, Loss: 1.2538
Epoch: 996, Loss: 0.8927
Epoch: 997, Loss: 0.9668
Epoch: 998, Loss: 1.7844
Epoch: 999, Loss: 1.0088
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36601.44it/s]
Epoch: 1000, Loss: 1.6761, Train: 0.9173, Val: 0.9146, test: 0.9156
Epoch: 1001, Loss: 0.8504
Epoch: 1002, Loss: 1.4455
Epoch: 1003, Loss: 1.2490
Epoch: 1004, Loss: 1.9928
Epoch: 1005, Loss: 1.3212
Epoch: 1006, Loss: 1.1406
Epoch: 1007, Loss: 1.1551
Epoch: 1008, Loss: 1.4684
Epoch: 1009, Loss: 1.6201
Epoch: 1010, Loss: 1.1185
Epoch: 1011, Loss: 1.1242
Epoch: 1012, Loss: 1.0065
Epoch: 1013, Loss: 1.3214
Epoch: 1014, Loss: 0.9834
Epoch: 1015, Loss: 1.2860
Epoch: 1016, Loss: 1.0132
Epoch: 1017, Loss: 2.0447
Epoch: 1018, Loss: 1.6806
Epoch: 1019, Loss: 1.1106
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35539.53it/s]
Epoch: 1020, Loss: 1.1897, Train: 0.9147, Val: 0.9119, test: 0.9062
Epoch: 1021, Loss: 1.1884
Epoch: 1022, Loss: 1.5016
Epoch: 1023, Loss: 1.2244
Epoch: 1024, Loss: 1.4523
Epoch: 1025, Loss: 1.2030
Epoch: 1026, Loss: 2.3032
Epoch: 1027, Loss: 1.7157
Epoch: 1028, Loss: 1.0096
Epoch: 1029, Loss: 1.3207
Epoch: 1030, Loss: 1.1552
Epoch: 1031, Loss: 1.0341
Epoch: 1032, Loss: 1.5198
Epoch: 1033, Loss: 1.4625
Epoch: 1034, Loss: 0.9036
Epoch: 1035, Loss: 0.8792
Epoch: 1036, Loss: 1.4287
Epoch: 1037, Loss: 1.0468
Epoch: 1038, Loss: 1.3917
Epoch: 1039, Loss: 1.4446
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36783.34it/s]
Epoch: 1040, Loss: 0.9940, Train: 0.9177, Val: 0.9156, test: 0.9149
Epoch: 1041, Loss: 1.7625
Epoch: 1042, Loss: 1.5948
Epoch: 1043, Loss: 1.7365
Epoch: 1044, Loss: 2.1850
Epoch: 1045, Loss: 1.4575
Epoch: 1046, Loss: 1.6146
Epoch: 1047, Loss: 2.1246
Epoch: 1048, Loss: 1.9004
Epoch: 1049, Loss: 1.6911
Epoch: 1050, Loss: 1.3057
Epoch: 1051, Loss: 1.1604
Epoch: 1052, Loss: 1.3904
Epoch: 1053, Loss: 1.0759
Epoch: 1054, Loss: 1.2544
Epoch: 1055, Loss: 1.4659
Epoch: 1056, Loss: 1.4405
Epoch: 1057, Loss: 2.2355
Epoch: 1058, Loss: 1.2582
Epoch: 1059, Loss: 0.9714
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37848.78it/s]
Epoch: 1060, Loss: 1.2198, Train: 0.9170, Val: 0.9146, test: 0.9121
Epoch: 1061, Loss: 1.2421
Epoch: 1062, Loss: 1.9652
Epoch: 1063, Loss: 1.2750
Epoch: 1064, Loss: 1.1964
Epoch: 1065, Loss: 1.0087
Epoch: 1066, Loss: 1.5406
Epoch: 1067, Loss: 0.8817
Epoch: 1068, Loss: 1.0913
Epoch: 1069, Loss: 1.1981
Epoch: 1070, Loss: 1.2707
Epoch: 1071, Loss: 1.2964
Epoch: 1072, Loss: 0.9459
Epoch: 1073, Loss: 1.0597
Epoch: 1074, Loss: 1.3578
Epoch: 1075, Loss: 0.8096
Epoch: 1076, Loss: 1.6068
Epoch: 1077, Loss: 1.1294
Epoch: 1078, Loss: 1.7433
Epoch: 1079, Loss: 1.3315
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35728.78it/s]
Epoch: 1080, Loss: 1.4793, Train: 0.9138, Val: 0.9097, test: 0.9049
Epoch: 1081, Loss: 1.0075
Epoch: 1082, Loss: 1.1735
Epoch: 1083, Loss: 1.0662
Epoch: 1084, Loss: 0.8531
Epoch: 1085, Loss: 1.1915
Epoch: 1086, Loss: 1.6065
Epoch: 1087, Loss: 1.1508
Epoch: 1088, Loss: 1.0500
Epoch: 1089, Loss: 0.8394
Epoch: 1090, Loss: 0.8990
Epoch: 1091, Loss: 0.8893
Epoch: 1092, Loss: 1.1227
Epoch: 1093, Loss: 0.7642
Epoch: 1094, Loss: 0.8188
Epoch: 1095, Loss: 0.9627
Epoch: 1096, Loss: 0.7112
Epoch: 1097, Loss: 0.6796
Epoch: 1098, Loss: 0.9197
Epoch: 1099, Loss: 1.0101
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35883.57it/s]
Epoch: 1100, Loss: 0.8796, Train: 0.9176, Val: 0.9150, test: 0.9134
Epoch: 1101, Loss: 0.7671
Epoch: 1102, Loss: 1.2921
Epoch: 1103, Loss: 0.9459
Epoch: 1104, Loss: 0.9399
Epoch: 1105, Loss: 0.8939
Epoch: 1106, Loss: 1.0018
Epoch: 1107, Loss: 0.8165
Epoch: 1108, Loss: 0.7630
Epoch: 1109, Loss: 0.8596
Epoch: 1110, Loss: 0.7751
Epoch: 1111, Loss: 0.7706
Epoch: 1112, Loss: 0.9415
Epoch: 1113, Loss: 1.1426
Epoch: 1114, Loss: 0.9255
Epoch: 1115, Loss: 0.8970
Epoch: 1116, Loss: 0.8049
Epoch: 1117, Loss: 0.9315
Epoch: 1118, Loss: 0.9463
Epoch: 1119, Loss: 0.8067
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37940.64it/s]
Epoch: 1120, Loss: 1.2189, Train: 0.9066, Val: 0.8994, test: 0.9005
Epoch: 1121, Loss: 1.1617
Epoch: 1122, Loss: 0.9316
Epoch: 1123, Loss: 0.8321
Epoch: 1124, Loss: 0.9670
Epoch: 1125, Loss: 0.9668
Epoch: 1126, Loss: 1.1831
Epoch: 1127, Loss: 1.6107
Epoch: 1128, Loss: 0.9616
Epoch: 1129, Loss: 1.2267
Epoch: 1130, Loss: 0.9969
Epoch: 1131, Loss: 0.8464
Epoch: 1132, Loss: 1.0205
Epoch: 1133, Loss: 1.1950
Epoch: 1134, Loss: 1.2691
Epoch: 1135, Loss: 0.6564
Epoch: 1136, Loss: 0.8518
Epoch: 1137, Loss: 1.2846
Epoch: 1138, Loss: 1.3174
Epoch: 1139, Loss: 1.3441
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35525.62it/s]
Epoch: 1140, Loss: 1.3368, Train: 0.9174, Val: 0.9122, test: 0.9109
Epoch: 1141, Loss: 1.0966
Epoch: 1142, Loss: 1.4106
Epoch: 1143, Loss: 1.6754
Epoch: 1144, Loss: 1.0745
Epoch: 1145, Loss: 0.9318
Epoch: 1146, Loss: 1.0190
Epoch: 1147, Loss: 1.2017
Epoch: 1148, Loss: 0.8697
Epoch: 1149, Loss: 0.9596
Epoch: 1150, Loss: 1.4283
Epoch: 1151, Loss: 0.8576
Epoch: 1152, Loss: 0.8301
Epoch: 1153, Loss: 0.7399
Epoch: 1154, Loss: 0.8883
Epoch: 1155, Loss: 0.6942
Epoch: 1156, Loss: 0.7247
Epoch: 1157, Loss: 0.6593
Epoch: 1158, Loss: 0.6752
Epoch: 1159, Loss: 0.8703
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37190.37it/s]
Epoch: 1160, Loss: 0.9872, Train: 0.9158, Val: 0.9109, test: 0.9093
Epoch: 1161, Loss: 0.7079
Epoch: 1162, Loss: 0.6668
Epoch: 1163, Loss: 0.6415
Epoch: 1164, Loss: 0.7086
Epoch: 1165, Loss: 0.9594
Epoch: 1166, Loss: 1.6887
Epoch: 1167, Loss: 1.5313
Epoch: 1168, Loss: 0.8220
Epoch: 1169, Loss: 1.0656
Epoch: 1170, Loss: 1.1946
Epoch: 1171, Loss: 1.1765
Epoch: 1172, Loss: 1.6324
Epoch: 1173, Loss: 1.5898
Epoch: 1174, Loss: 1.1122
Epoch: 1175, Loss: 0.9306
Epoch: 1176, Loss: 3.2959
Epoch: 1177, Loss: 2.2714
Epoch: 1178, Loss: 1.2163
Epoch: 1179, Loss: 1.1436
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37678.38it/s]
Epoch: 1180, Loss: 1.6027, Train: 0.9019, Val: 0.8979, test: 0.8986
Epoch: 1181, Loss: 2.1060
Epoch: 1182, Loss: 1.7483
Epoch: 1183, Loss: 1.3900
Epoch: 1184, Loss: 1.8565
Epoch: 1185, Loss: 1.7041
Epoch: 1186, Loss: 1.9363
Epoch: 1187, Loss: 1.3411
Epoch: 1188, Loss: 1.3336
Epoch: 1189, Loss: 1.3471
Epoch: 1190, Loss: 1.7911
Epoch: 1191, Loss: 1.2575
Epoch: 1192, Loss: 1.3185
Epoch: 1193, Loss: 1.2937
Epoch: 1194, Loss: 1.6317
Epoch: 1195, Loss: 1.0628
Epoch: 1196, Loss: 2.4308
Epoch: 1197, Loss: 1.3200
Epoch: 1198, Loss: 1.4874
Epoch: 1199, Loss: 1.0419
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37670.51it/s]
Epoch: 1200, Loss: 1.3617, Train: 0.9175, Val: 0.9143, test: 0.9143
Epoch: 1201, Loss: 1.2915
Epoch: 1202, Loss: 0.9342
Epoch: 1203, Loss: 0.9858
Epoch: 1204, Loss: 1.7805
Epoch: 1205, Loss: 1.0845
Epoch: 1206, Loss: 1.1340
Epoch: 1207, Loss: 0.8585
Epoch: 1208, Loss: 1.1009
Epoch: 1209, Loss: 0.9827
Epoch: 1210, Loss: 1.0080
Epoch: 1211, Loss: 1.3095
Epoch: 1212, Loss: 0.9041
Epoch: 1213, Loss: 1.1462
Epoch: 1214, Loss: 0.8658
Epoch: 1215, Loss: 0.9827
Epoch: 1216, Loss: 1.1138
Epoch: 1217, Loss: 0.7649
Epoch: 1218, Loss: 0.9440
Epoch: 1219, Loss: 0.9786
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38032.61it/s]
Epoch: 1220, Loss: 0.9082, Train: 0.9184, Val: 0.9150, test: 0.9162
Epoch: 1221, Loss: 0.8349
Epoch: 1222, Loss: 0.8572
Epoch: 1223, Loss: 0.9220
Epoch: 1224, Loss: 0.8440
Epoch: 1225, Loss: 0.9215
Epoch: 1226, Loss: 0.9689
Epoch: 1227, Loss: 1.3239
Epoch: 1228, Loss: 1.1612
Epoch: 1229, Loss: 1.5678
Epoch: 1230, Loss: 0.9841
Epoch: 1231, Loss: 0.8139
Epoch: 1232, Loss: 1.0017
Epoch: 1233, Loss: 1.2608
Epoch: 1234, Loss: 0.9405
Epoch: 1235, Loss: 0.9500
Epoch: 1236, Loss: 1.2095
Epoch: 1237, Loss: 1.0232
Epoch: 1238, Loss: 0.9852
Epoch: 1239, Loss: 1.5544
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36877.28it/s]
Epoch: 1240, Loss: 0.8783, Train: 0.9154, Val: 0.9131, test: 0.9109
Epoch: 1241, Loss: 1.0787
Epoch: 1242, Loss: 1.1361
Epoch: 1243, Loss: 1.3895
Epoch: 1244, Loss: 0.8795
Epoch: 1245, Loss: 1.1416
Epoch: 1246, Loss: 0.9308
Epoch: 1247, Loss: 0.6774
Epoch: 1248, Loss: 0.8972
Epoch: 1249, Loss: 0.7711
Epoch: 1250, Loss: 0.9567
Epoch: 1251, Loss: 1.5073
Epoch: 1252, Loss: 0.8314
Epoch: 1253, Loss: 0.9012
Epoch: 1254, Loss: 0.8005
Epoch: 1255, Loss: 0.9189
Epoch: 1256, Loss: 0.8926
Epoch: 1257, Loss: 0.8445
Epoch: 1258, Loss: 0.9935
Epoch: 1259, Loss: 1.2159
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38042.91it/s]
Epoch: 1260, Loss: 1.2536, Train: 0.9182, Val: 0.9134, test: 0.9156
Epoch: 1261, Loss: 0.7474
Epoch: 1262, Loss: 0.8178
Epoch: 1263, Loss: 0.8491
Epoch: 1264, Loss: 1.6505
Epoch: 1265, Loss: 0.9921
Epoch: 1266, Loss: 0.8413
Epoch: 1267, Loss: 0.8352
Epoch: 1268, Loss: 0.9695
Epoch: 1269, Loss: 1.5879
Epoch: 1270, Loss: 0.8221
Epoch: 1271, Loss: 0.9731
Epoch: 1272, Loss: 0.9482
Epoch: 1273, Loss: 1.2915
Epoch: 1274, Loss: 0.7460
Epoch: 1275, Loss: 0.8644
Epoch: 1276, Loss: 0.9942
Epoch: 1277, Loss: 0.9431
Epoch: 1278, Loss: 0.9728
Epoch: 1279, Loss: 1.0031
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36318.35it/s]
Epoch: 1280, Loss: 1.1560, Train: 0.9115, Val: 0.9094, test: 0.9043
Epoch: 1281, Loss: 0.8736
Epoch: 1282, Loss: 0.9868
Epoch: 1283, Loss: 0.7381
Epoch: 1284, Loss: 0.8782
Epoch: 1285, Loss: 1.0875
Epoch: 1286, Loss: 0.9306
Epoch: 1287, Loss: 0.9154
Epoch: 1288, Loss: 0.9169
Epoch: 1289, Loss: 1.1259
Epoch: 1290, Loss: 0.9336
Epoch: 1291, Loss: 0.8126
Epoch: 1292, Loss: 0.9053
Epoch: 1293, Loss: 0.7503
Epoch: 1294, Loss: 0.6948
Epoch: 1295, Loss: 0.8305
Epoch: 1296, Loss: 0.7750
Epoch: 1297, Loss: 1.1644
Epoch: 1298, Loss: 0.8904
Epoch: 1299, Loss: 0.7229
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30158.86it/s]
Epoch: 1300, Loss: 0.7902, Train: 0.9173, Val: 0.9137, test: 0.9127
Epoch: 1301, Loss: 1.4802
Epoch: 1302, Loss: 0.7009
Epoch: 1303, Loss: 0.8827
Epoch: 1304, Loss: 1.3891
Epoch: 1305, Loss: 1.0659
Epoch: 1306, Loss: 1.1429
Epoch: 1307, Loss: 1.1617
Epoch: 1308, Loss: 0.6078
Epoch: 1309, Loss: 1.0031
Epoch: 1310, Loss: 1.3369
Epoch: 1311, Loss: 0.8546
Epoch: 1312, Loss: 0.6231
Epoch: 1313, Loss: 0.8849
Epoch: 1314, Loss: 0.7302
Epoch: 1315, Loss: 0.8653
Epoch: 1316, Loss: 0.7005
Epoch: 1317, Loss: 0.7756
Epoch: 1318, Loss: 0.7767
Epoch: 1319, Loss: 0.6976
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36066.04it/s]
Epoch: 1320, Loss: 0.6801, Train: 0.9162, Val: 0.9125, test: 0.9105
Epoch: 1321, Loss: 1.0114
Epoch: 1322, Loss: 0.7589
Epoch: 1323, Loss: 0.6852
Epoch: 1324, Loss: 0.5760
Epoch: 1325, Loss: 0.7278
Epoch: 1326, Loss: 0.5450
Epoch: 1327, Loss: 0.7290
Epoch: 1328, Loss: 0.6132
Epoch: 1329, Loss: 0.6314
Epoch: 1330, Loss: 0.8724
Epoch: 1331, Loss: 0.7705
Epoch: 1332, Loss: 0.6672
Epoch: 1333, Loss: 0.7591
Epoch: 1334, Loss: 0.9794
Epoch: 1335, Loss: 0.6834
Epoch: 1336, Loss: 0.7908
Epoch: 1337, Loss: 0.7531
Epoch: 1338, Loss: 0.8032
Epoch: 1339, Loss: 0.7662
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36908.74it/s]
Epoch: 1340, Loss: 0.7635, Train: 0.9169, Val: 0.9125, test: 0.9099
Epoch: 1341, Loss: 0.7934
Epoch: 1342, Loss: 0.8035
Epoch: 1343, Loss: 1.1214
Epoch: 1344, Loss: 0.6475
Epoch: 1345, Loss: 0.7443
Epoch: 1346, Loss: 0.7117
Epoch: 1347, Loss: 0.6617
Epoch: 1348, Loss: 0.6276
Epoch: 1349, Loss: 0.6796
Epoch: 1350, Loss: 0.6787
Epoch: 1351, Loss: 0.9646
Epoch: 1352, Loss: 0.6202
Epoch: 1353, Loss: 0.6108
Epoch: 1354, Loss: 0.6529
Epoch: 1355, Loss: 0.5205
Epoch: 1356, Loss: 0.7842
Epoch: 1357, Loss: 0.5227
Epoch: 1358, Loss: 0.6480
Epoch: 1359, Loss: 0.5679
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36328.64it/s]
Epoch: 1360, Loss: 0.5663, Train: 0.9198, Val: 0.9159, test: 0.9175
Epoch: 1361, Loss: 0.5434
Epoch: 1362, Loss: 0.5430
Epoch: 1363, Loss: 0.6554
Epoch: 1364, Loss: 0.5648
Epoch: 1365, Loss: 0.5389
Epoch: 1366, Loss: 0.5491
Epoch: 1367, Loss: 0.7421
Epoch: 1368, Loss: 1.0613
Epoch: 1369, Loss: 0.6658
Epoch: 1370, Loss: 0.7339
Epoch: 1371, Loss: 0.9604
Epoch: 1372, Loss: 0.7904
Epoch: 1373, Loss: 1.0427
Epoch: 1374, Loss: 1.3202
Epoch: 1375, Loss: 0.6526
Epoch: 1376, Loss: 0.8905
Epoch: 1377, Loss: 1.4535
Epoch: 1378, Loss: 0.8431
Epoch: 1379, Loss: 1.4849
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36815.30it/s]
Epoch: 1380, Loss: 1.5950, Train: 0.9169, Val: 0.9125, test: 0.9143
Epoch: 1381, Loss: 1.0780
Epoch: 1382, Loss: 1.0449
Epoch: 1383, Loss: 1.0805
Epoch: 1384, Loss: 1.1112
Epoch: 1385, Loss: 1.1022
Epoch: 1386, Loss: 1.3985
Epoch: 1387, Loss: 1.1649
Epoch: 1388, Loss: 1.0876
Epoch: 1389, Loss: 0.9034
Epoch: 1390, Loss: 1.1771
Epoch: 1391, Loss: 1.1261
Epoch: 1392, Loss: 0.9453
Epoch: 1393, Loss: 1.1323
Epoch: 1394, Loss: 1.0216
Epoch: 1395, Loss: 1.0362
Epoch: 1396, Loss: 0.8425
Epoch: 1397, Loss: 0.8874
Epoch: 1398, Loss: 0.9824
Epoch: 1399, Loss: 1.4426
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36217.74it/s]
Epoch: 1400, Loss: 0.9702, Train: 0.9101, Val: 0.9088, test: 0.9049
Epoch: 1401, Loss: 0.8001
Epoch: 1402, Loss: 0.8530
Epoch: 1403, Loss: 1.0281
Epoch: 1404, Loss: 1.2044
Epoch: 1405, Loss: 0.8902
Epoch: 1406, Loss: 0.6787
Epoch: 1407, Loss: 1.0324
Epoch: 1408, Loss: 0.7620
Epoch: 1409, Loss: 0.8982
Epoch: 1410, Loss: 0.7399
Epoch: 1411, Loss: 0.8508
Epoch: 1412, Loss: 1.0492
Epoch: 1413, Loss: 0.8960
Epoch: 1414, Loss: 0.6778
Epoch: 1415, Loss: 0.9400
Epoch: 1416, Loss: 0.7238
Epoch: 1417, Loss: 0.9304
Epoch: 1418, Loss: 1.1734
Epoch: 1419, Loss: 0.7426
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36434.94it/s]
Epoch: 1420, Loss: 0.8573, Train: 0.9160, Val: 0.9134, test: 0.9083
Epoch: 1421, Loss: 0.8168
Epoch: 1422, Loss: 0.8031
Epoch: 1423, Loss: 0.8642
Epoch: 1424, Loss: 0.9180
Epoch: 1425, Loss: 0.8663
Epoch: 1426, Loss: 0.8362
Epoch: 1427, Loss: 0.8288
Epoch: 1428, Loss: 0.9511
Epoch: 1429, Loss: 0.6978
Epoch: 1430, Loss: 0.7739
Epoch: 1431, Loss: 0.7387
Epoch: 1432, Loss: 0.9444
Epoch: 1433, Loss: 1.2586
Epoch: 1434, Loss: 1.6668
Epoch: 1435, Loss: 0.7327
Epoch: 1436, Loss: 0.5748
Epoch: 1437, Loss: 0.6559
Epoch: 1438, Loss: 0.6182
Epoch: 1439, Loss: 0.6491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36913.89it/s]
Epoch: 1440, Loss: 0.9917, Train: 0.9065, Val: 0.9025, test: 0.9024
Epoch: 1441, Loss: 0.8746
Epoch: 1442, Loss: 0.6752
Epoch: 1443, Loss: 0.7265
Epoch: 1444, Loss: 0.6218
Epoch: 1445, Loss: 0.6322
Epoch: 1446, Loss: 0.7104
Epoch: 1447, Loss: 0.6868
Epoch: 1448, Loss: 0.6648
Epoch: 1449, Loss: 0.6560
Epoch: 1450, Loss: 0.8341
Epoch: 1451, Loss: 0.8494
Epoch: 1452, Loss: 0.7186
Epoch: 1453, Loss: 1.1411
Epoch: 1454, Loss: 0.5827
Epoch: 1455, Loss: 0.8721
Epoch: 1456, Loss: 0.8024
Epoch: 1457, Loss: 0.9423
Epoch: 1458, Loss: 0.8588
Epoch: 1459, Loss: 0.8244
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35173.84it/s]
Epoch: 1460, Loss: 0.7075, Train: 0.9189, Val: 0.9134, test: 0.9140
Epoch: 1461, Loss: 0.6215
Epoch: 1462, Loss: 0.8048
Epoch: 1463, Loss: 0.8622
Epoch: 1464, Loss: 1.1932
Epoch: 1465, Loss: 0.6918
Epoch: 1466, Loss: 0.9413
Epoch: 1467, Loss: 0.9546
Epoch: 1468, Loss: 2.1498
Epoch: 1469, Loss: 0.8175
Epoch: 1470, Loss: 0.7783
Epoch: 1471, Loss: 0.9236
Epoch: 1472, Loss: 0.7522
Epoch: 1473, Loss: 0.6458
Epoch: 1474, Loss: 0.7306
Epoch: 1475, Loss: 0.7237
Epoch: 1476, Loss: 1.5418
Epoch: 1477, Loss: 0.6784
Epoch: 1478, Loss: 0.5958
Epoch: 1479, Loss: 0.8493
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37423.44it/s]
Epoch: 1480, Loss: 0.9898, Train: 0.9171, Val: 0.9122, test: 0.9093
Epoch: 1481, Loss: 0.7216
Epoch: 1482, Loss: 0.6731
Epoch: 1483, Loss: 0.7615
Epoch: 1484, Loss: 0.7124
Epoch: 1485, Loss: 0.7824
Epoch: 1486, Loss: 0.6992
Epoch: 1487, Loss: 0.5279
Epoch: 1488, Loss: 0.6160
Epoch: 1489, Loss: 0.7074
Epoch: 1490, Loss: 0.9878
Epoch: 1491, Loss: 0.5665
Epoch: 1492, Loss: 0.5696
Epoch: 1493, Loss: 0.6339
Epoch: 1494, Loss: 0.7098
Epoch: 1495, Loss: 0.5825
Epoch: 1496, Loss: 0.6499
Epoch: 1497, Loss: 0.5305
Epoch: 1498, Loss: 0.7265
Epoch: 1499, Loss: 0.5312
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37600.37it/s]
Epoch: 1500, Loss: 0.4594, Train: 0.9165, Val: 0.9125, test: 0.9102
Epoch: 1501, Loss: 0.5362
Epoch: 1502, Loss: 0.4842
Epoch: 1503, Loss: 0.4766
Epoch: 1504, Loss: 0.5451
Epoch: 1505, Loss: 0.5700
Epoch: 1506, Loss: 0.5046
Epoch: 1507, Loss: 0.5759
Epoch: 1508, Loss: 0.6417
Epoch: 1509, Loss: 0.6300
Epoch: 1510, Loss: 0.5176
Epoch: 1511, Loss: 0.9433
Epoch: 1512, Loss: 0.6235
Epoch: 1513, Loss: 0.6413
Epoch: 1514, Loss: 0.6253
Epoch: 1515, Loss: 0.7074
Epoch: 1516, Loss: 0.7024
Epoch: 1517, Loss: 0.6790
Epoch: 1518, Loss: 0.6756
Epoch: 1519, Loss: 0.9857
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37602.72it/s]
Epoch: 1520, Loss: 0.6760, Train: 0.9187, Val: 0.9134, test: 0.9165
Epoch: 1521, Loss: 0.7036
Epoch: 1522, Loss: 0.8331
Epoch: 1523, Loss: 0.8360
Epoch: 1524, Loss: 0.6951
Epoch: 1525, Loss: 1.1215
Epoch: 1526, Loss: 0.6536
Epoch: 1527, Loss: 0.8339
Epoch: 1528, Loss: 2.1157
Epoch: 1529, Loss: 1.0220
Epoch: 1530, Loss: 1.3000
Epoch: 1531, Loss: 0.8510
Epoch: 1532, Loss: 0.8641
Epoch: 1533, Loss: 1.0463
Epoch: 1534, Loss: 1.0949
Epoch: 1535, Loss: 1.0861
Epoch: 1536, Loss: 1.1972
Epoch: 1537, Loss: 1.0232
Epoch: 1538, Loss: 1.0475
Epoch: 1539, Loss: 1.0052
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36502.41it/s]
Epoch: 1540, Loss: 0.8683, Train: 0.9196, Val: 0.9150, test: 0.9175
Epoch: 1541, Loss: 0.9585
Epoch: 1542, Loss: 0.7862
Epoch: 1543, Loss: 0.8713
Epoch: 1544, Loss: 0.8755
Epoch: 1545, Loss: 0.8751
Epoch: 1546, Loss: 0.7765
Epoch: 1547, Loss: 0.9523
Epoch: 1548, Loss: 1.0306
Epoch: 1549, Loss: 1.0104
Epoch: 1550, Loss: 0.8929
Epoch: 1551, Loss: 0.8034
Epoch: 1552, Loss: 0.6801
Epoch: 1553, Loss: 0.7119
Epoch: 1554, Loss: 0.7613
Epoch: 1555, Loss: 0.6624
Epoch: 1556, Loss: 0.5867
Epoch: 1557, Loss: 0.6107
Epoch: 1558, Loss: 0.6639
Epoch: 1559, Loss: 0.7417
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36568.77it/s]
Epoch: 1560, Loss: 0.5853, Train: 0.9195, Val: 0.9153, test: 0.9196
Epoch: 1561, Loss: 0.6596
Epoch: 1562, Loss: 1.4789
Epoch: 1563, Loss: 0.6093
Epoch: 1564, Loss: 0.8438
Epoch: 1565, Loss: 0.6422
Epoch: 1566, Loss: 0.7228
Epoch: 1567, Loss: 0.8074
Epoch: 1568, Loss: 0.8401
Epoch: 1569, Loss: 0.7145
Epoch: 1570, Loss: 0.8291
Epoch: 1571, Loss: 0.7294
Epoch: 1572, Loss: 0.4812
Epoch: 1573, Loss: 0.5764
Epoch: 1574, Loss: 0.6900
Epoch: 1575, Loss: 0.5844
Epoch: 1576, Loss: 1.5460
Epoch: 1577, Loss: 0.7293
Epoch: 1578, Loss: 0.8873
Epoch: 1579, Loss: 0.6392
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36141.34it/s]
Epoch: 1580, Loss: 0.7240, Train: 0.9186, Val: 0.9159, test: 0.9146
Epoch: 1581, Loss: 0.8428
Epoch: 1582, Loss: 0.8808
Epoch: 1583, Loss: 0.6136
Epoch: 1584, Loss: 0.7241
Epoch: 1585, Loss: 0.8413
Epoch: 1586, Loss: 0.5893
Epoch: 1587, Loss: 0.7474
Epoch: 1588, Loss: 1.0053
Epoch: 1589, Loss: 0.6297
Epoch: 1590, Loss: 0.7817
Epoch: 1591, Loss: 0.8893
Epoch: 1592, Loss: 0.8764
Epoch: 1593, Loss: 0.7394
Epoch: 1594, Loss: 1.0530
Epoch: 1595, Loss: 1.0249
Epoch: 1596, Loss: 0.9766
Epoch: 1597, Loss: 1.0574
Epoch: 1598, Loss: 1.1888
Epoch: 1599, Loss: 1.0611
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35569.19it/s]
Epoch: 1600, Loss: 0.9883, Train: 0.9208, Val: 0.9159, test: 0.9200
Epoch: 1601, Loss: 0.8623
Epoch: 1602, Loss: 1.1093
Epoch: 1603, Loss: 0.8817
Epoch: 1604, Loss: 0.7539
Epoch: 1605, Loss: 0.7530
Epoch: 1606, Loss: 0.6980
Epoch: 1607, Loss: 0.5896
Epoch: 1608, Loss: 0.7729
Epoch: 1609, Loss: 0.7580
Epoch: 1610, Loss: 0.6482
Epoch: 1611, Loss: 0.7277
Epoch: 1612, Loss: 0.7809
Epoch: 1613, Loss: 0.7536
Epoch: 1614, Loss: 0.9024
Epoch: 1615, Loss: 0.7297
Epoch: 1616, Loss: 0.6958
Epoch: 1617, Loss: 0.9235
Epoch: 1618, Loss: 1.1970
Epoch: 1619, Loss: 0.8322
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35475.65it/s]
Epoch: 1620, Loss: 0.6084, Train: 0.9186, Val: 0.9137, test: 0.9143
Epoch: 1621, Loss: 0.7792
Epoch: 1622, Loss: 0.7479
Epoch: 1623, Loss: 0.7645
Epoch: 1624, Loss: 0.7220
Epoch: 1625, Loss: 0.7669
Epoch: 1626, Loss: 0.5991
Epoch: 1627, Loss: 0.7240
Epoch: 1628, Loss: 0.6785
Epoch: 1629, Loss: 0.6546
Epoch: 1630, Loss: 0.6009
Epoch: 1631, Loss: 0.7601
Epoch: 1632, Loss: 0.6677
Epoch: 1633, Loss: 0.5352
Epoch: 1634, Loss: 0.6473
Epoch: 1635, Loss: 0.7137
Epoch: 1636, Loss: 0.5897
Epoch: 1637, Loss: 0.6607
Epoch: 1638, Loss: 0.5560
Epoch: 1639, Loss: 0.4644
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36055.42it/s]
Epoch: 1640, Loss: 0.6023, Train: 0.9182, Val: 0.9140, test: 0.9156
Epoch: 1641, Loss: 0.4715
Epoch: 1642, Loss: 0.4924
Epoch: 1643, Loss: 0.5558
Epoch: 1644, Loss: 0.6368
Epoch: 1645, Loss: 0.5617
Epoch: 1646, Loss: 0.5246
Epoch: 1647, Loss: 0.5187
Epoch: 1648, Loss: 0.4709
Epoch: 1649, Loss: 0.6026
Epoch: 1650, Loss: 0.5658
Epoch: 1651, Loss: 0.6044
Epoch: 1652, Loss: 0.5632
Epoch: 1653, Loss: 0.9245
Epoch: 1654, Loss: 0.5836
Epoch: 1655, Loss: 0.6510
Epoch: 1656, Loss: 0.6608
Epoch: 1657, Loss: 0.5885
Epoch: 1658, Loss: 0.7349
Epoch: 1659, Loss: 0.6274
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35753.31it/s]
Epoch: 1660, Loss: 0.9200, Train: 0.9206, Val: 0.9143, test: 0.9187
Epoch: 1661, Loss: 0.5781
Epoch: 1662, Loss: 0.6725
Epoch: 1663, Loss: 0.5004
Epoch: 1664, Loss: 0.5943
Epoch: 1665, Loss: 0.6371
Epoch: 1666, Loss: 0.4396
Epoch: 1667, Loss: 0.4687
Epoch: 1668, Loss: 0.4882
Epoch: 1669, Loss: 0.3984
Epoch: 1670, Loss: 0.4276
Epoch: 1671, Loss: 0.5422
Epoch: 1672, Loss: 0.4821
Epoch: 1673, Loss: 0.3978
Epoch: 1674, Loss: 0.4514
Epoch: 1675, Loss: 0.5219
Epoch: 1676, Loss: 0.4676
Epoch: 1677, Loss: 0.3852
Epoch: 1678, Loss: 0.5083
Epoch: 1679, Loss: 0.6652
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36674.66it/s]
Epoch: 1680, Loss: 0.4267, Train: 0.9178, Val: 0.9131, test: 0.9090
Epoch: 1681, Loss: 0.4322
Epoch: 1682, Loss: 0.4357
Epoch: 1683, Loss: 0.4278
Epoch: 1684, Loss: 0.4314
Epoch: 1685, Loss: 0.3984
Epoch: 1686, Loss: 0.4388
Epoch: 1687, Loss: 0.4180
Epoch: 1688, Loss: 0.3733
Epoch: 1689, Loss: 0.4126
Epoch: 1690, Loss: 0.4914
Epoch: 1691, Loss: 0.4149
Epoch: 1692, Loss: 0.4018
Epoch: 1693, Loss: 0.5137
Epoch: 1694, Loss: 0.6761
Epoch: 1695, Loss: 0.8811
Epoch: 1696, Loss: 0.7559
Epoch: 1697, Loss: 0.7243
Epoch: 1698, Loss: 0.9229
Epoch: 1699, Loss: 1.0590
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37707.06it/s]
Epoch: 1700, Loss: 0.8470, Train: 0.9048, Val: 0.9035, test: 0.8977
Epoch: 1701, Loss: 1.0033
Epoch: 1702, Loss: 1.0323
Epoch: 1703, Loss: 1.0285
Epoch: 1704, Loss: 0.7661
Epoch: 1705, Loss: 0.7706
Epoch: 1706, Loss: 0.6878
Epoch: 1707, Loss: 0.7629
Epoch: 1708, Loss: 0.6366
Epoch: 1709, Loss: 0.5909
Epoch: 1710, Loss: 1.0493
Epoch: 1711, Loss: 0.7889
Epoch: 1712, Loss: 0.4897
Epoch: 1713, Loss: 0.6895
Epoch: 1714, Loss: 0.6388
Epoch: 1715, Loss: 0.7786
Epoch: 1716, Loss: 0.9022
Epoch: 1717, Loss: 1.0184
Epoch: 1718, Loss: 0.8613
Epoch: 1719, Loss: 0.8303
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37954.97it/s]
Epoch: 1720, Loss: 0.5978, Train: 0.9201, Val: 0.9159, test: 0.9196
Epoch: 1721, Loss: 0.5409
Epoch: 1722, Loss: 0.5987
Epoch: 1723, Loss: 0.5902
Epoch: 1724, Loss: 1.1090
Epoch: 1725, Loss: 1.4444
Epoch: 1726, Loss: 0.6006
Epoch: 1727, Loss: 0.6229
Epoch: 1728, Loss: 0.7252
Epoch: 1729, Loss: 0.5669
Epoch: 1730, Loss: 0.6916
Epoch: 1731, Loss: 0.6026
Epoch: 1732, Loss: 0.6413
Epoch: 1733, Loss: 0.5575
Epoch: 1734, Loss: 0.5105
Epoch: 1735, Loss: 0.5195
Epoch: 1736, Loss: 0.6558
Epoch: 1737, Loss: 0.5735
Epoch: 1738, Loss: 0.4662
Epoch: 1739, Loss: 0.5150
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36579.33it/s]
Epoch: 1740, Loss: 0.5770, Train: 0.9200, Val: 0.9143, test: 0.9175
Epoch: 1741, Loss: 0.5431
Epoch: 1742, Loss: 0.5218
Epoch: 1743, Loss: 0.5875
Epoch: 1744, Loss: 0.4704
Epoch: 1745, Loss: 0.4650
Epoch: 1746, Loss: 0.4614
Epoch: 1747, Loss: 0.4226
Epoch: 1748, Loss: 0.4773
Epoch: 1749, Loss: 0.4619
Epoch: 1750, Loss: 0.5026
Epoch: 1751, Loss: 0.4591
Epoch: 1752, Loss: 0.7053
Epoch: 1753, Loss: 0.4825
Epoch: 1754, Loss: 0.5768
Epoch: 1755, Loss: 0.4972
Epoch: 1756, Loss: 0.4560
Epoch: 1757, Loss: 0.6119
Epoch: 1758, Loss: 1.0876
Epoch: 1759, Loss: 0.8277
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36207.36it/s]
Epoch: 1760, Loss: 0.4975, Train: 0.9177, Val: 0.9134, test: 0.9124
Epoch: 1761, Loss: 0.5551
Epoch: 1762, Loss: 0.8563
Epoch: 1763, Loss: 0.8294
Epoch: 1764, Loss: 0.9741
Epoch: 1765, Loss: 0.7177
Epoch: 1766, Loss: 0.9145
Epoch: 1767, Loss: 0.5685
Epoch: 1768, Loss: 1.0105
Epoch: 1769, Loss: 0.9643
Epoch: 1770, Loss: 0.6886
Epoch: 1771, Loss: 0.6707
Epoch: 1772, Loss: 0.8676
Epoch: 1773, Loss: 0.7870
Epoch: 1774, Loss: 0.7424
Epoch: 1775, Loss: 0.8373
Epoch: 1776, Loss: 0.8938
Epoch: 1777, Loss: 0.7009
Epoch: 1778, Loss: 0.6470
Epoch: 1779, Loss: 0.6990
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36023.87it/s]
Epoch: 1780, Loss: 0.8318, Train: 0.9166, Val: 0.9125, test: 0.9140
Epoch: 1781, Loss: 0.8029
Epoch: 1782, Loss: 0.7640
Epoch: 1783, Loss: 0.5580
Epoch: 1784, Loss: 0.5765
Epoch: 1785, Loss: 0.6598
Epoch: 1786, Loss: 0.7094
Epoch: 1787, Loss: 0.7973
Epoch: 1788, Loss: 0.7580
Epoch: 1789, Loss: 0.5407
Epoch: 1790, Loss: 0.7113
Epoch: 1791, Loss: 1.1939
Epoch: 1792, Loss: 0.8278
Epoch: 1793, Loss: 0.6064
Epoch: 1794, Loss: 0.6053
Epoch: 1795, Loss: 0.7899
Epoch: 1796, Loss: 0.6857
Epoch: 1797, Loss: 0.8629
Epoch: 1798, Loss: 0.9387
Epoch: 1799, Loss: 0.6376
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36910.48it/s]
Epoch: 1800, Loss: 0.8459, Train: 0.9155, Val: 0.9094, test: 0.9065
Epoch: 1801, Loss: 0.6733
Epoch: 1802, Loss: 0.6932
Epoch: 1803, Loss: 0.5599
Epoch: 1804, Loss: 0.5344
Epoch: 1805, Loss: 0.8693
Epoch: 1806, Loss: 0.5195
Epoch: 1807, Loss: 0.7633
Epoch: 1808, Loss: 0.5878
Epoch: 1809, Loss: 0.5930
Epoch: 1810, Loss: 0.7526
Epoch: 1811, Loss: 0.6061
Epoch: 1812, Loss: 0.5013
Epoch: 1813, Loss: 0.5970
Epoch: 1814, Loss: 0.5005
Epoch: 1815, Loss: 0.5375
Epoch: 1816, Loss: 0.6150
Epoch: 1817, Loss: 0.7546
Epoch: 1818, Loss: 0.5052
Epoch: 1819, Loss: 0.7613
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35860.45it/s]
Epoch: 1820, Loss: 0.5123, Train: 0.9155, Val: 0.9128, test: 0.9080
Epoch: 1821, Loss: 0.5241
Epoch: 1822, Loss: 0.5892
Epoch: 1823, Loss: 0.6261
Epoch: 1824, Loss: 0.4836
Epoch: 1825, Loss: 0.4887
Epoch: 1826, Loss: 0.6385
Epoch: 1827, Loss: 0.7465
Epoch: 1828, Loss: 0.5080
Epoch: 1829, Loss: 0.4384
Epoch: 1830, Loss: 0.4666
Epoch: 1831, Loss: 0.4539
Epoch: 1832, Loss: 0.4671
Epoch: 1833, Loss: 0.5305
Epoch: 1834, Loss: 0.4446
Epoch: 1835, Loss: 0.5112
Epoch: 1836, Loss: 0.4742
Epoch: 1837, Loss: 0.4348
Epoch: 1838, Loss: 0.4579
Epoch: 1839, Loss: 0.6702
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29606.93it/s]
Epoch: 1840, Loss: 0.3940, Train: 0.9149, Val: 0.9128, test: 0.9121
Epoch: 1841, Loss: 0.4501
Epoch: 1842, Loss: 0.5113
Epoch: 1843, Loss: 0.5794
Epoch: 1844, Loss: 0.4456
Epoch: 1845, Loss: 0.4555
Epoch: 1846, Loss: 0.4722
Epoch: 1847, Loss: 0.3745
Epoch: 1848, Loss: 0.4447
Epoch: 1849, Loss: 0.4369
Epoch: 1850, Loss: 0.4263
Epoch: 1851, Loss: 0.4517
Epoch: 1852, Loss: 0.4237
Epoch: 1853, Loss: 0.4275
Epoch: 1854, Loss: 0.4750
Epoch: 1855, Loss: 0.4748
Epoch: 1856, Loss: 0.3653
Epoch: 1857, Loss: 0.4430
Epoch: 1858, Loss: 0.3941
Epoch: 1859, Loss: 0.4497
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35827.30it/s]
Epoch: 1860, Loss: 0.4186, Train: 0.9174, Val: 0.9109, test: 0.9127
Epoch: 1861, Loss: 0.4407
Epoch: 1862, Loss: 0.5033
Epoch: 1863, Loss: 0.3689
Epoch: 1864, Loss: 0.4660
Epoch: 1865, Loss: 0.3750
Epoch: 1866, Loss: 0.4308
Epoch: 1867, Loss: 0.3611
Epoch: 1868, Loss: 0.4813
Epoch: 1869, Loss: 0.3815
Epoch: 1870, Loss: 0.4207
Epoch: 1871, Loss: 0.4151
Epoch: 1872, Loss: 0.3744
Epoch: 1873, Loss: 0.3756
Epoch: 1874, Loss: 0.4088
Epoch: 1875, Loss: 0.3643
Epoch: 1876, Loss: 0.5561
Epoch: 1877, Loss: 0.3926
Epoch: 1878, Loss: 0.5547
Epoch: 1879, Loss: 0.4435
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35735.57it/s]
Epoch: 1880, Loss: 0.4197, Train: 0.9131, Val: 0.9066, test: 0.9099
Epoch: 1881, Loss: 0.5113
Epoch: 1882, Loss: 0.3810
Epoch: 1883, Loss: 0.4133
Epoch: 1884, Loss: 0.4200
Epoch: 1885, Loss: 0.3939
Epoch: 1886, Loss: 0.3961
Epoch: 1887, Loss: 0.4776
Epoch: 1888, Loss: 0.5364
Epoch: 1889, Loss: 0.4791
Epoch: 1890, Loss: 0.4566
Epoch: 1891, Loss: 0.4591
Epoch: 1892, Loss: 0.3937
Epoch: 1893, Loss: 0.4325
Epoch: 1894, Loss: 0.3901
Epoch: 1895, Loss: 0.4706
Epoch: 1896, Loss: 0.4038
Epoch: 1897, Loss: 0.3720
Epoch: 1898, Loss: 0.3969
Epoch: 1899, Loss: 0.5812
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35096.15it/s]
Epoch: 1900, Loss: 0.4957, Train: 0.9167, Val: 0.9119, test: 0.9115
Epoch: 1901, Loss: 0.4148
Epoch: 1902, Loss: 0.4876
Epoch: 1903, Loss: 0.5272
Epoch: 1904, Loss: 0.4890
Epoch: 1905, Loss: 0.5495
Epoch: 1906, Loss: 0.4138
Epoch: 1907, Loss: 0.3694
Epoch: 1908, Loss: 0.4100
Epoch: 1909, Loss: 0.4554
Epoch: 1910, Loss: 0.4198
Epoch: 1911, Loss: 0.4515
Epoch: 1912, Loss: 0.4312
Epoch: 1913, Loss: 0.4081
Epoch: 1914, Loss: 0.4760
Epoch: 1915, Loss: 0.3652
Epoch: 1916, Loss: 0.3953
Epoch: 1917, Loss: 0.4280
Epoch: 1918, Loss: 0.4818
Epoch: 1919, Loss: 0.4431
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35729.01it/s]
Epoch: 1920, Loss: 0.4549, Train: 0.9183, Val: 0.9156, test: 0.9137
Epoch: 1921, Loss: 0.3744
Epoch: 1922, Loss: 0.4810
Epoch: 1923, Loss: 0.4363
Epoch: 1924, Loss: 0.4217
Epoch: 1925, Loss: 0.5315
Epoch: 1926, Loss: 0.4486
Epoch: 1927, Loss: 0.3806
Epoch: 1928, Loss: 0.3995
Epoch: 1929, Loss: 0.4373
Epoch: 1930, Loss: 0.3902
Epoch: 1931, Loss: 0.4327
Epoch: 1932, Loss: 0.3887
Epoch: 1933, Loss: 0.4583
Epoch: 1934, Loss: 0.3872
Epoch: 1935, Loss: 0.3726
Epoch: 1936, Loss: 0.3991
Epoch: 1937, Loss: 0.3888
Epoch: 1938, Loss: 0.4384
Epoch: 1939, Loss: 0.4147
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36798.78it/s]
Epoch: 1940, Loss: 0.3974, Train: 0.9202, Val: 0.9150, test: 0.9171
Epoch: 1941, Loss: 0.3688
Epoch: 1942, Loss: 0.3573
Epoch: 1943, Loss: 0.3505
Epoch: 1944, Loss: 0.3574
Epoch: 1945, Loss: 0.3788
Epoch: 1946, Loss: 0.3699
Epoch: 1947, Loss: 0.3886
Epoch: 1948, Loss: 0.3891
Epoch: 1949, Loss: 0.3972
Epoch: 1950, Loss: 0.4136
Epoch: 1951, Loss: 0.4043
Epoch: 1952, Loss: 0.5719
Epoch: 1953, Loss: 0.4188
Epoch: 1954, Loss: 0.4665
Epoch: 1955, Loss: 0.3800
Epoch: 1956, Loss: 0.3783
Epoch: 1957, Loss: 0.4000
Epoch: 1958, Loss: 0.3942
Epoch: 1959, Loss: 0.4136
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35284.39it/s]
Epoch: 1960, Loss: 0.3848, Train: 0.9191, Val: 0.9131, test: 0.9171
Epoch: 1961, Loss: 0.3848
Epoch: 1962, Loss: 0.4406
Epoch: 1963, Loss: 0.4194
Epoch: 1964, Loss: 0.3795
Epoch: 1965, Loss: 0.3818
Epoch: 1966, Loss: 0.3778
Epoch: 1967, Loss: 0.3521
Epoch: 1968, Loss: 0.3412
Epoch: 1969, Loss: 0.3863
Epoch: 1970, Loss: 0.3550
Epoch: 1971, Loss: 0.4045
Epoch: 1972, Loss: 0.3718
Epoch: 1973, Loss: 0.3337
Epoch: 1974, Loss: 0.3602
Epoch: 1975, Loss: 0.3237
Epoch: 1976, Loss: 0.3736
Epoch: 1977, Loss: 0.3763
Epoch: 1978, Loss: 0.3289
Epoch: 1979, Loss: 0.3784
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37449.40it/s]
Epoch: 1980, Loss: 0.3996, Train: 0.9192, Val: 0.9131, test: 0.9121
Epoch: 1981, Loss: 0.3991
Epoch: 1982, Loss: 0.3801
Epoch: 1983, Loss: 0.3637
Epoch: 1984, Loss: 0.3807
Epoch: 1985, Loss: 0.3533
Epoch: 1986, Loss: 0.3347
Epoch: 1987, Loss: 0.4241
Epoch: 1988, Loss: 0.3221
Epoch: 1989, Loss: 0.3530
Epoch: 1990, Loss: 0.5049
Epoch: 1991, Loss: 0.3721
Epoch: 1992, Loss: 0.4072
Epoch: 1993, Loss: 0.4850
Epoch: 1994, Loss: 0.3684
Epoch: 1995, Loss: 0.4045
Epoch: 1996, Loss: 0.3625
Epoch: 1997, Loss: 0.4437
Epoch: 1998, Loss: 0.3578
Epoch: 1999, Loss: 0.3864
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36867.50it/s]
Epoch: 2000, Loss: 0.3839, Train: 0.9189, Val: 0.9134, test: 0.9149
Epoch: 2001, Loss: 0.3424
Epoch: 2002, Loss: 0.3634
Epoch: 2003, Loss: 0.3369
Epoch: 2004, Loss: 0.3434
Epoch: 2005, Loss: 0.4101
Epoch: 2006, Loss: 0.3933
Epoch: 2007, Loss: 0.3733
Epoch: 2008, Loss: 0.3714
Epoch: 2009, Loss: 0.3156
Epoch: 2010, Loss: 0.3443
Epoch: 2011, Loss: 0.3243
Epoch: 2012, Loss: 0.5672
Epoch: 2013, Loss: 0.4283
Epoch: 2014, Loss: 0.3422
Epoch: 2015, Loss: 0.5100
Epoch: 2016, Loss: 0.4243
Epoch: 2017, Loss: 0.5500
Epoch: 2018, Loss: 0.4822
Epoch: 2019, Loss: 0.3742
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36660.89it/s]
Epoch: 2020, Loss: 0.4453, Train: 0.9195, Val: 0.9137, test: 0.9181
Epoch: 2021, Loss: 0.4318
Epoch: 2022, Loss: 0.4708
Epoch: 2023, Loss: 0.5789
Epoch: 2024, Loss: 0.4038
Epoch: 2025, Loss: 0.4037
Epoch: 2026, Loss: 0.4537
Epoch: 2027, Loss: 0.4296
Epoch: 2028, Loss: 0.3649
Epoch: 2029, Loss: 0.4344
Epoch: 2030, Loss: 0.3632
Epoch: 2031, Loss: 0.3753
Epoch: 2032, Loss: 0.4415
Epoch: 2033, Loss: 0.3476
Epoch: 2034, Loss: 0.3723
Epoch: 2035, Loss: 0.4083
Epoch: 2036, Loss: 0.4535
Epoch: 2037, Loss: 0.3901
Epoch: 2038, Loss: 0.4362
Epoch: 2039, Loss: 0.3645
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36218.50it/s]
Epoch: 2040, Loss: 0.3638, Train: 0.9198, Val: 0.9140, test: 0.9181
Epoch: 2041, Loss: 0.4341
Epoch: 2042, Loss: 0.4112
Epoch: 2043, Loss: 0.4672
Epoch: 2044, Loss: 0.3768
Epoch: 2045, Loss: 0.3398
Epoch: 2046, Loss: 0.3361
Epoch: 2047, Loss: 0.3349
Epoch: 2048, Loss: 0.3717
Epoch: 2049, Loss: 0.3760
Epoch: 2050, Loss: 0.3902
Epoch: 2051, Loss: 0.6345
Epoch: 2052, Loss: 0.5130
Epoch: 2053, Loss: 0.3458
Epoch: 2054, Loss: 0.4008
Epoch: 2055, Loss: 0.3862
Epoch: 2056, Loss: 0.4776
Epoch: 2057, Loss: 0.4013
Epoch: 2058, Loss: 0.4158
Epoch: 2059, Loss: 0.3768
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35900.37it/s]
Epoch: 2060, Loss: 0.4966, Train: 0.9176, Val: 0.9125, test: 0.9127
Epoch: 2061, Loss: 0.4190
Epoch: 2062, Loss: 0.4296
Epoch: 2063, Loss: 0.4159
Epoch: 2064, Loss: 0.3737
Epoch: 2065, Loss: 0.4166
Epoch: 2066, Loss: 0.4214
Epoch: 2067, Loss: 0.3908
Epoch: 2068, Loss: 0.7307
Epoch: 2069, Loss: 0.5222
Epoch: 2070, Loss: 0.4033
Epoch: 2071, Loss: 0.3866
Epoch: 2072, Loss: 0.5238
Epoch: 2073, Loss: 0.4626
Epoch: 2074, Loss: 0.4903
Epoch: 2075, Loss: 0.4685
Epoch: 2076, Loss: 0.4584
Epoch: 2077, Loss: 0.3796
Epoch: 2078, Loss: 0.4132
Epoch: 2079, Loss: 0.3806
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36714.36it/s]
Epoch: 2080, Loss: 0.4658, Train: 0.9193, Val: 0.9131, test: 0.9143
Epoch: 2081, Loss: 0.4631
Epoch: 2082, Loss: 0.5042
Epoch: 2083, Loss: 0.3965
Epoch: 2084, Loss: 0.3823
Epoch: 2085, Loss: 0.5518
Epoch: 2086, Loss: 0.4100
Epoch: 2087, Loss: 0.4174
Epoch: 2088, Loss: 0.4228
Epoch: 2089, Loss: 0.3913
Epoch: 2090, Loss: 0.4605
Epoch: 2091, Loss: 0.3640
Epoch: 2092, Loss: 0.3996
Epoch: 2093, Loss: 0.4536
Epoch: 2094, Loss: 0.6367
Epoch: 2095, Loss: 0.3765
Epoch: 2096, Loss: 0.4095
Epoch: 2097, Loss: 0.3863
Epoch: 2098, Loss: 0.3753
Epoch: 2099, Loss: 0.4390
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37374.49it/s]
Epoch: 2100, Loss: 0.8822, Train: 0.9212, Val: 0.9162, test: 0.9171
Epoch: 2101, Loss: 0.3954
Epoch: 2102, Loss: 0.3938
Epoch: 2103, Loss: 0.4945
Epoch: 2104, Loss: 0.4277
Epoch: 2105, Loss: 0.4631
Epoch: 2106, Loss: 0.4387
Epoch: 2107, Loss: 0.4309
Epoch: 2108, Loss: 0.4031
Epoch: 2109, Loss: 0.4269
Epoch: 2110, Loss: 0.5815
Epoch: 2111, Loss: 0.4859
Epoch: 2112, Loss: 0.4741
Epoch: 2113, Loss: 0.6340
Epoch: 2114, Loss: 0.6394
Epoch: 2115, Loss: 0.4245
Epoch: 2116, Loss: 0.4529
Epoch: 2117, Loss: 0.4051
Epoch: 2118, Loss: 0.4178
Epoch: 2119, Loss: 0.4650
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37704.30it/s]
Epoch: 2120, Loss: 0.4197, Train: 0.9198, Val: 0.9143, test: 0.9175
Epoch: 2121, Loss: 0.4543
Epoch: 2122, Loss: 0.4001
Epoch: 2123, Loss: 0.4469
Epoch: 2124, Loss: 0.5114
Epoch: 2125, Loss: 0.4331
Epoch: 2126, Loss: 0.7049
Epoch: 2127, Loss: 0.4661
Epoch: 2128, Loss: 0.3927
Epoch: 2129, Loss: 0.4708
Epoch: 2130, Loss: 0.4135
Epoch: 2131, Loss: 0.3870
Epoch: 2132, Loss: 0.4072
Epoch: 2133, Loss: 0.3742
Epoch: 2134, Loss: 0.3670
Epoch: 2135, Loss: 0.3695
Epoch: 2136, Loss: 0.4023
Epoch: 2137, Loss: 0.3678
Epoch: 2138, Loss: 0.4716
Epoch: 2139, Loss: 0.4665
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37173.29it/s]
Epoch: 2140, Loss: 0.6209, Train: 0.9189, Val: 0.9150, test: 0.9143
Epoch: 2141, Loss: 0.3887
Epoch: 2142, Loss: 0.4018
Epoch: 2143, Loss: 0.4369
Epoch: 2144, Loss: 0.4382
Epoch: 2145, Loss: 0.4082
Epoch: 2146, Loss: 0.4966
Epoch: 2147, Loss: 0.4121
Epoch: 2148, Loss: 0.3540
Epoch: 2149, Loss: 0.3655
Epoch: 2150, Loss: 0.7343
Epoch: 2151, Loss: 0.4846
Epoch: 2152, Loss: 0.4745
Epoch: 2153, Loss: 0.4460
Epoch: 2154, Loss: 0.4786
Epoch: 2155, Loss: 0.4661
Epoch: 2156, Loss: 0.6337
Epoch: 2157, Loss: 0.5131
Epoch: 2158, Loss: 0.5089
Epoch: 2159, Loss: 0.5133
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30112.81it/s]
Epoch: 2160, Loss: 0.4334, Train: 0.9195, Val: 0.9125, test: 0.9184
Epoch: 2161, Loss: 0.4317
Epoch: 2162, Loss: 0.4444
Epoch: 2163, Loss: 0.3846
Epoch: 2164, Loss: 0.4461
Epoch: 2165, Loss: 0.4701
Epoch: 2166, Loss: 0.3833
Epoch: 2167, Loss: 0.3783
Epoch: 2168, Loss: 0.4797
Epoch: 2169, Loss: 1.1513
Epoch: 2170, Loss: 0.4315
Epoch: 2171, Loss: 0.4827
Epoch: 2172, Loss: 0.4613
Epoch: 2173, Loss: 0.5242
Epoch: 2174, Loss: 0.6166
Epoch: 2175, Loss: 0.6428
Epoch: 2176, Loss: 0.5561
Epoch: 2177, Loss: 0.4021
Epoch: 2178, Loss: 0.5102
Epoch: 2179, Loss: 0.5372
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36462.24it/s]
Epoch: 2180, Loss: 0.4840, Train: 0.9223, Val: 0.9162, test: 0.9231
Epoch: 2181, Loss: 0.4165
Epoch: 2182, Loss: 0.6584
Epoch: 2183, Loss: 0.4226
Epoch: 2184, Loss: 0.4111
Epoch: 2185, Loss: 0.5669
Epoch: 2186, Loss: 0.5157
Epoch: 2187, Loss: 0.4235
Epoch: 2188, Loss: 0.5937
Epoch: 2189, Loss: 0.5545
Epoch: 2190, Loss: 0.4417
Epoch: 2191, Loss: 0.4257
Epoch: 2192, Loss: 0.4606
Epoch: 2193, Loss: 0.4830
Epoch: 2194, Loss: 0.4906
Epoch: 2195, Loss: 0.4680
Epoch: 2196, Loss: 0.4453
Epoch: 2197, Loss: 0.5858
Epoch: 2198, Loss: 0.6169
Epoch: 2199, Loss: 0.5228
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29889.00it/s]
Epoch: 2200, Loss: 0.4908, Train: 0.9177, Val: 0.9119, test: 0.9146
Epoch: 2201, Loss: 0.5257
Epoch: 2202, Loss: 0.6936
Epoch: 2203, Loss: 0.5005
Epoch: 2204, Loss: 0.5207
Epoch: 2205, Loss: 0.6608
Epoch: 2206, Loss: 0.6546
Epoch: 2207, Loss: 0.6344
Epoch: 2208, Loss: 0.5956
Epoch: 2209, Loss: 0.5856
Epoch: 2210, Loss: 0.6076
Epoch: 2211, Loss: 0.7451
Epoch: 2212, Loss: 0.5044
Epoch: 2213, Loss: 0.7067
Epoch: 2214, Loss: 0.4766
Epoch: 2215, Loss: 0.4266
Epoch: 2216, Loss: 0.6182
Epoch: 2217, Loss: 0.5207
Epoch: 2218, Loss: 0.4363
Epoch: 2219, Loss: 0.5055
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30126.75it/s]
Epoch: 2220, Loss: 0.5341, Train: 0.9181, Val: 0.9134, test: 0.9165
Epoch: 2221, Loss: 0.7307
Epoch: 2222, Loss: 0.4715
Epoch: 2223, Loss: 0.4846
Epoch: 2224, Loss: 0.4679
Epoch: 2225, Loss: 0.4807
Epoch: 2226, Loss: 0.4343
Epoch: 2227, Loss: 0.4188
Epoch: 2228, Loss: 0.4176
Epoch: 2229, Loss: 0.4199
Epoch: 2230, Loss: 0.5210
Epoch: 2231, Loss: 0.3907
Epoch: 2232, Loss: 0.4631
Epoch: 2233, Loss: 0.4990
Epoch: 2234, Loss: 0.4019
Epoch: 2235, Loss: 0.4971
Epoch: 2236, Loss: 0.4303
Epoch: 2237, Loss: 0.4774
Epoch: 2238, Loss: 0.5010
Epoch: 2239, Loss: 0.3774
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29809.73it/s]
Epoch: 2240, Loss: 0.5354, Train: 0.9228, Val: 0.9174, test: 0.9215
Epoch: 2241, Loss: 0.3586
Epoch: 2242, Loss: 0.4468
Epoch: 2243, Loss: 0.4020
Epoch: 2244, Loss: 0.4525
Epoch: 2245, Loss: 0.3460
Epoch: 2246, Loss: 0.3607
Epoch: 2247, Loss: 0.4027
Epoch: 2248, Loss: 0.3789
Epoch: 2249, Loss: 0.5252
Epoch: 2250, Loss: 0.4670
Epoch: 2251, Loss: 0.4636
Epoch: 2252, Loss: 0.4210
Epoch: 2253, Loss: 0.3930
Epoch: 2254, Loss: 0.3664
Epoch: 2255, Loss: 0.4122
Epoch: 2256, Loss: 0.3813
Epoch: 2257, Loss: 0.3523
Epoch: 2258, Loss: 0.3619
Epoch: 2259, Loss: 0.8214
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30444.46it/s]
Epoch: 2260, Loss: 0.6038, Train: 0.9179, Val: 0.9128, test: 0.9162
Epoch: 2261, Loss: 0.5009
Epoch: 2262, Loss: 0.4872
Epoch: 2263, Loss: 0.5529
Epoch: 2264, Loss: 0.4750
Epoch: 2265, Loss: 0.5449
Epoch: 2266, Loss: 0.5629
Epoch: 2267, Loss: 0.5663
Epoch: 2268, Loss: 0.5139
Epoch: 2269, Loss: 0.4512
Epoch: 2270, Loss: 0.4198
Epoch: 2271, Loss: 0.5279
Epoch: 2272, Loss: 0.3830
Epoch: 2273, Loss: 0.4175
Epoch: 2274, Loss: 0.3550
Epoch: 2275, Loss: 0.3464
Epoch: 2276, Loss: 0.4169
Epoch: 2277, Loss: 0.3985
Epoch: 2278, Loss: 0.3639
Epoch: 2279, Loss: 0.4651
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35678.25it/s]
Epoch: 2280, Loss: 0.4584, Train: 0.9198, Val: 0.9150, test: 0.9175
Epoch: 2281, Loss: 0.4279
Epoch: 2282, Loss: 0.3845
Epoch: 2283, Loss: 0.4813
Epoch: 2284, Loss: 0.3448
Epoch: 2285, Loss: 0.3465
Epoch: 2286, Loss: 0.3602
Epoch: 2287, Loss: 0.4386
Epoch: 2288, Loss: 0.3792
Epoch: 2289, Loss: 0.7263
Epoch: 2290, Loss: 0.4284
Epoch: 2291, Loss: 0.4412
Epoch: 2292, Loss: 0.3977
Epoch: 2293, Loss: 0.3961
Epoch: 2294, Loss: 0.3943
Epoch: 2295, Loss: 0.5119
Epoch: 2296, Loss: 0.3558
Epoch: 2297, Loss: 0.4075
Epoch: 2298, Loss: 0.3775
Epoch: 2299, Loss: 0.4174
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35594.30it/s]
Epoch: 2300, Loss: 0.3807, Train: 0.9218, Val: 0.9184, test: 0.9203
Epoch: 2301, Loss: 0.4025
Epoch: 2302, Loss: 0.3922
Epoch: 2303, Loss: 0.4320
Epoch: 2304, Loss: 0.3646
Epoch: 2305, Loss: 0.4821
Epoch: 2306, Loss: 0.3774
Epoch: 2307, Loss: 0.3882
Epoch: 2308, Loss: 0.3624
Epoch: 2309, Loss: 0.3547
Epoch: 2310, Loss: 0.3713
Epoch: 2311, Loss: 0.7285
Epoch: 2312, Loss: 0.4376
Epoch: 2313, Loss: 0.4161
Epoch: 2314, Loss: 0.4232
Epoch: 2315, Loss: 0.3791
Epoch: 2316, Loss: 0.4223
Epoch: 2317, Loss: 0.5065
Epoch: 2318, Loss: 0.4312
Epoch: 2319, Loss: 0.3679
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36755.04it/s]
Epoch: 2320, Loss: 0.3471, Train: 0.9214, Val: 0.9181, test: 0.9178
Epoch: 2321, Loss: 0.3541
Epoch: 2322, Loss: 0.3796
Epoch: 2323, Loss: 0.3600
Epoch: 2324, Loss: 0.3975
Epoch: 2325, Loss: 0.3958
Epoch: 2326, Loss: 0.3518
Epoch: 2327, Loss: 0.3620
Epoch: 2328, Loss: 0.3681
Epoch: 2329, Loss: 0.5069
Epoch: 2330, Loss: 0.4590
Epoch: 2331, Loss: 0.5067
Epoch: 2332, Loss: 0.4874
Epoch: 2333, Loss: 0.4644
Epoch: 2334, Loss: 0.4522
Epoch: 2335, Loss: 0.5312
Epoch: 2336, Loss: 0.4394
Epoch: 2337, Loss: 0.3892
Epoch: 2338, Loss: 0.4444
Epoch: 2339, Loss: 0.3941
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37743.97it/s]
Epoch: 2340, Loss: 0.6611, Train: 0.9206, Val: 0.9153, test: 0.9203
Epoch: 2341, Loss: 0.4529
Epoch: 2342, Loss: 0.4983
Epoch: 2343, Loss: 0.4294
Epoch: 2344, Loss: 0.4769
Epoch: 2345, Loss: 0.5187
Epoch: 2346, Loss: 0.4779
Epoch: 2347, Loss: 0.4250
Epoch: 2348, Loss: 0.4175
Epoch: 2349, Loss: 0.4368
Epoch: 2350, Loss: 0.4144
Epoch: 2351, Loss: 0.3885
Epoch: 2352, Loss: 0.3506
Epoch: 2353, Loss: 0.3900
Epoch: 2354, Loss: 0.3681
Epoch: 2355, Loss: 0.3502
Epoch: 2356, Loss: 0.3959
Epoch: 2357, Loss: 0.3585
Epoch: 2358, Loss: 0.3649
Epoch: 2359, Loss: 0.3312
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37361.09it/s]
Epoch: 2360, Loss: 0.5488, Train: 0.9202, Val: 0.9162, test: 0.9171
Epoch: 2361, Loss: 0.3528
Epoch: 2362, Loss: 0.3580
Epoch: 2363, Loss: 0.3526
Epoch: 2364, Loss: 0.3675
Epoch: 2365, Loss: 0.3173
Epoch: 2366, Loss: 1.5238
Epoch: 2367, Loss: 0.3381
Epoch: 2368, Loss: 0.4878
Epoch: 2369, Loss: 0.4084
Epoch: 2370, Loss: 0.4266
Epoch: 2371, Loss: 0.4066
Epoch: 2372, Loss: 0.4494
Epoch: 2373, Loss: 0.4189
Epoch: 2374, Loss: 0.5647
Epoch: 2375, Loss: 0.3888
Epoch: 2376, Loss: 0.3598
Epoch: 2377, Loss: 0.3929
Epoch: 2378, Loss: 0.4711
Epoch: 2379, Loss: 0.3451
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30361.88it/s]
Epoch: 2380, Loss: 0.4047, Train: 0.9208, Val: 0.9168, test: 0.9178
Epoch: 2381, Loss: 0.4216
Epoch: 2382, Loss: 0.4237
Epoch: 2383, Loss: 0.3769
Epoch: 2384, Loss: 0.3592
Epoch: 2385, Loss: 0.4042
Epoch: 2386, Loss: 0.4209
Epoch: 2387, Loss: 0.3753
Epoch: 2388, Loss: 0.4313
Epoch: 2389, Loss: 0.6171
Epoch: 2390, Loss: 0.3585
Epoch: 2391, Loss: 0.5032
Epoch: 2392, Loss: 0.4409
Epoch: 2393, Loss: 0.3754
Epoch: 2394, Loss: 0.4256
Epoch: 2395, Loss: 0.4864
Epoch: 2396, Loss: 0.9006
Epoch: 2397, Loss: 0.4985
Epoch: 2398, Loss: 0.4322
Epoch: 2399, Loss: 0.4643
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29351.18it/s]
Epoch: 2400, Loss: 0.5176, Train: 0.9222, Val: 0.9190, test: 0.9196
Epoch: 2401, Loss: 0.5469
Epoch: 2402, Loss: 0.5259
Epoch: 2403, Loss: 0.9263
Epoch: 2404, Loss: 0.5407
Epoch: 2405, Loss: 0.4814
Epoch: 2406, Loss: 0.4829
Epoch: 2407, Loss: 0.4258
Epoch: 2408, Loss: 0.7742
Epoch: 2409, Loss: 0.4343
Epoch: 2410, Loss: 0.4326
Epoch: 2411, Loss: 0.4492
Epoch: 2412, Loss: 0.3773
Epoch: 2413, Loss: 0.5507
Epoch: 2414, Loss: 0.4934
Epoch: 2415, Loss: 0.4254
Epoch: 2416, Loss: 0.6542
Epoch: 2417, Loss: 0.4519
Epoch: 2418, Loss: 0.4158
Epoch: 2419, Loss: 0.4015
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37429.07it/s]
Epoch: 2420, Loss: 0.3933, Train: 0.9205, Val: 0.9156, test: 0.9193
Epoch: 2421, Loss: 0.4294
Epoch: 2422, Loss: 0.3895
Epoch: 2423, Loss: 0.4042
Epoch: 2424, Loss: 0.4589
Epoch: 2425, Loss: 0.4292
Epoch: 2426, Loss: 0.4243
Epoch: 2427, Loss: 0.3471
Epoch: 2428, Loss: 0.4151
Epoch: 2429, Loss: 0.3908
Epoch: 2430, Loss: 0.3589
Epoch: 2431, Loss: 0.3711
Epoch: 2432, Loss: 0.3334
Epoch: 2433, Loss: 0.4024
Epoch: 2434, Loss: 0.3819
Epoch: 2435, Loss: 0.3786
Epoch: 2436, Loss: 0.3629
Epoch: 2437, Loss: 0.3388
Epoch: 2438, Loss: 0.3440
Epoch: 2439, Loss: 0.3722
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37433.72it/s]
Epoch: 2440, Loss: 0.3635, Train: 0.9208, Val: 0.9165, test: 0.9187
Epoch: 2441, Loss: 0.3698
Epoch: 2442, Loss: 0.3416
Epoch: 2443, Loss: 0.3800
Epoch: 2444, Loss: 0.3228
Epoch: 2445, Loss: 0.3516
Epoch: 2446, Loss: 0.3285
Epoch: 2447, Loss: 0.3372
Epoch: 2448, Loss: 0.3244
Epoch: 2449, Loss: 0.3328
Epoch: 2450, Loss: 0.3586
Epoch: 2451, Loss: 0.3616
Epoch: 2452, Loss: 0.5222
Epoch: 2453, Loss: 0.3333
Epoch: 2454, Loss: 0.3030
Epoch: 2455, Loss: 0.3181
Epoch: 2456, Loss: 0.3203
Epoch: 2457, Loss: 0.3084
Epoch: 2458, Loss: 0.2954
Epoch: 2459, Loss: 0.3101
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36633.42it/s]
Epoch: 2460, Loss: 0.5299, Train: 0.9212, Val: 0.9184, test: 0.9178
Epoch: 2461, Loss: 0.3280
Epoch: 2462, Loss: 0.3079
Epoch: 2463, Loss: 0.3364
Epoch: 2464, Loss: 0.3522
Epoch: 2465, Loss: 0.3162
Epoch: 2466, Loss: 0.3130
Epoch: 2467, Loss: 0.3919
Epoch: 2468, Loss: 0.3465
Epoch: 2469, Loss: 0.3462
Epoch: 2470, Loss: 0.3247
Epoch: 2471, Loss: 0.3995
Epoch: 2472, Loss: 0.3634
Epoch: 2473, Loss: 0.3598
Epoch: 2474, Loss: 0.4018
Epoch: 2475, Loss: 0.4746
Epoch: 2476, Loss: 0.3243
Epoch: 2477, Loss: 0.3653
Epoch: 2478, Loss: 0.3545
Epoch: 2479, Loss: 0.3288
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34946.08it/s]
Epoch: 2480, Loss: 0.3319, Train: 0.9213, Val: 0.9184, test: 0.9193
Epoch: 2481, Loss: 0.3407
Epoch: 2482, Loss: 0.3390
Epoch: 2483, Loss: 0.4624
Epoch: 2484, Loss: 0.2934
Epoch: 2485, Loss: 0.4558
Epoch: 2486, Loss: 0.4233
Epoch: 2487, Loss: 0.3393
Epoch: 2488, Loss: 0.3412
Epoch: 2489, Loss: 0.3631
Epoch: 2490, Loss: 0.3713
Epoch: 2491, Loss: 0.3598
Epoch: 2492, Loss: 0.3757
Epoch: 2493, Loss: 0.3467
Epoch: 2494, Loss: 0.3483
Epoch: 2495, Loss: 0.3639
Epoch: 2496, Loss: 0.3214
Epoch: 2497, Loss: 0.3307
Epoch: 2498, Loss: 0.3202
Epoch: 2499, Loss: 0.3199
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35605.95it/s]
Epoch: 2500, Loss: 0.3047, Train: 0.9209, Val: 0.9165, test: 0.9184
Epoch: 2501, Loss: 0.3049
Epoch: 2502, Loss: 0.3151
Epoch: 2503, Loss: 0.3444
Epoch: 2504, Loss: 0.3344
Epoch: 2505, Loss: 0.3187
Epoch: 2506, Loss: 0.3723
Epoch: 2507, Loss: 0.3133
Epoch: 2508, Loss: 0.3002
Epoch: 2509, Loss: 0.3560
Epoch: 2510, Loss: 0.2862
Epoch: 2511, Loss: 0.2915
Epoch: 2512, Loss: 0.3065
Epoch: 2513, Loss: 0.3225
Epoch: 2514, Loss: 0.3437
Epoch: 2515, Loss: 0.2974
Epoch: 2516, Loss: 0.3000
Epoch: 2517, Loss: 0.3376
Epoch: 2518, Loss: 0.3179
Epoch: 2519, Loss: 0.3498
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36125.05it/s]
Epoch: 2520, Loss: 0.3004, Train: 0.9218, Val: 0.9181, test: 0.9184
Epoch: 2521, Loss: 0.3292
Epoch: 2522, Loss: 0.3168
Epoch: 2523, Loss: 0.3510
Epoch: 2524, Loss: 0.2966
Epoch: 2525, Loss: 0.3215
Epoch: 2526, Loss: 0.3238
Epoch: 2527, Loss: 0.2983
Epoch: 2528, Loss: 0.3584
Epoch: 2529, Loss: 0.3379
Epoch: 2530, Loss: 0.3374
Epoch: 2531, Loss: 0.2978
Epoch: 2532, Loss: 0.3179
Epoch: 2533, Loss: 0.2883
Epoch: 2534, Loss: 0.3090
Epoch: 2535, Loss: 0.2877
Epoch: 2536, Loss: 0.3103
Epoch: 2537, Loss: 0.3527
Epoch: 2538, Loss: 0.3081
Epoch: 2539, Loss: 0.3062
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36641.07it/s]
Epoch: 2540, Loss: 0.3121, Train: 0.9223, Val: 0.9178, test: 0.9187
Epoch: 2541, Loss: 0.2940
Epoch: 2542, Loss: 0.3068
Epoch: 2543, Loss: 0.3139
Epoch: 2544, Loss: 0.2830
Epoch: 2545, Loss: 0.2879
Epoch: 2546, Loss: 0.2897
Epoch: 2547, Loss: 0.2954
Epoch: 2548, Loss: 0.2734
Epoch: 2549, Loss: 0.2774
Epoch: 2550, Loss: 0.3582
Epoch: 2551, Loss: 0.2696
Epoch: 2552, Loss: 0.2749
Epoch: 2553, Loss: 0.2879
Epoch: 2554, Loss: 0.2728
Epoch: 2555, Loss: 0.2723
Epoch: 2556, Loss: 0.2968
Epoch: 2557, Loss: 0.2794
Epoch: 2558, Loss: 0.2777
Epoch: 2559, Loss: 0.3843
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36694.60it/s]
Epoch: 2560, Loss: 0.2955, Train: 0.9211, Val: 0.9187, test: 0.9193
Epoch: 2561, Loss: 0.3557
Epoch: 2562, Loss: 0.3299
Epoch: 2563, Loss: 0.3288
Epoch: 2564, Loss: 0.3564
Epoch: 2565, Loss: 0.3885
Epoch: 2566, Loss: 0.3364
Epoch: 2567, Loss: 0.3685
Epoch: 2568, Loss: 0.3618
Epoch: 2569, Loss: 0.3290
Epoch: 2570, Loss: 0.3637
Epoch: 2571, Loss: 0.4510
Epoch: 2572, Loss: 0.3549
Epoch: 2573, Loss: 0.3775
Epoch: 2574, Loss: 0.3947
Epoch: 2575, Loss: 0.4234
Epoch: 2576, Loss: 0.3723
Epoch: 2577, Loss: 0.4471
Epoch: 2578, Loss: 0.5045
Epoch: 2579, Loss: 0.4574
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35490.32it/s]
Epoch: 2580, Loss: 0.3616, Train: 0.9209, Val: 0.9181, test: 0.9193
Epoch: 2581, Loss: 0.4792
Epoch: 2582, Loss: 0.3712
Epoch: 2583, Loss: 0.3609
Epoch: 2584, Loss: 0.3501
Epoch: 2585, Loss: 0.3863
Epoch: 2586, Loss: 0.3933
Epoch: 2587, Loss: 0.3710
Epoch: 2588, Loss: 0.3983
Epoch: 2589, Loss: 0.3164
Epoch: 2590, Loss: 0.3649
Epoch: 2591, Loss: 0.3839
Epoch: 2592, Loss: 0.3565
Epoch: 2593, Loss: 0.3950
Epoch: 2594, Loss: 0.4049
Epoch: 2595, Loss: 0.3973
Epoch: 2596, Loss: 0.3932
Epoch: 2597, Loss: 0.3949
Epoch: 2598, Loss: 0.3761
Epoch: 2599, Loss: 0.3562
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35725.90it/s]
Epoch: 2600, Loss: 0.3308, Train: 0.9224, Val: 0.9196, test: 0.9228
Epoch: 2601, Loss: 0.3991
Epoch: 2602, Loss: 0.3622
Epoch: 2603, Loss: 0.3402
Epoch: 2604, Loss: 0.3657
Epoch: 2605, Loss: 0.3126
Epoch: 2606, Loss: 0.3339
Epoch: 2607, Loss: 0.3785
Epoch: 2608, Loss: 0.3194
Epoch: 2609, Loss: 0.3461
Epoch: 2610, Loss: 0.3317
Epoch: 2611, Loss: 0.3991
Epoch: 2612, Loss: 0.3584
Epoch: 2613, Loss: 0.3620
Epoch: 2614, Loss: 0.3381
Epoch: 2615, Loss: 0.3963
Epoch: 2616, Loss: 0.3322
Epoch: 2617, Loss: 0.3223
Epoch: 2618, Loss: 0.3881
Epoch: 2619, Loss: 0.3510
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36039.68it/s]
Epoch: 2620, Loss: 0.3107, Train: 0.9218, Val: 0.9178, test: 0.9165
Epoch: 2621, Loss: 0.3239
Epoch: 2622, Loss: 0.3089
Epoch: 2623, Loss: 0.3041
Epoch: 2624, Loss: 0.3508
Epoch: 2625, Loss: 0.3123
Epoch: 2626, Loss: 0.3413
Epoch: 2627, Loss: 0.3392
Epoch: 2628, Loss: 0.3070
Epoch: 2629, Loss: 0.3628
Epoch: 2630, Loss: 0.3033
Epoch: 2631, Loss: 0.2902
Epoch: 2632, Loss: 0.2981
Epoch: 2633, Loss: 0.2982
Epoch: 2634, Loss: 0.3182
Epoch: 2635, Loss: 0.2829
Epoch: 2636, Loss: 0.3130
Epoch: 2637, Loss: 0.2850
Epoch: 2638, Loss: 0.2851
Epoch: 2639, Loss: 0.3514
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35777.51it/s]
Epoch: 2640, Loss: 0.2917, Train: 0.9208, Val: 0.9171, test: 0.9184
Epoch: 2641, Loss: 0.3587
Epoch: 2642, Loss: 0.3117
Epoch: 2643, Loss: 0.3253
Epoch: 2644, Loss: 0.3480
Epoch: 2645, Loss: 0.3137
Epoch: 2646, Loss: 0.3043
Epoch: 2647, Loss: 0.3198
Epoch: 2648, Loss: 0.2959
Epoch: 2649, Loss: 0.4588
Epoch: 2650, Loss: 0.3360
Epoch: 2651, Loss: 0.3040
Epoch: 2652, Loss: 0.3545
Epoch: 2653, Loss: 0.3055
Epoch: 2654, Loss: 0.2941
Epoch: 2655, Loss: 0.3016
Epoch: 2656, Loss: 0.3002
Epoch: 2657, Loss: 0.2818
Epoch: 2658, Loss: 0.2832
Epoch: 2659, Loss: 0.3022
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35099.41it/s]
Epoch: 2660, Loss: 0.3113, Train: 0.9216, Val: 0.9184, test: 0.9187
Epoch: 2661, Loss: 0.3508
Epoch: 2662, Loss: 0.3256
Epoch: 2663, Loss: 0.2838
Epoch: 2664, Loss: 0.3173
Epoch: 2665, Loss: 0.3296
Epoch: 2666, Loss: 0.3063
Epoch: 2667, Loss: 0.2882
Epoch: 2668, Loss: 0.2794
Epoch: 2669, Loss: 0.3187
Epoch: 2670, Loss: 0.3229
Epoch: 2671, Loss: 0.3193
Epoch: 2672, Loss: 0.2780
Epoch: 2673, Loss: 0.3000
Epoch: 2674, Loss: 0.2702
Epoch: 2675, Loss: 0.3530
Epoch: 2676, Loss: 0.3275
Epoch: 2677, Loss: 0.2881
Epoch: 2678, Loss: 0.2941
Epoch: 2679, Loss: 0.3336
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35669.86it/s]
Epoch: 2680, Loss: 0.2909, Train: 0.9216, Val: 0.9181, test: 0.9178
Epoch: 2681, Loss: 0.3028
Epoch: 2682, Loss: 0.2930
Epoch: 2683, Loss: 0.2891
Epoch: 2684, Loss: 0.4091
Epoch: 2685, Loss: 0.3244
Epoch: 2686, Loss: 0.3320
Epoch: 2687, Loss: 0.3016
Epoch: 2688, Loss: 0.3035
Epoch: 2689, Loss: 0.3721
Epoch: 2690, Loss: 0.3759
Epoch: 2691, Loss: 0.2941
Epoch: 2692, Loss: 0.2817
Epoch: 2693, Loss: 0.3026
Epoch: 2694, Loss: 0.2808
Epoch: 2695, Loss: 0.2882
Epoch: 2696, Loss: 0.3788
Epoch: 2697, Loss: 0.2855
Epoch: 2698, Loss: 0.2852
Epoch: 2699, Loss: 0.2801
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35839.48it/s]
Epoch: 2700, Loss: 0.2780, Train: 0.9218, Val: 0.9196, test: 0.9200
Epoch: 2701, Loss: 0.2882
Epoch: 2702, Loss: 0.2849
Epoch: 2703, Loss: 0.2798
Epoch: 2704, Loss: 0.2946
Epoch: 2705, Loss: 0.3136
Epoch: 2706, Loss: 0.2813
Epoch: 2707, Loss: 0.2945
Epoch: 2708, Loss: 0.2845
Epoch: 2709, Loss: 0.3318
Epoch: 2710, Loss: 0.3216
Epoch: 2711, Loss: 0.3012
Epoch: 2712, Loss: 0.2816
Epoch: 2713, Loss: 0.3094
Epoch: 2714, Loss: 0.3638
Epoch: 2715, Loss: 0.3169
Epoch: 2716, Loss: 0.2935
Epoch: 2717, Loss: 0.3209
Epoch: 2718, Loss: 0.2761
Epoch: 2719, Loss: 0.3098
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37950.00it/s]
Epoch: 2720, Loss: 0.3546, Train: 0.9219, Val: 0.9187, test: 0.9187
Epoch: 2721, Loss: 0.2883
Epoch: 2722, Loss: 0.2838
Epoch: 2723, Loss: 0.2827
Epoch: 2724, Loss: 0.2814
Epoch: 2725, Loss: 0.2922
Epoch: 2726, Loss: 0.2744
Epoch: 2727, Loss: 0.3575
Epoch: 2728, Loss: 0.3015
Epoch: 2729, Loss: 0.2986
Epoch: 2730, Loss: 0.2979
Epoch: 2731, Loss: 0.2892
Epoch: 2732, Loss: 0.2915
Epoch: 2733, Loss: 0.2981
Epoch: 2734, Loss: 0.2933
Epoch: 2735, Loss: 0.2845
Epoch: 2736, Loss: 0.2906
Epoch: 2737, Loss: 0.4704
Epoch: 2738, Loss: 0.2933
Epoch: 2739, Loss: 0.3018
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37701.36it/s]
Epoch: 2740, Loss: 0.2936, Train: 0.9215, Val: 0.9187, test: 0.9200
Epoch: 2741, Loss: 0.3057
Epoch: 2742, Loss: 0.3170
Epoch: 2743, Loss: 0.3201
Epoch: 2744, Loss: 0.3637
Epoch: 2745, Loss: 0.3099
Epoch: 2746, Loss: 0.3024
Epoch: 2747, Loss: 0.2911
Epoch: 2748, Loss: 0.2941
Epoch: 2749, Loss: 0.3234
Epoch: 2750, Loss: 0.2986
Epoch: 2751, Loss: 0.2903
Epoch: 2752, Loss: 0.2901
Epoch: 2753, Loss: 0.3126
Epoch: 2754, Loss: 0.2945
Epoch: 2755, Loss: 0.3062
Epoch: 2756, Loss: 0.3488
Epoch: 2757, Loss: 0.2988
Epoch: 2758, Loss: 0.2831
Epoch: 2759, Loss: 0.2851
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38030.16it/s]
Epoch: 2760, Loss: 0.2967, Train: 0.9208, Val: 0.9171, test: 0.9171
Epoch: 2761, Loss: 0.3285
Epoch: 2762, Loss: 0.3101
Epoch: 2763, Loss: 0.2809
Epoch: 2764, Loss: 0.2936
Epoch: 2765, Loss: 0.3471
Epoch: 2766, Loss: 0.3561
Epoch: 2767, Loss: 0.3248
Epoch: 2768, Loss: 0.3560
Epoch: 2769, Loss: 0.3999
Epoch: 2770, Loss: 0.4284
Epoch: 2771, Loss: 0.3558
Epoch: 2772, Loss: 0.3601
Epoch: 2773, Loss: 0.3718
Epoch: 2774, Loss: 0.3455
Epoch: 2775, Loss: 0.3528
Epoch: 2776, Loss: 0.3412
Epoch: 2777, Loss: 0.3133
Epoch: 2778, Loss: 0.3289
Epoch: 2779, Loss: 0.3284
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37352.81it/s]
Epoch: 2780, Loss: 0.3393, Train: 0.9209, Val: 0.9171, test: 0.9181
Epoch: 2781, Loss: 0.3471
Epoch: 2782, Loss: 0.4199
Epoch: 2783, Loss: 0.3214
Epoch: 2784, Loss: 0.3834
Epoch: 2785, Loss: 0.3791
Epoch: 2786, Loss: 0.3654
Epoch: 2787, Loss: 0.3626
Epoch: 2788, Loss: 0.3767
Epoch: 2789, Loss: 0.3605
Epoch: 2790, Loss: 0.3911
Epoch: 2791, Loss: 0.3859
Epoch: 2792, Loss: 0.3505
Epoch: 2793, Loss: 0.3190
Epoch: 2794, Loss: 0.3174
Epoch: 2795, Loss: 0.3365
Epoch: 2796, Loss: 0.3118
Epoch: 2797, Loss: 0.2985
Epoch: 2798, Loss: 0.3247
Epoch: 2799, Loss: 0.3228
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37796.79it/s]
Epoch: 2800, Loss: 0.3122, Train: 0.9213, Val: 0.9187, test: 0.9184
Epoch: 2801, Loss: 0.3211
Epoch: 2802, Loss: 0.2930
Epoch: 2803, Loss: 0.2965
Epoch: 2804, Loss: 0.3099
Epoch: 2805, Loss: 0.4313
Epoch: 2806, Loss: 0.3138
Epoch: 2807, Loss: 0.3153
Epoch: 2808, Loss: 0.3135
Epoch: 2809, Loss: 0.3005
Epoch: 2810, Loss: 0.2889
Epoch: 2811, Loss: 0.2916
Epoch: 2812, Loss: 0.2867
Epoch: 2813, Loss: 0.3722
Epoch: 2814, Loss: 0.4149
Epoch: 2815, Loss: 0.2843
Epoch: 2816, Loss: 0.2826
Epoch: 2817, Loss: 0.2779
Epoch: 2818, Loss: 0.2945
Epoch: 2819, Loss: 0.2866
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38098.86it/s]
Epoch: 2820, Loss: 0.2767, Train: 0.9210, Val: 0.9171, test: 0.9153
Epoch: 2821, Loss: 0.2783
Epoch: 2822, Loss: 0.2907
Epoch: 2823, Loss: 0.2761
Epoch: 2824, Loss: 0.2833
Epoch: 2825, Loss: 0.2723
Epoch: 2826, Loss: 0.2719
Epoch: 2827, Loss: 0.2831
Epoch: 2828, Loss: 0.2734
Epoch: 2829, Loss: 0.2692
Epoch: 2830, Loss: 0.2887
Epoch: 2831, Loss: 0.3405
Epoch: 2832, Loss: 0.2984
Epoch: 2833, Loss: 0.3092
Epoch: 2834, Loss: 0.2878
Epoch: 2835, Loss: 0.2924
Epoch: 2836, Loss: 0.2828
Epoch: 2837, Loss: 0.2830
Epoch: 2838, Loss: 0.2731
Epoch: 2839, Loss: 0.2794
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38056.72it/s]
Epoch: 2840, Loss: 0.2806, Train: 0.9215, Val: 0.9187, test: 0.9200
Epoch: 2841, Loss: 0.2800
Epoch: 2842, Loss: 0.3009
Epoch: 2843, Loss: 0.2660
Epoch: 2844, Loss: 0.2832
Epoch: 2845, Loss: 0.2681
Epoch: 2846, Loss: 0.2820
Epoch: 2847, Loss: 0.2680
Epoch: 2848, Loss: 0.3701
Epoch: 2849, Loss: 0.2808
Epoch: 2850, Loss: 0.2906
Epoch: 2851, Loss: 0.3434
Epoch: 2852, Loss: 0.2840
Epoch: 2853, Loss: 0.2701
Epoch: 2854, Loss: 0.3021
Epoch: 2855, Loss: 0.2858
Epoch: 2856, Loss: 0.2676
Epoch: 2857, Loss: 0.3450
Epoch: 2858, Loss: 0.2751
Epoch: 2859, Loss: 0.2867
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35970.27it/s]
Epoch: 2860, Loss: 0.2950, Train: 0.9224, Val: 0.9174, test: 0.9196
Epoch: 2861, Loss: 0.2884
Epoch: 2862, Loss: 0.2810
Epoch: 2863, Loss: 0.2837
Epoch: 2864, Loss: 0.2828
Epoch: 2865, Loss: 0.3114
Epoch: 2866, Loss: 0.2791
Epoch: 2867, Loss: 0.2874
Epoch: 2868, Loss: 0.2853
Epoch: 2869, Loss: 0.3889
Epoch: 2870, Loss: 0.6130
Epoch: 2871, Loss: 0.4030
Epoch: 2872, Loss: 0.4964
Epoch: 2873, Loss: 0.4250
Epoch: 2874, Loss: 0.4539
Epoch: 2875, Loss: 0.4376
Epoch: 2876, Loss: 0.4921
Epoch: 2877, Loss: 0.4601
Epoch: 2878, Loss: 0.4183
Epoch: 2879, Loss: 0.4792
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30180.25it/s]
Epoch: 2880, Loss: 0.4667, Train: 0.9199, Val: 0.9168, test: 0.9190
Epoch: 2881, Loss: 0.3992
Epoch: 2882, Loss: 0.4436
Epoch: 2883, Loss: 0.5390
Epoch: 2884, Loss: 0.4624
Epoch: 2885, Loss: 0.4200
Epoch: 2886, Loss: 0.4418
Epoch: 2887, Loss: 0.4165
Epoch: 2888, Loss: 0.4287
Epoch: 2889, Loss: 0.6192
Epoch: 2890, Loss: 0.4861
Epoch: 2891, Loss: 0.4960
Epoch: 2892, Loss: 0.4741
Epoch: 2893, Loss: 0.4572
Epoch: 2894, Loss: 0.4826
Epoch: 2895, Loss: 0.6881
Epoch: 2896, Loss: 0.5176
Epoch: 2897, Loss: 0.4204
Epoch: 2898, Loss: 0.4008
Epoch: 2899, Loss: 0.4811
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30357.68it/s]
Epoch: 2900, Loss: 0.3915, Train: 0.9199, Val: 0.9162, test: 0.9159
Epoch: 2901, Loss: 0.3764
Epoch: 2902, Loss: 0.4531
Epoch: 2903, Loss: 0.3458
Epoch: 2904, Loss: 0.3901
Epoch: 2905, Loss: 0.3913
Epoch: 2906, Loss: 0.3405
Epoch: 2907, Loss: 0.3462
Epoch: 2908, Loss: 0.4065
Epoch: 2909, Loss: 0.3329
Epoch: 2910, Loss: 0.4326
Epoch: 2911, Loss: 0.3199
Epoch: 2912, Loss: 0.3312
Epoch: 2913, Loss: 0.3236
Epoch: 2914, Loss: 0.3565
Epoch: 2915, Loss: 0.3525
Epoch: 2916, Loss: 0.3256
Epoch: 2917, Loss: 0.3346
Epoch: 2918, Loss: 0.3033
Epoch: 2919, Loss: 0.3177
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37691.02it/s]
Epoch: 2920, Loss: 0.3034, Train: 0.9197, Val: 0.9162, test: 0.9153
Epoch: 2921, Loss: 0.4124
Epoch: 2922, Loss: 0.3220
Epoch: 2923, Loss: 0.3140
Epoch: 2924, Loss: 0.3421
Epoch: 2925, Loss: 0.3288
Epoch: 2926, Loss: 0.3195
Epoch: 2927, Loss: 0.3609
Epoch: 2928, Loss: 0.3162
Epoch: 2929, Loss: 0.3042
Epoch: 2930, Loss: 0.3694
Epoch: 2931, Loss: 0.2903
Epoch: 2932, Loss: 0.3200
Epoch: 2933, Loss: 0.3312
Epoch: 2934, Loss: 0.3243
Epoch: 2935, Loss: 0.3324
Epoch: 2936, Loss: 0.2801
Epoch: 2937, Loss: 0.4639
Epoch: 2938, Loss: 0.2974
Epoch: 2939, Loss: 0.2937
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36567.30it/s]
Epoch: 2940, Loss: 0.2922, Train: 0.9201, Val: 0.9174, test: 0.9165
Epoch: 2941, Loss: 0.2855
Epoch: 2942, Loss: 0.2851
Epoch: 2943, Loss: 0.3052
Epoch: 2944, Loss: 0.3020
Epoch: 2945, Loss: 0.2874
Epoch: 2946, Loss: 0.2834
Epoch: 2947, Loss: 0.3563
Epoch: 2948, Loss: 0.2833
Epoch: 2949, Loss: 0.3097
Epoch: 2950, Loss: 0.2920
Epoch: 2951, Loss: 0.2771
Epoch: 2952, Loss: 0.2830
Epoch: 2953, Loss: 0.2945
Epoch: 2954, Loss: 0.3260
Epoch: 2955, Loss: 0.6962
Epoch: 2956, Loss: 0.3872
Epoch: 2957, Loss: 0.4193
Epoch: 2958, Loss: 0.4675
Epoch: 2959, Loss: 0.4304
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36741.97it/s]
Epoch: 2960, Loss: 0.4364, Train: 0.9194, Val: 0.9171, test: 0.9159
Epoch: 2961, Loss: 0.3930
Epoch: 2962, Loss: 0.4252
Epoch: 2963, Loss: 0.3813
Epoch: 2964, Loss: 0.4622
Epoch: 2965, Loss: 0.3727
Epoch: 2966, Loss: 0.3986
Epoch: 2967, Loss: 0.3580
Epoch: 2968, Loss: 0.3232
Epoch: 2969, Loss: 0.3499
Epoch: 2970, Loss: 0.3920
Epoch: 2971, Loss: 0.3737
Epoch: 2972, Loss: 0.3921
Epoch: 2973, Loss: 0.3666
Epoch: 2974, Loss: 0.3326
Epoch: 2975, Loss: 0.3305
Epoch: 2976, Loss: 0.3293
Epoch: 2977, Loss: 0.3169
Epoch: 2978, Loss: 0.3305
Epoch: 2979, Loss: 0.3199
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36551.52it/s]
Epoch: 2980, Loss: 0.3129, Train: 0.9196, Val: 0.9146, test: 0.9168
Epoch: 2981, Loss: 0.3094
Epoch: 2982, Loss: 0.2934
Epoch: 2983, Loss: 0.2947
Epoch: 2984, Loss: 0.4090
Epoch: 2985, Loss: 0.4092
Epoch: 2986, Loss: 0.2901
Epoch: 2987, Loss: 0.3440
Epoch: 2988, Loss: 0.3797
Epoch: 2989, Loss: 0.3039
Epoch: 2990, Loss: 0.2977
Epoch: 2991, Loss: 0.3122
Epoch: 2992, Loss: 0.2909
Epoch: 2993, Loss: 0.3085
Epoch: 2994, Loss: 0.3429
Epoch: 2995, Loss: 0.3132
Epoch: 2996, Loss: 0.2765
Epoch: 2997, Loss: 0.2939
Epoch: 2998, Loss: 0.2710
Epoch: 2999, Loss: 0.2811
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36419.09it/s]
Epoch: 3000, Loss: 0.2774, Train: 0.9206, Val: 0.9168, test: 0.9156
Epoch: 3001, Loss: 0.3022
Epoch: 3002, Loss: 0.2732
Epoch: 3003, Loss: 0.2748
Epoch: 3004, Loss: 0.2882
Epoch: 3005, Loss: 0.2884
Epoch: 3006, Loss: 0.2748
Epoch: 3007, Loss: 0.2894
Epoch: 3008, Loss: 0.2705
Epoch: 3009, Loss: 0.2771
Epoch: 3010, Loss: 0.2800
Epoch: 3011, Loss: 0.2656
Epoch: 3012, Loss: 0.2657
Epoch: 3013, Loss: 0.2681
Epoch: 3014, Loss: 0.2689
Epoch: 3015, Loss: 0.2784
Epoch: 3016, Loss: 0.2766
Epoch: 3017, Loss: 0.2793
Epoch: 3018, Loss: 0.2717
Epoch: 3019, Loss: 0.2641
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35425.55it/s]
Epoch: 3020, Loss: 0.2965, Train: 0.9202, Val: 0.9174, test: 0.9165
Epoch: 3021, Loss: 0.2673
Epoch: 3022, Loss: 0.2721
Epoch: 3023, Loss: 0.2831
Epoch: 3024, Loss: 0.2644
Epoch: 3025, Loss: 0.2675
Epoch: 3026, Loss: 0.2671
Epoch: 3027, Loss: 0.3508
Epoch: 3028, Loss: 0.3150
Epoch: 3029, Loss: 0.2827
Epoch: 3030, Loss: 0.2853
Epoch: 3031, Loss: 0.2927
Epoch: 3032, Loss: 0.2922
Epoch: 3033, Loss: 0.3176
Epoch: 3034, Loss: 0.3139
Epoch: 3035, Loss: 0.3298
Epoch: 3036, Loss: 0.2973
Epoch: 3037, Loss: 0.2955
Epoch: 3038, Loss: 0.2991
Epoch: 3039, Loss: 0.3312
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37402.33it/s]
Epoch: 3040, Loss: 0.3588, Train: 0.9200, Val: 0.9181, test: 0.9165
Epoch: 3041, Loss: 0.3543
Epoch: 3042, Loss: 0.3140
Epoch: 3043, Loss: 0.3319
Epoch: 3044, Loss: 0.3320
Epoch: 3045, Loss: 0.3416
Epoch: 3046, Loss: 0.3077
Epoch: 3047, Loss: 0.3060
Epoch: 3048, Loss: 0.3262
Epoch: 3049, Loss: 0.3624
Epoch: 3050, Loss: 0.4015
Epoch: 3051, Loss: 0.3365
Epoch: 3052, Loss: 0.3077
Epoch: 3053, Loss: 0.3101
Epoch: 3054, Loss: 0.4770
Epoch: 3055, Loss: 0.3205
Epoch: 3056, Loss: 0.3220
Epoch: 3057, Loss: 0.3098
Epoch: 3058, Loss: 0.3117
Epoch: 3059, Loss: 0.3539
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37392.61it/s]
Epoch: 3060, Loss: 0.3249, Train: 0.9197, Val: 0.9159, test: 0.9162
Epoch: 3061, Loss: 0.3014
Epoch: 3062, Loss: 0.3417
Epoch: 3063, Loss: 0.3249
Epoch: 3064, Loss: 0.2962
Epoch: 3065, Loss: 0.2896
Epoch: 3066, Loss: 0.2773
Epoch: 3067, Loss: 0.3026
Epoch: 3068, Loss: 0.2851
Epoch: 3069, Loss: 0.3080
Epoch: 3070, Loss: 0.2755
Epoch: 3071, Loss: 0.2692
Epoch: 3072, Loss: 0.2991
Epoch: 3073, Loss: 0.2779
Epoch: 3074, Loss: 0.2968
Epoch: 3075, Loss: 0.2681
Epoch: 3076, Loss: 0.3002
Epoch: 3077, Loss: 0.2704
Epoch: 3078, Loss: 0.2678
Epoch: 3079, Loss: 0.2654
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37088.77it/s]
Epoch: 3080, Loss: 0.2961, Train: 0.9201, Val: 0.9150, test: 0.9156
Epoch: 3081, Loss: 0.2846
Epoch: 3082, Loss: 0.2734
Epoch: 3083, Loss: 0.2814
Epoch: 3084, Loss: 0.2684
Epoch: 3085, Loss: 0.3003
Epoch: 3086, Loss: 0.2620
Epoch: 3087, Loss: 0.2636
Epoch: 3088, Loss: 0.2701
Epoch: 3089, Loss: 0.2825
Epoch: 3090, Loss: 0.2630
Epoch: 3091, Loss: 0.2742
Epoch: 3092, Loss: 0.2673
Epoch: 3093, Loss: 0.2745
Epoch: 3094, Loss: 0.2737
Epoch: 3095, Loss: 0.2870
Epoch: 3096, Loss: 0.2690
Epoch: 3097, Loss: 0.3164
Epoch: 3098, Loss: 0.3113
Epoch: 3099, Loss: 0.2634
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37184.18it/s]
Epoch: 3100, Loss: 0.2860, Train: 0.9206, Val: 0.9181, test: 0.9162
Epoch: 3101, Loss: 0.2808
Epoch: 3102, Loss: 0.2653
Epoch: 3103, Loss: 0.3552
Epoch: 3104, Loss: 0.2686
Epoch: 3105, Loss: 0.2734
Epoch: 3106, Loss: 0.2690
Epoch: 3107, Loss: 0.2692
Epoch: 3108, Loss: 0.2929
Epoch: 3109, Loss: 0.2627
Epoch: 3110, Loss: 0.2739
Epoch: 3111, Loss: 0.2623
Epoch: 3112, Loss: 0.2721
Epoch: 3113, Loss: 0.2708
Epoch: 3114, Loss: 0.2604
Epoch: 3115, Loss: 0.2673
Epoch: 3116, Loss: 0.2753
Epoch: 3117, Loss: 0.2596
Epoch: 3118, Loss: 0.3067
Epoch: 3119, Loss: 0.2605
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35970.63it/s]
Epoch: 3120, Loss: 0.2635, Train: 0.9206, Val: 0.9165, test: 0.9159
Epoch: 3121, Loss: 0.2670
Epoch: 3122, Loss: 0.2608
Epoch: 3123, Loss: 0.2757
Epoch: 3124, Loss: 0.2634
Epoch: 3125, Loss: 0.2582
Epoch: 3126, Loss: 0.2687
Epoch: 3127, Loss: 0.2607
Epoch: 3128, Loss: 0.2668
Epoch: 3129, Loss: 0.2975
Epoch: 3130, Loss: 0.2641
Epoch: 3131, Loss: 0.2716
Epoch: 3132, Loss: 0.2586
Epoch: 3133, Loss: 0.2621
Epoch: 3134, Loss: 0.2677
Epoch: 3135, Loss: 0.2697
Epoch: 3136, Loss: 0.2658
Epoch: 3137, Loss: 0.2624
Epoch: 3138, Loss: 0.2564
Epoch: 3139, Loss: 0.2556
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35855.03it/s]
Epoch: 3140, Loss: 0.2572, Train: 0.9196, Val: 0.9165, test: 0.9159
Epoch: 3141, Loss: 0.2623
Epoch: 3142, Loss: 0.2662
Epoch: 3143, Loss: 0.2787
Epoch: 3144, Loss: 0.2674
Epoch: 3145, Loss: 0.2554
Epoch: 3146, Loss: 0.2574
Epoch: 3147, Loss: 0.2740
Epoch: 3148, Loss: 0.2691
Epoch: 3149, Loss: 0.2690
Epoch: 3150, Loss: 0.7968
Epoch: 3151, Loss: 0.4592
Epoch: 3152, Loss: 0.3400
Epoch: 3153, Loss: 0.3428
Epoch: 3154, Loss: 0.3975
Epoch: 3155, Loss: 0.3796
Epoch: 3156, Loss: 0.4931
Epoch: 3157, Loss: 0.4466
Epoch: 3158, Loss: 0.3775
Epoch: 3159, Loss: 0.4125
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35384.54it/s]
Epoch: 3160, Loss: 0.3846, Train: 0.9188, Val: 0.9156, test: 0.9181
Epoch: 3161, Loss: 0.3872
Epoch: 3162, Loss: 0.3616
Epoch: 3163, Loss: 0.3311
Epoch: 3164, Loss: 0.3117
Epoch: 3165, Loss: 0.2993
Epoch: 3166, Loss: 0.3054
Epoch: 3167, Loss: 0.2987
Epoch: 3168, Loss: 0.3051
Epoch: 3169, Loss: 0.2988
Epoch: 3170, Loss: 0.6535
Epoch: 3171, Loss: 0.3033
Epoch: 3172, Loss: 0.3002
Epoch: 3173, Loss: 0.3097
Epoch: 3174, Loss: 0.2826
Epoch: 3175, Loss: 0.2966
Epoch: 3176, Loss: 0.2804
Epoch: 3177, Loss: 0.2796
Epoch: 3178, Loss: 0.2909
Epoch: 3179, Loss: 0.2969
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35958.29it/s]
Epoch: 3180, Loss: 0.3005, Train: 0.9198, Val: 0.9168, test: 0.9171
Epoch: 3181, Loss: 0.2925
Epoch: 3182, Loss: 0.2861
Epoch: 3183, Loss: 0.2924
Epoch: 3184, Loss: 0.3049
Epoch: 3185, Loss: 0.2761
Epoch: 3186, Loss: 0.2684
Epoch: 3187, Loss: 0.3088
Epoch: 3188, Loss: 0.2686
Epoch: 3189, Loss: 0.2797
Epoch: 3190, Loss: 0.2769
Epoch: 3191, Loss: 0.2691
Epoch: 3192, Loss: 0.2788
Epoch: 3193, Loss: 0.2690
Epoch: 3194, Loss: 0.2794
Epoch: 3195, Loss: 0.2945
Epoch: 3196, Loss: 0.2889
Epoch: 3197, Loss: 0.2695
Epoch: 3198, Loss: 0.2715
Epoch: 3199, Loss: 0.2964
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35634.74it/s]
Epoch: 3200, Loss: 0.2804, Train: 0.9202, Val: 0.9171, test: 0.9162
Epoch: 3201, Loss: 0.2757
Epoch: 3202, Loss: 0.2741
Epoch: 3203, Loss: 0.2768
Epoch: 3204, Loss: 0.2664
Epoch: 3205, Loss: 0.2741
Epoch: 3206, Loss: 0.2608
Epoch: 3207, Loss: 0.2954
Epoch: 3208, Loss: 0.2650
Epoch: 3209, Loss: 0.2639
Epoch: 3210, Loss: 0.2687
Epoch: 3211, Loss: 0.2602
Epoch: 3212, Loss: 0.2647
Epoch: 3213, Loss: 0.2584
Epoch: 3214, Loss: 0.2650
Epoch: 3215, Loss: 0.2609
Epoch: 3216, Loss: 0.2629
Epoch: 3217, Loss: 0.2584
Epoch: 3218, Loss: 0.2571
Epoch: 3219, Loss: 0.2844
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30211.37it/s]
Epoch: 3220, Loss: 0.2646, Train: 0.9206, Val: 0.9184, test: 0.9168
Epoch: 3221, Loss: 0.2569
Epoch: 3222, Loss: 0.2572
Epoch: 3223, Loss: 0.2554
Epoch: 3224, Loss: 0.2598
Epoch: 3225, Loss: 0.2576
Epoch: 3226, Loss: 0.2552
Epoch: 3227, Loss: 0.2587
Epoch: 3228, Loss: 0.2535
Epoch: 3229, Loss: 0.2589
Epoch: 3230, Loss: 0.2615
Epoch: 3231, Loss: 0.2636
Epoch: 3232, Loss: 0.2649
Epoch: 3233, Loss: 0.2576
Epoch: 3234, Loss: 0.2713
Epoch: 3235, Loss: 0.2537
Epoch: 3236, Loss: 0.2604
Epoch: 3237, Loss: 0.2560
Epoch: 3238, Loss: 0.2536
Epoch: 3239, Loss: 0.2528
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35546.32it/s]
Epoch: 3240, Loss: 0.2586, Train: 0.9207, Val: 0.9181, test: 0.9168
Epoch: 3241, Loss: 0.2565
Epoch: 3242, Loss: 0.2584
Epoch: 3243, Loss: 0.2553
Epoch: 3244, Loss: 0.2537
Epoch: 3245, Loss: 0.2586
Epoch: 3246, Loss: 0.2725
Epoch: 3247, Loss: 0.3028
Epoch: 3248, Loss: 0.2544
Epoch: 3249, Loss: 0.2584
Epoch: 3250, Loss: 0.2733
Epoch: 3251, Loss: 0.2735
Epoch: 3252, Loss: 0.2637
Epoch: 3253, Loss: 0.2662
Epoch: 3254, Loss: 0.2602
Epoch: 3255, Loss: 0.3012
Epoch: 3256, Loss: 0.2775
Epoch: 3257, Loss: 0.2560
Epoch: 3258, Loss: 0.2553
Epoch: 3259, Loss: 0.2690
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35982.55it/s]
Epoch: 3260, Loss: 0.2691, Train: 0.9222, Val: 0.9196, test: 0.9190
Epoch: 3261, Loss: 0.3327
Epoch: 3262, Loss: 0.2666
Epoch: 3263, Loss: 0.2608
Epoch: 3264, Loss: 0.2569
Epoch: 3265, Loss: 0.2667
Epoch: 3266, Loss: 0.2657
Epoch: 3267, Loss: 0.2589
Epoch: 3268, Loss: 0.2569
Epoch: 3269, Loss: 0.2591
Epoch: 3270, Loss: 0.2626
Epoch: 3271, Loss: 0.2730
Epoch: 3272, Loss: 0.2600
Epoch: 3273, Loss: 0.2534
Epoch: 3274, Loss: 0.2619
Epoch: 3275, Loss: 0.2763
Epoch: 3276, Loss: 0.2528
Epoch: 3277, Loss: 0.2678
Epoch: 3278, Loss: 0.2703
Epoch: 3279, Loss: 0.2603
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37829.04it/s]
Epoch: 3280, Loss: 0.2607, Train: 0.9220, Val: 0.9184, test: 0.9175
Epoch: 3281, Loss: 0.2684
Epoch: 3282, Loss: 0.2567
Epoch: 3283, Loss: 0.2563
Epoch: 3284, Loss: 0.2543
Epoch: 3285, Loss: 0.2537
Epoch: 3286, Loss: 0.2615
Epoch: 3287, Loss: 0.2523
Epoch: 3288, Loss: 0.2607
Epoch: 3289, Loss: 0.2578
Epoch: 3290, Loss: 0.2546
Epoch: 3291, Loss: 0.2573
Epoch: 3292, Loss: 0.2494
Epoch: 3293, Loss: 0.2570
Epoch: 3294, Loss: 0.2543
Epoch: 3295, Loss: 0.2483
Epoch: 3296, Loss: 0.2505
Epoch: 3297, Loss: 0.2521
Epoch: 3298, Loss: 0.2502
Epoch: 3299, Loss: 0.2826
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36036.65it/s]
Epoch: 3300, Loss: 0.2544, Train: 0.9212, Val: 0.9178, test: 0.9175
Epoch: 3301, Loss: 0.2588
Epoch: 3302, Loss: 0.2828
Epoch: 3303, Loss: 0.2611
Epoch: 3304, Loss: 0.2517
Epoch: 3305, Loss: 0.2523
Epoch: 3306, Loss: 0.2512
Epoch: 3307, Loss: 0.2564
Epoch: 3308, Loss: 0.2575
Epoch: 3309, Loss: 0.2555
Epoch: 3310, Loss: 0.2589
Epoch: 3311, Loss: 0.2624
Epoch: 3312, Loss: 0.2562
Epoch: 3313, Loss: 0.2536
Epoch: 3314, Loss: 0.2616
Epoch: 3315, Loss: 0.2539
Epoch: 3316, Loss: 0.2539
Epoch: 3317, Loss: 0.2506
Epoch: 3318, Loss: 0.2525
Epoch: 3319, Loss: 0.2521
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35659.49it/s]
Epoch: 3320, Loss: 0.2666, Train: 0.9213, Val: 0.9178, test: 0.9196
Epoch: 3321, Loss: 0.2535
Epoch: 3322, Loss: 0.2582
Epoch: 3323, Loss: 0.2547
Epoch: 3324, Loss: 0.2611
Epoch: 3325, Loss: 0.2556
Epoch: 3326, Loss: 0.2511
Epoch: 3327, Loss: 0.2564
Epoch: 3328, Loss: 0.2624
Epoch: 3329, Loss: 0.2561
Epoch: 3330, Loss: 0.2596
Epoch: 3331, Loss: 0.2814
Epoch: 3332, Loss: 0.2601
Epoch: 3333, Loss: 0.2687
Epoch: 3334, Loss: 0.2574
Epoch: 3335, Loss: 0.2675
Epoch: 3336, Loss: 0.2577
Epoch: 3337, Loss: 0.2624
Epoch: 3338, Loss: 0.2567
Epoch: 3339, Loss: 0.2581
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36183.16it/s]
Epoch: 3340, Loss: 0.2532, Train: 0.9224, Val: 0.9199, test: 0.9196
Epoch: 3341, Loss: 0.2558
Epoch: 3342, Loss: 0.2555
Epoch: 3343, Loss: 0.2664
Epoch: 3344, Loss: 0.2590
Epoch: 3345, Loss: 0.2585
Epoch: 3346, Loss: 0.2661
Epoch: 3347, Loss: 0.2541
Epoch: 3348, Loss: 0.3023
Epoch: 3349, Loss: 0.2526
Epoch: 3350, Loss: 0.2554
Epoch: 3351, Loss: 0.2563
Epoch: 3352, Loss: 0.2611
Epoch: 3353, Loss: 0.2542
Epoch: 3354, Loss: 0.2561
Epoch: 3355, Loss: 0.2556
Epoch: 3356, Loss: 0.2517
Epoch: 3357, Loss: 0.2525
Epoch: 3358, Loss: 0.2529
Epoch: 3359, Loss: 0.2549
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36633.07it/s]
Epoch: 3360, Loss: 0.2503, Train: 0.9230, Val: 0.9212, test: 0.9193
Epoch: 3361, Loss: 0.2485
Epoch: 3362, Loss: 0.2553
Epoch: 3363, Loss: 0.2539
Epoch: 3364, Loss: 0.2587
Epoch: 3365, Loss: 0.2680
Epoch: 3366, Loss: 0.2515
Epoch: 3367, Loss: 0.2537
Epoch: 3368, Loss: 0.2524
Epoch: 3369, Loss: 0.2517
Epoch: 3370, Loss: 0.2531
Epoch: 3371, Loss: 0.2674
Epoch: 3372, Loss: 0.2515
Epoch: 3373, Loss: 0.2539
Epoch: 3374, Loss: 0.2565
Epoch: 3375, Loss: 0.2515
Epoch: 3376, Loss: 0.4098
Epoch: 3377, Loss: 0.3214
Epoch: 3378, Loss: 0.2560
Epoch: 3379, Loss: 0.2680
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36634.35it/s]
Epoch: 3380, Loss: 0.2939, Train: 0.9225, Val: 0.9209, test: 0.9190
Epoch: 3381, Loss: 0.2899
Epoch: 3382, Loss: 0.2823
Epoch: 3383, Loss: 0.2939
Epoch: 3384, Loss: 0.2689
Epoch: 3385, Loss: 0.2839
Epoch: 3386, Loss: 0.2642
Epoch: 3387, Loss: 0.2844
Epoch: 3388, Loss: 0.2667
Epoch: 3389, Loss: 0.2661
Epoch: 3390, Loss: 0.2628
Epoch: 3391, Loss: 0.2666
Epoch: 3392, Loss: 0.2688
Epoch: 3393, Loss: 0.2626
Epoch: 3394, Loss: 0.2630
Epoch: 3395, Loss: 0.2622
Epoch: 3396, Loss: 0.2581
Epoch: 3397, Loss: 0.2631
Epoch: 3398, Loss: 0.2580
Epoch: 3399, Loss: 0.2587
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29373.43it/s]
Epoch: 3400, Loss: 0.2556, Train: 0.9218, Val: 0.9199, test: 0.9203
Epoch: 3401, Loss: 0.2534
Epoch: 3402, Loss: 0.2554
Epoch: 3403, Loss: 0.2532
Epoch: 3404, Loss: 0.2557
Epoch: 3405, Loss: 0.2591
Epoch: 3406, Loss: 0.2519
Epoch: 3407, Loss: 0.2555
Epoch: 3408, Loss: 0.2533
Epoch: 3409, Loss: 0.2543
Epoch: 3410, Loss: 0.2538
Epoch: 3411, Loss: 0.2826
Epoch: 3412, Loss: 0.2535
Epoch: 3413, Loss: 0.2563
Epoch: 3414, Loss: 0.2570
Epoch: 3415, Loss: 0.2567
Epoch: 3416, Loss: 0.2569
Epoch: 3417, Loss: 0.2608
Epoch: 3418, Loss: 0.2618
Epoch: 3419, Loss: 0.2553
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37520.91it/s]
Epoch: 3420, Loss: 0.2521, Train: 0.9226, Val: 0.9205, test: 0.9187
Epoch: 3421, Loss: 0.2551
Epoch: 3422, Loss: 0.2579
Epoch: 3423, Loss: 0.2515
Epoch: 3424, Loss: 0.2507
Epoch: 3425, Loss: 0.2562
Epoch: 3426, Loss: 0.2546
Epoch: 3427, Loss: 0.2574
Epoch: 3428, Loss: 0.2723
Epoch: 3429, Loss: 0.2615
Epoch: 3430, Loss: 0.2509
Epoch: 3431, Loss: 0.2562
Epoch: 3432, Loss: 0.2552
Epoch: 3433, Loss: 0.2590
Epoch: 3434, Loss: 0.2589
Epoch: 3435, Loss: 0.2579
Epoch: 3436, Loss: 0.2519
Epoch: 3437, Loss: 0.2666
Epoch: 3438, Loss: 0.2837
Epoch: 3439, Loss: 0.2507
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36476.68it/s]
Epoch: 3440, Loss: 0.2517, Train: 0.9226, Val: 0.9199, test: 0.9193
Epoch: 3441, Loss: 0.2518
Epoch: 3442, Loss: 0.2515
Epoch: 3443, Loss: 0.2527
Epoch: 3444, Loss: 0.2518
Epoch: 3445, Loss: 0.2488
Epoch: 3446, Loss: 0.2481
Epoch: 3447, Loss: 0.2560
Epoch: 3448, Loss: 0.3220
Epoch: 3449, Loss: 0.4459
Epoch: 3450, Loss: 0.2759
Epoch: 3451, Loss: 0.3014
Epoch: 3452, Loss: 0.3395
Epoch: 3453, Loss: 0.3413
Epoch: 3454, Loss: 0.3449
Epoch: 3455, Loss: 0.3385
Epoch: 3456, Loss: 0.3542
Epoch: 3457, Loss: 0.3509
Epoch: 3458, Loss: 0.3351
Epoch: 3459, Loss: 0.4651
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37353.82it/s]
Epoch: 3460, Loss: 0.3505, Train: 0.9225, Val: 0.9187, test: 0.9218
Epoch: 3461, Loss: 0.3201
Epoch: 3462, Loss: 0.2966
Epoch: 3463, Loss: 0.3173
Epoch: 3464, Loss: 0.2952
Epoch: 3465, Loss: 0.2917
Epoch: 3466, Loss: 0.2777
Epoch: 3467, Loss: 0.2984
Epoch: 3468, Loss: 0.2770
Epoch: 3469, Loss: 0.2882
Epoch: 3470, Loss: 0.2902
Epoch: 3471, Loss: 0.3163
Epoch: 3472, Loss: 0.2824
Epoch: 3473, Loss: 0.2820
Epoch: 3474, Loss: 0.2868
Epoch: 3475, Loss: 0.2793
Epoch: 3476, Loss: 0.2836
Epoch: 3477, Loss: 0.2814
Epoch: 3478, Loss: 0.2775
Epoch: 3479, Loss: 0.2788
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37561.15it/s]
Epoch: 3480, Loss: 0.2767, Train: 0.9224, Val: 0.9202, test: 0.9200
Epoch: 3481, Loss: 0.2796
Epoch: 3482, Loss: 0.2819
Epoch: 3483, Loss: 0.3511
Epoch: 3484, Loss: 0.2672
Epoch: 3485, Loss: 0.2652
Epoch: 3486, Loss: 0.2820
Epoch: 3487, Loss: 0.2892
Epoch: 3488, Loss: 0.2782
Epoch: 3489, Loss: 0.2720
Epoch: 3490, Loss: 0.2846
Epoch: 3491, Loss: 0.2729
Epoch: 3492, Loss: 0.2868
Epoch: 3493, Loss: 0.2873
Epoch: 3494, Loss: 0.2631
Epoch: 3495, Loss: 0.2659
Epoch: 3496, Loss: 0.2722
Epoch: 3497, Loss: 0.2737
Epoch: 3498, Loss: 0.2644
Epoch: 3499, Loss: 0.2516
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37327.44it/s]
Epoch: 3500, Loss: 0.2578, Train: 0.9227, Val: 0.9187, test: 0.9190
Epoch: 3501, Loss: 0.2575
Epoch: 3502, Loss: 0.2703
Epoch: 3503, Loss: 0.2578
Epoch: 3504, Loss: 0.2551
Epoch: 3505, Loss: 0.2620
Epoch: 3506, Loss: 0.2616
Epoch: 3507, Loss: 0.2463
Epoch: 3508, Loss: 0.2538
Epoch: 3509, Loss: 0.2658
Epoch: 3510, Loss: 0.2638
Epoch: 3511, Loss: 0.2514
Epoch: 3512, Loss: 0.2634
Epoch: 3513, Loss: 0.2668
Epoch: 3514, Loss: 0.2520
Epoch: 3515, Loss: 0.2588
Epoch: 3516, Loss: 0.2511
Epoch: 3517, Loss: 0.2615
Epoch: 3518, Loss: 0.2535
Epoch: 3519, Loss: 0.2507
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36853.38it/s]
Epoch: 3520, Loss: 0.2519, Train: 0.9119, Val: 0.9056, test: 0.9118
Epoch: 3521, Loss: 0.3324
Epoch: 3522, Loss: 0.2624
Epoch: 3523, Loss: 0.2753
Epoch: 3524, Loss: 0.2779
Epoch: 3525, Loss: 0.2867
Epoch: 3526, Loss: 0.2791
Epoch: 3527, Loss: 0.3310
Epoch: 3528, Loss: 0.2797
Epoch: 3529, Loss: 0.3770
Epoch: 3530, Loss: 0.2777
Epoch: 3531, Loss: 0.2808
Epoch: 3532, Loss: 0.2861
Epoch: 3533, Loss: 0.3033
Epoch: 3534, Loss: 0.2832
Epoch: 3535, Loss: 0.2704
Epoch: 3536, Loss: 0.3033
Epoch: 3537, Loss: 0.2800
Epoch: 3538, Loss: 0.2852
Epoch: 3539, Loss: 0.2791
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36794.34it/s]
Epoch: 3540, Loss: 0.2660, Train: 0.9209, Val: 0.9159, test: 0.9175
Epoch: 3541, Loss: 0.2679
Epoch: 3542, Loss: 0.2655
Epoch: 3543, Loss: 0.2824
Epoch: 3544, Loss: 0.2744
Epoch: 3545, Loss: 0.2681
Epoch: 3546, Loss: 0.2679
Epoch: 3547, Loss: 0.2800
Epoch: 3548, Loss: 0.2669
Epoch: 3549, Loss: 0.2636
Epoch: 3550, Loss: 0.2677
Epoch: 3551, Loss: 0.2710
Epoch: 3552, Loss: 0.2607
Epoch: 3553, Loss: 0.2716
Epoch: 3554, Loss: 0.2624
Epoch: 3555, Loss: 0.2578
Epoch: 3556, Loss: 0.2665
Epoch: 3557, Loss: 0.2676
Epoch: 3558, Loss: 0.2608
Epoch: 3559, Loss: 0.2687
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36707.91it/s]
Epoch: 3560, Loss: 0.2639, Train: 0.9205, Val: 0.9181, test: 0.9165
Epoch: 3561, Loss: 0.2516
Epoch: 3562, Loss: 0.2591
Epoch: 3563, Loss: 0.2661
Epoch: 3564, Loss: 0.2545
Epoch: 3565, Loss: 0.2651
Epoch: 3566, Loss: 0.2648
Epoch: 3567, Loss: 0.2513
Epoch: 3568, Loss: 0.2602
Epoch: 3569, Loss: 0.2560
Epoch: 3570, Loss: 0.2636
Epoch: 3571, Loss: 0.2568
Epoch: 3572, Loss: 0.2565
Epoch: 3573, Loss: 0.2594
Epoch: 3574, Loss: 0.2611
Epoch: 3575, Loss: 0.2613
Epoch: 3576, Loss: 0.2595
Epoch: 3577, Loss: 0.2690
Epoch: 3578, Loss: 0.2601
Epoch: 3579, Loss: 0.2527
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37377.63it/s]
Epoch: 3580, Loss: 0.2513, Train: 0.9207, Val: 0.9187, test: 0.9184
Epoch: 3581, Loss: 0.2506
Epoch: 3582, Loss: 0.2606
Epoch: 3583, Loss: 0.2544
Epoch: 3584, Loss: 0.2530
Epoch: 3585, Loss: 0.2916
Epoch: 3586, Loss: 0.2679
Epoch: 3587, Loss: 0.2549
Epoch: 3588, Loss: 0.2708
Epoch: 3589, Loss: 0.2629
Epoch: 3590, Loss: 0.2619
Epoch: 3591, Loss: 0.2526
Epoch: 3592, Loss: 0.2562
Epoch: 3593, Loss: 0.2570
Epoch: 3594, Loss: 0.2551
Epoch: 3595, Loss: 0.2563
Epoch: 3596, Loss: 0.2528
Epoch: 3597, Loss: 0.2535
Epoch: 3598, Loss: 0.2568
Epoch: 3599, Loss: 0.2543
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36393.66it/s]
Epoch: 3600, Loss: 0.2561, Train: 0.9204, Val: 0.9174, test: 0.9162
Epoch: 3601, Loss: 0.2529
Epoch: 3602, Loss: 0.2596
Epoch: 3603, Loss: 0.2551
Epoch: 3604, Loss: 0.2600
Epoch: 3605, Loss: 0.2573
Epoch: 3606, Loss: 0.2649
Epoch: 3607, Loss: 0.2575
Epoch: 3608, Loss: 0.2555
Epoch: 3609, Loss: 0.2527
Epoch: 3610, Loss: 0.2513
Epoch: 3611, Loss: 0.2600
Epoch: 3612, Loss: 0.2522
Epoch: 3613, Loss: 0.2533
Epoch: 3614, Loss: 0.2567
Epoch: 3615, Loss: 0.2473
Epoch: 3616, Loss: 0.2535
Epoch: 3617, Loss: 0.2537
Epoch: 3618, Loss: 0.2587
Epoch: 3619, Loss: 0.2697
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36806.72it/s]
Epoch: 3620, Loss: 0.3693, Train: 0.9198, Val: 0.9187, test: 0.9165
Epoch: 3621, Loss: 0.2699
Epoch: 3622, Loss: 0.2843
Epoch: 3623, Loss: 0.2775
Epoch: 3624, Loss: 0.2995
Epoch: 3625, Loss: 0.3026
Epoch: 3626, Loss: 0.3342
Epoch: 3627, Loss: 0.3203
Epoch: 3628, Loss: 0.3314
Epoch: 3629, Loss: 0.3062
Epoch: 3630, Loss: 0.3337
Epoch: 3631, Loss: 0.3252
Epoch: 3632, Loss: 0.3010
Epoch: 3633, Loss: 0.2911
Epoch: 3634, Loss: 0.2982
Epoch: 3635, Loss: 0.2788
Epoch: 3636, Loss: 0.2779
Epoch: 3637, Loss: 0.2986
Epoch: 3638, Loss: 0.2782
Epoch: 3639, Loss: 0.2847
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35294.54it/s]
Epoch: 3640, Loss: 0.3413, Train: 0.9196, Val: 0.9174, test: 0.9159
Epoch: 3641, Loss: 0.2827
Epoch: 3642, Loss: 0.3079
Epoch: 3643, Loss: 0.2873
Epoch: 3644, Loss: 0.2749
Epoch: 3645, Loss: 0.2773
Epoch: 3646, Loss: 0.3090
Epoch: 3647, Loss: 0.2828
Epoch: 3648, Loss: 0.2960
Epoch: 3649, Loss: 0.2978
Epoch: 3650, Loss: 0.2802
Epoch: 3651, Loss: 0.2936
Epoch: 3652, Loss: 0.2791
Epoch: 3653, Loss: 0.2799
Epoch: 3654, Loss: 0.2786
Epoch: 3655, Loss: 0.2623
Epoch: 3656, Loss: 0.2672
Epoch: 3657, Loss: 0.2656
Epoch: 3658, Loss: 0.2690
Epoch: 3659, Loss: 0.2715
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35029.92it/s]
Epoch: 3660, Loss: 0.2724, Train: 0.9201, Val: 0.9168, test: 0.9178
Epoch: 3661, Loss: 0.2772
Epoch: 3662, Loss: 0.2677
Epoch: 3663, Loss: 0.2745
Epoch: 3664, Loss: 0.2750
Epoch: 3665, Loss: 0.2623
Epoch: 3666, Loss: 0.2688
Epoch: 3667, Loss: 0.2739
Epoch: 3668, Loss: 0.2605
Epoch: 3669, Loss: 0.2649
Epoch: 3670, Loss: 0.2661
Epoch: 3671, Loss: 0.2714
Epoch: 3672, Loss: 0.2574
Epoch: 3673, Loss: 0.2665
Epoch: 3674, Loss: 0.2629
Epoch: 3675, Loss: 0.2665
Epoch: 3676, Loss: 0.2590
Epoch: 3677, Loss: 0.2596
Epoch: 3678, Loss: 0.2681
Epoch: 3679, Loss: 0.2576
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34727.26it/s]
Epoch: 3680, Loss: 0.2559, Train: 0.9213, Val: 0.9174, test: 0.9184
Epoch: 3681, Loss: 0.4590
Epoch: 3682, Loss: 0.2720
Epoch: 3683, Loss: 0.2605
Epoch: 3684, Loss: 0.2569
Epoch: 3685, Loss: 0.2637
Epoch: 3686, Loss: 0.2689
Epoch: 3687, Loss: 0.2631
Epoch: 3688, Loss: 0.2637
Epoch: 3689, Loss: 0.2606
Epoch: 3690, Loss: 0.2679
Epoch: 3691, Loss: 0.2826
Epoch: 3692, Loss: 0.2638
Epoch: 3693, Loss: 0.2663
Epoch: 3694, Loss: 0.2651
Epoch: 3695, Loss: 0.2662
Epoch: 3696, Loss: 0.2633
Epoch: 3697, Loss: 0.2724
Epoch: 3698, Loss: 0.2635
Epoch: 3699, Loss: 0.2689
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35195.17it/s]
Epoch: 3700, Loss: 0.2631, Train: 0.9199, Val: 0.9159, test: 0.9168
Epoch: 3701, Loss: 0.2607
Epoch: 3702, Loss: 0.2645
Epoch: 3703, Loss: 0.2553
Epoch: 3704, Loss: 0.2559
Epoch: 3705, Loss: 0.2527
Epoch: 3706, Loss: 0.2591
Epoch: 3707, Loss: 0.2551
Epoch: 3708, Loss: 0.2526
Epoch: 3709, Loss: 0.2657
Epoch: 3710, Loss: 0.2609
Epoch: 3711, Loss: 0.2594
Epoch: 3712, Loss: 0.2540
Epoch: 3713, Loss: 0.2501
Epoch: 3714, Loss: 0.2550
Epoch: 3715, Loss: 0.2517
Epoch: 3716, Loss: 0.2557
Epoch: 3717, Loss: 0.2598
Epoch: 3718, Loss: 0.2625
Epoch: 3719, Loss: 0.2544
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37273.26it/s]
Epoch: 3720, Loss: 0.2520, Train: 0.9213, Val: 0.9190, test: 0.9175
Epoch: 3721, Loss: 0.2506
Epoch: 3722, Loss: 0.2501
Epoch: 3723, Loss: 0.2529
Epoch: 3724, Loss: 0.2486
Epoch: 3725, Loss: 0.2656
Epoch: 3726, Loss: 0.2532
Epoch: 3727, Loss: 0.2540
Epoch: 3728, Loss: 0.2548
Epoch: 3729, Loss: 0.2532
Epoch: 3730, Loss: 0.2535
Epoch: 3731, Loss: 0.2519
Epoch: 3732, Loss: 0.2492
Epoch: 3733, Loss: 0.2494
Epoch: 3734, Loss: 0.2553
Epoch: 3735, Loss: 0.2479
Epoch: 3736, Loss: 0.2483
Epoch: 3737, Loss: 0.2503
Epoch: 3738, Loss: 0.2507
Epoch: 3739, Loss: 0.2470
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37497.54it/s]
Epoch: 3740, Loss: 0.2502, Train: 0.9209, Val: 0.9187, test: 0.9171
Epoch: 3741, Loss: 0.2511
Epoch: 3742, Loss: 0.2511
Epoch: 3743, Loss: 0.2468
Epoch: 3744, Loss: 0.2656
Epoch: 3745, Loss: 0.2558
Epoch: 3746, Loss: 0.2594
Epoch: 3747, Loss: 0.2522
Epoch: 3748, Loss: 0.2607
Epoch: 3749, Loss: 0.2530
Epoch: 3750, Loss: 0.2542
Epoch: 3751, Loss: 0.2538
Epoch: 3752, Loss: 0.2557
Epoch: 3753, Loss: 0.2569
Epoch: 3754, Loss: 0.2582
Epoch: 3755, Loss: 0.2555
Epoch: 3756, Loss: 0.2548
Epoch: 3757, Loss: 0.2609
Epoch: 3758, Loss: 0.2508
Epoch: 3759, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37338.59it/s]
Epoch: 3760, Loss: 0.2493, Train: 0.9211, Val: 0.9199, test: 0.9178
Epoch: 3761, Loss: 0.2533
Epoch: 3762, Loss: 0.2525
Epoch: 3763, Loss: 0.2542
Epoch: 3764, Loss: 0.2573
Epoch: 3765, Loss: 0.2512
Epoch: 3766, Loss: 0.2489
Epoch: 3767, Loss: 0.2523
Epoch: 3768, Loss: 0.2919
Epoch: 3769, Loss: 0.2519
Epoch: 3770, Loss: 0.2594
Epoch: 3771, Loss: 0.2515
Epoch: 3772, Loss: 0.2602
Epoch: 3773, Loss: 0.2508
Epoch: 3774, Loss: 0.2506
Epoch: 3775, Loss: 0.2487
Epoch: 3776, Loss: 0.2479
Epoch: 3777, Loss: 0.2519
Epoch: 3778, Loss: 0.2513
Epoch: 3779, Loss: 0.2507
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36800.51it/s]
Epoch: 3780, Loss: 0.2543, Train: 0.9205, Val: 0.9174, test: 0.9175
Epoch: 3781, Loss: 0.2532
Epoch: 3782, Loss: 0.2549
Epoch: 3783, Loss: 0.2508
Epoch: 3784, Loss: 0.2632
Epoch: 3785, Loss: 0.2504
Epoch: 3786, Loss: 0.2506
Epoch: 3787, Loss: 0.2749
Epoch: 3788, Loss: 0.2544
Epoch: 3789, Loss: 0.2570
Epoch: 3790, Loss: 0.2580
Epoch: 3791, Loss: 0.2964
Epoch: 3792, Loss: 0.2849
Epoch: 3793, Loss: 0.2581
Epoch: 3794, Loss: 0.2678
Epoch: 3795, Loss: 0.2670
Epoch: 3796, Loss: 0.2849
Epoch: 3797, Loss: 0.2691
Epoch: 3798, Loss: 0.2752
Epoch: 3799, Loss: 0.2682
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36813.71it/s]
Epoch: 3800, Loss: 0.2758, Train: 0.9210, Val: 0.9193, test: 0.9187
Epoch: 3801, Loss: 0.2536
Epoch: 3802, Loss: 0.2767
Epoch: 3803, Loss: 0.2581
Epoch: 3804, Loss: 0.2626
Epoch: 3805, Loss: 0.2556
Epoch: 3806, Loss: 0.2591
Epoch: 3807, Loss: 0.2594
Epoch: 3808, Loss: 0.2581
Epoch: 3809, Loss: 0.2628
Epoch: 3810, Loss: 0.2805
Epoch: 3811, Loss: 0.2697
Epoch: 3812, Loss: 0.2678
Epoch: 3813, Loss: 0.2689
Epoch: 3814, Loss: 0.2596
Epoch: 3815, Loss: 0.2637
Epoch: 3816, Loss: 0.2608
Epoch: 3817, Loss: 0.2671
Epoch: 3818, Loss: 0.2636
Epoch: 3819, Loss: 0.2549
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37126.44it/s]
Epoch: 3820, Loss: 0.2575, Train: 0.9206, Val: 0.9193, test: 0.9184
Epoch: 3821, Loss: 0.2613
Epoch: 3822, Loss: 0.2661
Epoch: 3823, Loss: 0.2570
Epoch: 3824, Loss: 0.2627
Epoch: 3825, Loss: 0.2696
Epoch: 3826, Loss: 0.2647
Epoch: 3827, Loss: 0.2606
Epoch: 3828, Loss: 0.2564
Epoch: 3829, Loss: 0.2601
Epoch: 3830, Loss: 0.2516
Epoch: 3831, Loss: 0.2593
Epoch: 3832, Loss: 0.2569
Epoch: 3833, Loss: 0.2552
Epoch: 3834, Loss: 0.2687
Epoch: 3835, Loss: 0.2681
Epoch: 3836, Loss: 0.2684
Epoch: 3837, Loss: 0.2546
Epoch: 3838, Loss: 0.2655
Epoch: 3839, Loss: 0.2636
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36790.29it/s]
Epoch: 3840, Loss: 0.2571, Train: 0.9206, Val: 0.9178, test: 0.9175
Epoch: 3841, Loss: 0.2724
Epoch: 3842, Loss: 0.2627
Epoch: 3843, Loss: 0.2594
Epoch: 3844, Loss: 0.2645
Epoch: 3845, Loss: 0.2572
Epoch: 3846, Loss: 0.2549
Epoch: 3847, Loss: 0.2536
Epoch: 3848, Loss: 0.2580
Epoch: 3849, Loss: 0.2642
Epoch: 3850, Loss: 0.2589
Epoch: 3851, Loss: 0.2728
Epoch: 3852, Loss: 0.2649
Epoch: 3853, Loss: 0.2760
Epoch: 3854, Loss: 0.2620
Epoch: 3855, Loss: 0.2720
Epoch: 3856, Loss: 0.2790
Epoch: 3857, Loss: 0.2660
Epoch: 3858, Loss: 0.2639
Epoch: 3859, Loss: 0.2605
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36900.16it/s]
Epoch: 3860, Loss: 0.2556, Train: 0.9213, Val: 0.9199, test: 0.9178
Epoch: 3861, Loss: 0.2513
Epoch: 3862, Loss: 0.2542
Epoch: 3863, Loss: 0.2632
Epoch: 3864, Loss: 0.2943
Epoch: 3865, Loss: 0.2663
Epoch: 3866, Loss: 0.2501
Epoch: 3867, Loss: 0.2557
Epoch: 3868, Loss: 0.2603
Epoch: 3869, Loss: 0.2827
Epoch: 3870, Loss: 0.2652
Epoch: 3871, Loss: 0.2618
Epoch: 3872, Loss: 0.2666
Epoch: 3873, Loss: 0.2661
Epoch: 3874, Loss: 0.2574
Epoch: 3875, Loss: 0.2556
Epoch: 3876, Loss: 0.2520
Epoch: 3877, Loss: 0.2577
Epoch: 3878, Loss: 0.2485
Epoch: 3879, Loss: 0.2544
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36924.37it/s]
Epoch: 3880, Loss: 0.2629, Train: 0.9204, Val: 0.9199, test: 0.9181
Epoch: 3881, Loss: 0.2562
Epoch: 3882, Loss: 0.2536
Epoch: 3883, Loss: 0.2518
Epoch: 3884, Loss: 0.2503
Epoch: 3885, Loss: 0.2549
Epoch: 3886, Loss: 0.2548
Epoch: 3887, Loss: 0.2506
Epoch: 3888, Loss: 0.2548
Epoch: 3889, Loss: 0.2504
Epoch: 3890, Loss: 0.2576
Epoch: 3891, Loss: 0.2575
Epoch: 3892, Loss: 0.2638
Epoch: 3893, Loss: 0.2512
Epoch: 3894, Loss: 0.2533
Epoch: 3895, Loss: 0.2565
Epoch: 3896, Loss: 0.2558
Epoch: 3897, Loss: 0.2497
Epoch: 3898, Loss: 0.2538
Epoch: 3899, Loss: 0.2577
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37811.37it/s]
Epoch: 3900, Loss: 0.2498, Train: 0.9215, Val: 0.9178, test: 0.9181
Epoch: 3901, Loss: 0.2538
Epoch: 3902, Loss: 0.2726
Epoch: 3903, Loss: 0.2537
Epoch: 3904, Loss: 0.2538
Epoch: 3905, Loss: 0.2562
Epoch: 3906, Loss: 0.2615
Epoch: 3907, Loss: 0.2543
Epoch: 3908, Loss: 0.2542
Epoch: 3909, Loss: 0.2548
Epoch: 3910, Loss: 0.2578
Epoch: 3911, Loss: 0.2569
Epoch: 3912, Loss: 0.2631
Epoch: 3913, Loss: 0.2543
Epoch: 3914, Loss: 0.2513
Epoch: 3915, Loss: 0.2653
Epoch: 3916, Loss: 0.2547
Epoch: 3917, Loss: 0.2491
Epoch: 3918, Loss: 0.2549
Epoch: 3919, Loss: 0.2531
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35956.68it/s]
Epoch: 3920, Loss: 0.2603, Train: 0.9206, Val: 0.9199, test: 0.9190
Epoch: 3921, Loss: 0.2542
Epoch: 3922, Loss: 0.2504
Epoch: 3923, Loss: 0.2577
Epoch: 3924, Loss: 0.2556
Epoch: 3925, Loss: 0.2485
Epoch: 3926, Loss: 0.2578
Epoch: 3927, Loss: 0.2541
Epoch: 3928, Loss: 0.2516
Epoch: 3929, Loss: 0.2472
Epoch: 3930, Loss: 0.2499
Epoch: 3931, Loss: 0.2515
Epoch: 3932, Loss: 0.2543
Epoch: 3933, Loss: 0.2548
Epoch: 3934, Loss: 0.2522
Epoch: 3935, Loss: 0.2477
Epoch: 3936, Loss: 0.2556
Epoch: 3937, Loss: 0.2552
Epoch: 3938, Loss: 0.2553
Epoch: 3939, Loss: 0.2492
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36605.58it/s]
Epoch: 3940, Loss: 0.2513, Train: 0.9213, Val: 0.9193, test: 0.9190
Epoch: 3941, Loss: 0.2549
Epoch: 3942, Loss: 0.2528
Epoch: 3943, Loss: 0.2597
Epoch: 3944, Loss: 0.2483
Epoch: 3945, Loss: 0.2563
Epoch: 3946, Loss: 0.2558
Epoch: 3947, Loss: 0.2571
Epoch: 3948, Loss: 0.2531
Epoch: 3949, Loss: 0.2541
Epoch: 3950, Loss: 0.2519
Epoch: 3951, Loss: 0.2501
Epoch: 3952, Loss: 0.2516
Epoch: 3953, Loss: 0.2476
Epoch: 3954, Loss: 0.2661
Epoch: 3955, Loss: 0.2471
Epoch: 3956, Loss: 0.2498
Epoch: 3957, Loss: 0.2729
Epoch: 3958, Loss: 0.2482
Epoch: 3959, Loss: 0.2496
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37938.26it/s]
Epoch: 3960, Loss: 0.2573, Train: 0.9214, Val: 0.9199, test: 0.9181
Epoch: 3961, Loss: 0.2549
Epoch: 3962, Loss: 0.2463
Epoch: 3963, Loss: 0.2503
Epoch: 3964, Loss: 0.2451
Epoch: 3965, Loss: 0.2483
Epoch: 3966, Loss: 0.2561
Epoch: 3967, Loss: 0.2477
Epoch: 3968, Loss: 0.2486
Epoch: 3969, Loss: 0.2480
Epoch: 3970, Loss: 0.2471
Epoch: 3971, Loss: 0.2468
Epoch: 3972, Loss: 0.2464
Epoch: 3973, Loss: 0.2473
Epoch: 3974, Loss: 0.2478
Epoch: 3975, Loss: 0.2438
Epoch: 3976, Loss: 0.2486
Epoch: 3977, Loss: 0.2521
Epoch: 3978, Loss: 0.2472
Epoch: 3979, Loss: 0.2612
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30368.51it/s]
Epoch: 3980, Loss: 0.2437, Train: 0.9208, Val: 0.9181, test: 0.9184
Epoch: 3981, Loss: 0.2503
Epoch: 3982, Loss: 0.2526
Epoch: 3983, Loss: 0.2436
Epoch: 3984, Loss: 0.2466
Epoch: 3985, Loss: 0.2507
Epoch: 3986, Loss: 0.2544
Epoch: 3987, Loss: 0.2496
Epoch: 3988, Loss: 0.2530
Epoch: 3989, Loss: 0.2552
Epoch: 3990, Loss: 0.2562
Epoch: 3991, Loss: 0.2475
Epoch: 3992, Loss: 0.2499
Epoch: 3993, Loss: 0.2497
Epoch: 3994, Loss: 0.2450
Epoch: 3995, Loss: 0.2510
Epoch: 3996, Loss: 0.2458
Epoch: 3997, Loss: 0.2499
Epoch: 3998, Loss: 0.2457
Epoch: 3999, Loss: 0.2523
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29675.84it/s]
Epoch: 4000, Loss: 0.2518, Train: 0.9213, Val: 0.9184, test: 0.9175
Epoch: 4001, Loss: 0.2484
Epoch: 4002, Loss: 0.2475
Epoch: 4003, Loss: 0.2485
Epoch: 4004, Loss: 0.2487
Epoch: 4005, Loss: 0.2487
Epoch: 4006, Loss: 0.2471
Epoch: 4007, Loss: 0.2441
Epoch: 4008, Loss: 0.2574
Epoch: 4009, Loss: 0.2437
Epoch: 4010, Loss: 0.2506
Epoch: 4011, Loss: 0.2495
Epoch: 4012, Loss: 0.2475
Epoch: 4013, Loss: 0.2469
Epoch: 4014, Loss: 0.2509
Epoch: 4015, Loss: 0.2481
Epoch: 4016, Loss: 0.2492
Epoch: 4017, Loss: 0.2524
Epoch: 4018, Loss: 0.2447
Epoch: 4019, Loss: 0.2544
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29607.26it/s]
Epoch: 4020, Loss: 0.2466, Train: 0.9216, Val: 0.9199, test: 0.9187
Epoch: 4021, Loss: 0.2503
Epoch: 4022, Loss: 0.2476
Epoch: 4023, Loss: 0.2503
Epoch: 4024, Loss: 0.2633
Epoch: 4025, Loss: 0.2495
Epoch: 4026, Loss: 0.2505
Epoch: 4027, Loss: 0.2518
Epoch: 4028, Loss: 0.2548
Epoch: 4029, Loss: 0.2457
Epoch: 4030, Loss: 0.2571
Epoch: 4031, Loss: 0.2514
Epoch: 4032, Loss: 0.2559
Epoch: 4033, Loss: 0.2488
Epoch: 4034, Loss: 0.2559
Epoch: 4035, Loss: 0.2531
Epoch: 4036, Loss: 0.2560
Epoch: 4037, Loss: 0.2528
Epoch: 4038, Loss: 0.2618
Epoch: 4039, Loss: 0.2523
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29608.16it/s]
Epoch: 4040, Loss: 0.2567, Train: 0.9211, Val: 0.9209, test: 0.9187
Epoch: 4041, Loss: 0.2595
Epoch: 4042, Loss: 0.2520
Epoch: 4043, Loss: 0.2525
Epoch: 4044, Loss: 0.2540
Epoch: 4045, Loss: 0.2525
Epoch: 4046, Loss: 0.2568
Epoch: 4047, Loss: 0.2487
Epoch: 4048, Loss: 0.2516
Epoch: 4049, Loss: 0.2527
Epoch: 4050, Loss: 0.2477
Epoch: 4051, Loss: 0.2508
Epoch: 4052, Loss: 0.2529
Epoch: 4053, Loss: 0.2532
Epoch: 4054, Loss: 0.2473
Epoch: 4055, Loss: 0.2512
Epoch: 4056, Loss: 0.2460
Epoch: 4057, Loss: 0.2542
Epoch: 4058, Loss: 0.2492
Epoch: 4059, Loss: 0.2469
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29465.19it/s]
Epoch: 4060, Loss: 0.2499, Train: 0.9206, Val: 0.9181, test: 0.9184
Epoch: 4061, Loss: 0.2501
Epoch: 4062, Loss: 0.2456
Epoch: 4063, Loss: 0.2478
Epoch: 4064, Loss: 0.2530
Epoch: 4065, Loss: 0.2566
Epoch: 4066, Loss: 0.2499
Epoch: 4067, Loss: 0.2439
Epoch: 4068, Loss: 0.2458
Epoch: 4069, Loss: 0.2472
Epoch: 4070, Loss: 0.2434
Epoch: 4071, Loss: 0.2459
Epoch: 4072, Loss: 0.2445
Epoch: 4073, Loss: 0.2472
Epoch: 4074, Loss: 0.2463
Epoch: 4075, Loss: 0.2457
Epoch: 4076, Loss: 0.2477
Epoch: 4077, Loss: 0.2460
Epoch: 4078, Loss: 0.2505
Epoch: 4079, Loss: 0.2473
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30164.75it/s]
Epoch: 4080, Loss: 0.2456, Train: 0.9217, Val: 0.9196, test: 0.9187
Epoch: 4081, Loss: 0.2471
Epoch: 4082, Loss: 0.2443
Epoch: 4083, Loss: 0.2545
Epoch: 4084, Loss: 0.2473
Epoch: 4085, Loss: 0.2464
Epoch: 4086, Loss: 0.2594
Epoch: 4087, Loss: 0.2467
Epoch: 4088, Loss: 0.2453
Epoch: 4089, Loss: 0.2464
Epoch: 4090, Loss: 0.2473
Epoch: 4091, Loss: 0.2480
Epoch: 4092, Loss: 0.2480
Epoch: 4093, Loss: 0.2461
Epoch: 4094, Loss: 0.2469
Epoch: 4095, Loss: 0.2473
Epoch: 4096, Loss: 0.2494
Epoch: 4097, Loss: 0.2499
Epoch: 4098, Loss: 0.2533
Epoch: 4099, Loss: 0.2506
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30385.19it/s]
Epoch: 4100, Loss: 0.2519, Train: 0.9212, Val: 0.9174, test: 0.9187
Epoch: 4101, Loss: 0.2576
Epoch: 4102, Loss: 0.2627
Epoch: 4103, Loss: 0.2610
Epoch: 4104, Loss: 0.2603
Epoch: 4105, Loss: 0.2606
Epoch: 4106, Loss: 0.2712
Epoch: 4107, Loss: 0.2760
Epoch: 4108, Loss: 0.2550
Epoch: 4109, Loss: 0.2734
Epoch: 4110, Loss: 0.2587
Epoch: 4111, Loss: 0.2621
Epoch: 4112, Loss: 0.2672
Epoch: 4113, Loss: 0.2622
Epoch: 4114, Loss: 0.2670
Epoch: 4115, Loss: 0.2621
Epoch: 4116, Loss: 0.2598
Epoch: 4117, Loss: 0.2639
Epoch: 4118, Loss: 0.2665
Epoch: 4119, Loss: 0.2546
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30154.05it/s]
Epoch: 4120, Loss: 0.2584, Train: 0.9212, Val: 0.9205, test: 0.9200
Epoch: 4121, Loss: 0.2571
Epoch: 4122, Loss: 0.2555
Epoch: 4123, Loss: 0.2559
Epoch: 4124, Loss: 0.2586
Epoch: 4125, Loss: 0.2571
Epoch: 4126, Loss: 0.2573
Epoch: 4127, Loss: 0.2527
Epoch: 4128, Loss: 0.2531
Epoch: 4129, Loss: 0.2596
Epoch: 4130, Loss: 0.2494
Epoch: 4131, Loss: 0.2552
Epoch: 4132, Loss: 0.2544
Epoch: 4133, Loss: 0.2558
Epoch: 4134, Loss: 0.2564
Epoch: 4135, Loss: 0.2516
Epoch: 4136, Loss: 0.2583
Epoch: 4137, Loss: 0.2556
Epoch: 4138, Loss: 0.2533
Epoch: 4139, Loss: 0.2563
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35267.18it/s]
Epoch: 4140, Loss: 0.2523, Train: 0.9209, Val: 0.9168, test: 0.9171
Epoch: 4141, Loss: 0.2517
Epoch: 4142, Loss: 0.2609
Epoch: 4143, Loss: 0.2568
Epoch: 4144, Loss: 0.2568
Epoch: 4145, Loss: 0.2500
Epoch: 4146, Loss: 0.2517
Epoch: 4147, Loss: 0.2543
Epoch: 4148, Loss: 0.2504
Epoch: 4149, Loss: 0.2533
Epoch: 4150, Loss: 0.2534
Epoch: 4151, Loss: 0.2532
Epoch: 4152, Loss: 0.2550
Epoch: 4153, Loss: 0.2522
Epoch: 4154, Loss: 0.2535
Epoch: 4155, Loss: 0.2506
Epoch: 4156, Loss: 0.2528
Epoch: 4157, Loss: 0.2518
Epoch: 4158, Loss: 0.2510
Epoch: 4159, Loss: 0.2518
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35540.75it/s]
Epoch: 4160, Loss: 0.2768, Train: 0.9190, Val: 0.9162, test: 0.9153
Epoch: 4161, Loss: 0.2749
Epoch: 4162, Loss: 0.2530
Epoch: 4163, Loss: 0.2657
Epoch: 4164, Loss: 0.2700
Epoch: 4165, Loss: 0.2671
Epoch: 4166, Loss: 0.2830
Epoch: 4167, Loss: 0.2766
Epoch: 4168, Loss: 0.3428
Epoch: 4169, Loss: 0.2708
Epoch: 4170, Loss: 0.2816
Epoch: 4171, Loss: 0.2722
Epoch: 4172, Loss: 0.2688
Epoch: 4173, Loss: 0.2703
Epoch: 4174, Loss: 0.2690
Epoch: 4175, Loss: 0.2651
Epoch: 4176, Loss: 0.2598
Epoch: 4177, Loss: 0.2715
Epoch: 4178, Loss: 0.2673
Epoch: 4179, Loss: 0.2624
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36164.94it/s]
Epoch: 4180, Loss: 0.2687, Train: 0.9200, Val: 0.9168, test: 0.9200
Epoch: 4181, Loss: 0.2630
Epoch: 4182, Loss: 0.2649
Epoch: 4183, Loss: 0.2650
Epoch: 4184, Loss: 0.2609
Epoch: 4185, Loss: 0.2563
Epoch: 4186, Loss: 0.2617
Epoch: 4187, Loss: 0.2612
Epoch: 4188, Loss: 0.2559
Epoch: 4189, Loss: 0.2634
Epoch: 4190, Loss: 0.2595
Epoch: 4191, Loss: 0.3790
Epoch: 4192, Loss: 0.2844
Epoch: 4193, Loss: 0.2836
Epoch: 4194, Loss: 0.2889
Epoch: 4195, Loss: 0.3079
Epoch: 4196, Loss: 0.3074
Epoch: 4197, Loss: 0.2877
Epoch: 4198, Loss: 0.3230
Epoch: 4199, Loss: 0.2799
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36570.93it/s]
Epoch: 4200, Loss: 0.2808, Train: 0.9213, Val: 0.9178, test: 0.9187
Epoch: 4201, Loss: 0.2798
Epoch: 4202, Loss: 0.2838
Epoch: 4203, Loss: 0.2848
Epoch: 4204, Loss: 0.2954
Epoch: 4205, Loss: 0.2683
Epoch: 4206, Loss: 0.2677
Epoch: 4207, Loss: 0.2783
Epoch: 4208, Loss: 0.2634
Epoch: 4209, Loss: 0.2706
Epoch: 4210, Loss: 0.2691
Epoch: 4211, Loss: 0.2692
Epoch: 4212, Loss: 0.2761
Epoch: 4213, Loss: 0.2647
Epoch: 4214, Loss: 0.2611
Epoch: 4215, Loss: 0.2644
Epoch: 4216, Loss: 0.2597
Epoch: 4217, Loss: 0.2825
Epoch: 4218, Loss: 0.2714
Epoch: 4219, Loss: 0.2660
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36872.08it/s]
Epoch: 4220, Loss: 0.2731, Train: 0.9209, Val: 0.9174, test: 0.9190
Epoch: 4221, Loss: 0.2613
Epoch: 4222, Loss: 0.2576
Epoch: 4223, Loss: 0.2646
Epoch: 4224, Loss: 0.2587
Epoch: 4225, Loss: 0.2572
Epoch: 4226, Loss: 0.2560
Epoch: 4227, Loss: 0.2540
Epoch: 4228, Loss: 0.2557
Epoch: 4229, Loss: 0.2554
Epoch: 4230, Loss: 0.2572
Epoch: 4231, Loss: 0.2526
Epoch: 4232, Loss: 0.2571
Epoch: 4233, Loss: 0.2530
Epoch: 4234, Loss: 0.2548
Epoch: 4235, Loss: 0.2629
Epoch: 4236, Loss: 0.2642
Epoch: 4237, Loss: 0.2585
Epoch: 4238, Loss: 0.2547
Epoch: 4239, Loss: 0.2554
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36270.36it/s]
Epoch: 4240, Loss: 0.2552, Train: 0.9220, Val: 0.9181, test: 0.9190
Epoch: 4241, Loss: 0.2646
Epoch: 4242, Loss: 0.2589
Epoch: 4243, Loss: 0.2826
Epoch: 4244, Loss: 0.2784
Epoch: 4245, Loss: 0.2599
Epoch: 4246, Loss: 0.2597
Epoch: 4247, Loss: 0.2601
Epoch: 4248, Loss: 0.2620
Epoch: 4249, Loss: 0.2600
Epoch: 4250, Loss: 0.2607
Epoch: 4251, Loss: 0.2658
Epoch: 4252, Loss: 0.2639
Epoch: 4253, Loss: 0.2585
Epoch: 4254, Loss: 0.2582
Epoch: 4255, Loss: 0.2550
Epoch: 4256, Loss: 0.2754
Epoch: 4257, Loss: 0.2555
Epoch: 4258, Loss: 0.2508
Epoch: 4259, Loss: 0.2551
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35541.81it/s]
Epoch: 4260, Loss: 0.2586, Train: 0.9207, Val: 0.9205, test: 0.9184
Epoch: 4261, Loss: 0.2851
Epoch: 4262, Loss: 0.2552
Epoch: 4263, Loss: 0.2816
Epoch: 4264, Loss: 0.2574
Epoch: 4265, Loss: 0.2508
Epoch: 4266, Loss: 0.2507
Epoch: 4267, Loss: 0.2523
Epoch: 4268, Loss: 0.2548
Epoch: 4269, Loss: 0.2615
Epoch: 4270, Loss: 0.2557
Epoch: 4271, Loss: 0.2516
Epoch: 4272, Loss: 0.2537
Epoch: 4273, Loss: 0.2512
Epoch: 4274, Loss: 0.2526
Epoch: 4275, Loss: 0.2540
Epoch: 4276, Loss: 0.2559
Epoch: 4277, Loss: 0.2529
Epoch: 4278, Loss: 0.2528
Epoch: 4279, Loss: 0.2538
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35913.34it/s]
Epoch: 4280, Loss: 0.2721, Train: 0.9213, Val: 0.9190, test: 0.9190
Epoch: 4281, Loss: 0.2534
Epoch: 4282, Loss: 0.2542
Epoch: 4283, Loss: 0.2546
Epoch: 4284, Loss: 0.2523
Epoch: 4285, Loss: 0.2596
Epoch: 4286, Loss: 0.2510
Epoch: 4287, Loss: 0.2539
Epoch: 4288, Loss: 0.2508
Epoch: 4289, Loss: 0.2511
Epoch: 4290, Loss: 0.2477
Epoch: 4291, Loss: 0.2546
Epoch: 4292, Loss: 0.2492
Epoch: 4293, Loss: 0.2528
Epoch: 4294, Loss: 0.2534
Epoch: 4295, Loss: 0.2512
Epoch: 4296, Loss: 0.2524
Epoch: 4297, Loss: 0.2540
Epoch: 4298, Loss: 0.2491
Epoch: 4299, Loss: 0.2529
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37346.79it/s]
Epoch: 4300, Loss: 0.2510, Train: 0.9218, Val: 0.9193, test: 0.9196
Epoch: 4301, Loss: 0.2528
Epoch: 4302, Loss: 0.2544
Epoch: 4303, Loss: 0.2515
Epoch: 4304, Loss: 0.2597
Epoch: 4305, Loss: 0.2503
Epoch: 4306, Loss: 0.2488
Epoch: 4307, Loss: 0.2523
Epoch: 4308, Loss: 0.2657
Epoch: 4309, Loss: 0.2517
Epoch: 4310, Loss: 0.2547
Epoch: 4311, Loss: 0.2606
Epoch: 4312, Loss: 0.2506
Epoch: 4313, Loss: 0.2522
Epoch: 4314, Loss: 0.2545
Epoch: 4315, Loss: 0.2548
Epoch: 4316, Loss: 0.2492
Epoch: 4317, Loss: 0.2497
Epoch: 4318, Loss: 0.2544
Epoch: 4319, Loss: 0.2524
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37072.18it/s]
Epoch: 4320, Loss: 0.2498, Train: 0.9207, Val: 0.9171, test: 0.9190
Epoch: 4321, Loss: 0.2529
Epoch: 4322, Loss: 0.2495
Epoch: 4323, Loss: 0.2525
Epoch: 4324, Loss: 0.2526
Epoch: 4325, Loss: 0.2529
Epoch: 4326, Loss: 0.2517
Epoch: 4327, Loss: 0.2507
Epoch: 4328, Loss: 0.2500
Epoch: 4329, Loss: 0.2566
Epoch: 4330, Loss: 0.2507
Epoch: 4331, Loss: 0.2555
Epoch: 4332, Loss: 0.2495
Epoch: 4333, Loss: 0.2564
Epoch: 4334, Loss: 0.2504
Epoch: 4335, Loss: 0.2506
Epoch: 4336, Loss: 0.2531
Epoch: 4337, Loss: 0.2683
Epoch: 4338, Loss: 0.2516
Epoch: 4339, Loss: 0.2539
Evaluating: 100%|| 64484/64484 [00:02<00:00, 32174.16it/s]
Epoch: 4340, Loss: 0.2596, Train: 0.9215, Val: 0.9181, test: 0.9193
Epoch: 4341, Loss: 0.2538
Epoch: 4342, Loss: 0.2503
Epoch: 4343, Loss: 0.2517
Epoch: 4344, Loss: 0.2537
Epoch: 4345, Loss: 0.2514
Epoch: 4346, Loss: 0.2513
Epoch: 4347, Loss: 0.2496
Epoch: 4348, Loss: 0.2483
Epoch: 4349, Loss: 0.3064
Epoch: 4350, Loss: 0.2616
Epoch: 4351, Loss: 0.2725
Epoch: 4352, Loss: 0.2597
Epoch: 4353, Loss: 0.2604
Epoch: 4354, Loss: 0.2674
Epoch: 4355, Loss: 0.2585
Epoch: 4356, Loss: 0.2665
Epoch: 4357, Loss: 0.2651
Epoch: 4358, Loss: 0.2587
Epoch: 4359, Loss: 0.2582
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36636.50it/s]
Epoch: 4360, Loss: 0.2637, Train: 0.9213, Val: 0.9174, test: 0.9184
Epoch: 4361, Loss: 0.2744
Epoch: 4362, Loss: 0.2594
Epoch: 4363, Loss: 0.2635
Epoch: 4364, Loss: 0.2578
Epoch: 4365, Loss: 0.2549
Epoch: 4366, Loss: 0.2525
Epoch: 4367, Loss: 0.2598
Epoch: 4368, Loss: 0.2532
Epoch: 4369, Loss: 0.2638
Epoch: 4370, Loss: 0.2617
Epoch: 4371, Loss: 0.2560
Epoch: 4372, Loss: 0.2628
Epoch: 4373, Loss: 0.2617
Epoch: 4374, Loss: 0.2613
Epoch: 4375, Loss: 0.2541
Epoch: 4376, Loss: 0.2575
Epoch: 4377, Loss: 0.2542
Epoch: 4378, Loss: 0.2640
Epoch: 4379, Loss: 0.2531
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36050.01it/s]
Epoch: 4380, Loss: 0.2538, Train: 0.9212, Val: 0.9196, test: 0.9193
Epoch: 4381, Loss: 0.2501
Epoch: 4382, Loss: 0.2576
Epoch: 4383, Loss: 0.2480
Epoch: 4384, Loss: 0.2518
Epoch: 4385, Loss: 0.2521
Epoch: 4386, Loss: 0.2496
Epoch: 4387, Loss: 0.2541
Epoch: 4388, Loss: 0.2532
Epoch: 4389, Loss: 0.2538
Epoch: 4390, Loss: 0.2507
Epoch: 4391, Loss: 0.2538
Epoch: 4392, Loss: 0.2510
Epoch: 4393, Loss: 0.2572
Epoch: 4394, Loss: 0.2521
Epoch: 4395, Loss: 2.6194
Epoch: 4396, Loss: 0.3706
Epoch: 4397, Loss: 0.3317
Epoch: 4398, Loss: 0.3362
Epoch: 4399, Loss: 0.3283
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36788.70it/s]
Epoch: 4400, Loss: 0.5310, Train: 0.9204, Val: 0.9178, test: 0.9165
Epoch: 4401, Loss: 0.3274
Epoch: 4402, Loss: 0.3470
Epoch: 4403, Loss: 0.4473
Epoch: 4404, Loss: 0.3261
Epoch: 4405, Loss: 0.4123
Epoch: 4406, Loss: 0.3851
Epoch: 4407, Loss: 0.3498
Epoch: 4408, Loss: 0.2906
Epoch: 4409, Loss: 0.3519
Epoch: 4410, Loss: 0.3651
Epoch: 4411, Loss: 0.3289
Epoch: 4412, Loss: 0.3309
Epoch: 4413, Loss: 0.3196
Epoch: 4414, Loss: 0.4034
Epoch: 4415, Loss: 0.3278
Epoch: 4416, Loss: 0.5039
Epoch: 4417, Loss: 0.3262
Epoch: 4418, Loss: 0.2643
Epoch: 4419, Loss: 0.3295
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36192.25it/s]
Epoch: 4420, Loss: 0.3175, Train: 0.9193, Val: 0.9165, test: 0.9153
Epoch: 4421, Loss: 0.3134
Epoch: 4422, Loss: 0.3103
Epoch: 4423, Loss: 0.3273
Epoch: 4424, Loss: 0.3408
Epoch: 4425, Loss: 0.3050
Epoch: 4426, Loss: 0.2867
Epoch: 4427, Loss: 0.2821
Epoch: 4428, Loss: 0.2693
Epoch: 4429, Loss: 0.3085
Epoch: 4430, Loss: 0.2772
Epoch: 4431, Loss: 0.2753
Epoch: 4432, Loss: 0.2744
Epoch: 4433, Loss: 0.2606
Epoch: 4434, Loss: 0.2666
Epoch: 4435, Loss: 0.2798
Epoch: 4436, Loss: 0.2892
Epoch: 4437, Loss: 0.2856
Epoch: 4438, Loss: 0.2589
Epoch: 4439, Loss: 0.2769
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36349.61it/s]
Epoch: 4440, Loss: 0.2803, Train: 0.9207, Val: 0.9193, test: 0.9193
Epoch: 4441, Loss: 0.2584
Epoch: 4442, Loss: 0.2858
Epoch: 4443, Loss: 0.2657
Epoch: 4444, Loss: 0.2552
Epoch: 4445, Loss: 0.2719
Epoch: 4446, Loss: 0.2544
Epoch: 4447, Loss: 0.2669
Epoch: 4448, Loss: 0.2549
Epoch: 4449, Loss: 0.2629
Epoch: 4450, Loss: 0.2598
Epoch: 4451, Loss: 0.2694
Epoch: 4452, Loss: 0.2606
Epoch: 4453, Loss: 0.2594
Epoch: 4454, Loss: 0.2630
Epoch: 4455, Loss: 0.3118
Epoch: 4456, Loss: 0.2619
Epoch: 4457, Loss: 0.2903
Epoch: 4458, Loss: 0.3062
Epoch: 4459, Loss: 0.3191
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36662.27it/s]
Epoch: 4460, Loss: 0.2903, Train: 0.9197, Val: 0.9184, test: 0.9168
Epoch: 4461, Loss: 0.3092
Epoch: 4462, Loss: 0.2949
Epoch: 4463, Loss: 0.2896
Epoch: 4464, Loss: 0.3078
Epoch: 4465, Loss: 0.3112
Epoch: 4466, Loss: 0.3097
Epoch: 4467, Loss: 0.3491
Epoch: 4468, Loss: 0.3070
Epoch: 4469, Loss: 0.3173
Epoch: 4470, Loss: 0.3294
Epoch: 4471, Loss: 0.3416
Epoch: 4472, Loss: 0.2831
Epoch: 4473, Loss: 0.2937
Epoch: 4474, Loss: 0.2932
Epoch: 4475, Loss: 0.3111
Epoch: 4476, Loss: 0.3701
Epoch: 4477, Loss: 0.2765
Epoch: 4478, Loss: 0.2906
Epoch: 4479, Loss: 0.2758
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37746.68it/s]
Epoch: 4480, Loss: 0.2850, Train: 0.9196, Val: 0.9178, test: 0.9175
Epoch: 4481, Loss: 0.3044
Epoch: 4482, Loss: 0.3012
Epoch: 4483, Loss: 0.2926
Epoch: 4484, Loss: 0.3268
Epoch: 4485, Loss: 0.2931
Epoch: 4486, Loss: 0.2877
Epoch: 4487, Loss: 0.4121
Epoch: 4488, Loss: 0.2739
Epoch: 4489, Loss: 0.3232
Epoch: 4490, Loss: 0.2988
Epoch: 4491, Loss: 0.2641
Epoch: 4492, Loss: 0.2788
Epoch: 4493, Loss: 0.2789
Epoch: 4494, Loss: 0.2785
Epoch: 4495, Loss: 0.2666
Epoch: 4496, Loss: 0.2841
Epoch: 4497, Loss: 0.2646
Epoch: 4498, Loss: 0.2648
Epoch: 4499, Loss: 0.2590
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37892.85it/s]
Epoch: 4500, Loss: 0.2733, Train: 0.9194, Val: 0.9168, test: 0.9184
Epoch: 4501, Loss: 0.2587
Epoch: 4502, Loss: 0.2935
Epoch: 4503, Loss: 0.2642
Epoch: 4504, Loss: 0.2641
Epoch: 4505, Loss: 0.2723
Epoch: 4506, Loss: 0.2609
Epoch: 4507, Loss: 0.2586
Epoch: 4508, Loss: 0.2738
Epoch: 4509, Loss: 0.2566
Epoch: 4510, Loss: 0.2990
Epoch: 4511, Loss: 0.2611
Epoch: 4512, Loss: 0.2651
Epoch: 4513, Loss: 0.2685
Epoch: 4514, Loss: 0.2762
Epoch: 4515, Loss: 0.2761
Epoch: 4516, Loss: 0.2755
Epoch: 4517, Loss: 0.2805
Epoch: 4518, Loss: 0.2707
Epoch: 4519, Loss: 0.2676
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37743.41it/s]
Epoch: 4520, Loss: 0.2625, Train: 0.9205, Val: 0.9187, test: 0.9190
Epoch: 4521, Loss: 0.2616
Epoch: 4522, Loss: 0.2696
Epoch: 4523, Loss: 0.2560
Epoch: 4524, Loss: 0.2563
Epoch: 4525, Loss: 0.2746
Epoch: 4526, Loss: 0.2550
Epoch: 4527, Loss: 0.2542
Epoch: 4528, Loss: 0.2594
Epoch: 4529, Loss: 0.2763
Epoch: 4530, Loss: 0.2557
Epoch: 4531, Loss: 0.2539
Epoch: 4532, Loss: 0.2592
Epoch: 4533, Loss: 0.2544
Epoch: 4534, Loss: 0.2577
Epoch: 4535, Loss: 0.2619
Epoch: 4536, Loss: 0.2537
Epoch: 4537, Loss: 0.2563
Epoch: 4538, Loss: 0.2539
Epoch: 4539, Loss: 0.2540
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37846.47it/s]
Epoch: 4540, Loss: 0.2509, Train: 0.9212, Val: 0.9202, test: 0.9178
Epoch: 4541, Loss: 0.2487
Epoch: 4542, Loss: 0.2506
Epoch: 4543, Loss: 0.2500
Epoch: 4544, Loss: 0.2756
Epoch: 4545, Loss: 0.2579
Epoch: 4546, Loss: 0.2487
Epoch: 4547, Loss: 0.2933
Epoch: 4548, Loss: 0.2591
Epoch: 4549, Loss: 0.2552
Epoch: 4550, Loss: 0.2737
Epoch: 4551, Loss: 0.2528
Epoch: 4552, Loss: 0.2569
Epoch: 4553, Loss: 0.2738
Epoch: 4554, Loss: 0.2649
Epoch: 4555, Loss: 0.2850
Epoch: 4556, Loss: 0.2607
Epoch: 4557, Loss: 0.2793
Epoch: 4558, Loss: 0.2491
Epoch: 4559, Loss: 0.2534
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37730.74it/s]
Epoch: 4560, Loss: 0.2560, Train: 0.9206, Val: 0.9196, test: 0.9196
Epoch: 4561, Loss: 0.2509
Epoch: 4562, Loss: 0.2512
Epoch: 4563, Loss: 0.2543
Epoch: 4564, Loss: 0.2498
Epoch: 4565, Loss: 0.2493
Epoch: 4566, Loss: 0.2564
Epoch: 4567, Loss: 0.2535
Epoch: 4568, Loss: 0.2507
Epoch: 4569, Loss: 0.2494
Epoch: 4570, Loss: 0.2519
Epoch: 4571, Loss: 0.2517
Epoch: 4572, Loss: 0.2565
Epoch: 4573, Loss: 0.2655
Epoch: 4574, Loss: 0.2553
Epoch: 4575, Loss: 0.2568
Epoch: 4576, Loss: 0.2522
Epoch: 4577, Loss: 0.2526
Epoch: 4578, Loss: 0.2576
Epoch: 4579, Loss: 0.2542
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36438.89it/s]
Epoch: 4580, Loss: 0.2504, Train: 0.9206, Val: 0.9199, test: 0.9181
Epoch: 4581, Loss: 0.2664
Epoch: 4582, Loss: 0.2613
Epoch: 4583, Loss: 0.2667
Epoch: 4584, Loss: 0.2674
Epoch: 4585, Loss: 0.2577
Epoch: 4586, Loss: 0.2560
Epoch: 4587, Loss: 0.2537
Epoch: 4588, Loss: 0.2543
Epoch: 4589, Loss: 0.2570
Epoch: 4590, Loss: 0.2527
Epoch: 4591, Loss: 0.2522
Epoch: 4592, Loss: 0.2480
Epoch: 4593, Loss: 0.2507
Epoch: 4594, Loss: 0.2515
Epoch: 4595, Loss: 0.2599
Epoch: 4596, Loss: 0.2547
Epoch: 4597, Loss: 0.2532
Epoch: 4598, Loss: 0.2671
Epoch: 4599, Loss: 0.2504
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36266.10it/s]
Epoch: 4600, Loss: 0.2533, Train: 0.9203, Val: 0.9162, test: 0.9193
Epoch: 4601, Loss: 0.2495
Epoch: 4602, Loss: 0.2546
Epoch: 4603, Loss: 0.2491
Epoch: 4604, Loss: 0.2523
Epoch: 4605, Loss: 0.2564
Epoch: 4606, Loss: 0.2510
Epoch: 4607, Loss: 0.2529
Epoch: 4608, Loss: 0.2502
Epoch: 4609, Loss: 0.2813
Epoch: 4610, Loss: 0.2503
Epoch: 4611, Loss: 0.2502
Epoch: 4612, Loss: 0.2504
Epoch: 4613, Loss: 0.2527
Epoch: 4614, Loss: 0.2516
Epoch: 4615, Loss: 0.2694
Epoch: 4616, Loss: 0.2493
Epoch: 4617, Loss: 0.2513
Epoch: 4618, Loss: 0.2494
Epoch: 4619, Loss: 0.2500
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36132.18it/s]
Epoch: 4620, Loss: 0.2479, Train: 0.9204, Val: 0.9190, test: 0.9190
Epoch: 4621, Loss: 0.2546
Epoch: 4622, Loss: 0.2512
Epoch: 4623, Loss: 0.2555
Epoch: 4624, Loss: 0.2508
Epoch: 4625, Loss: 0.2483
Epoch: 4626, Loss: 0.2481
Epoch: 4627, Loss: 0.2514
Epoch: 4628, Loss: 0.2497
Epoch: 4629, Loss: 0.2483
Epoch: 4630, Loss: 0.2456
Epoch: 4631, Loss: 0.2453
Epoch: 4632, Loss: 0.2486
Epoch: 4633, Loss: 0.2487
Epoch: 4634, Loss: 0.2516
Epoch: 4635, Loss: 0.2468
Epoch: 4636, Loss: 0.2481
Epoch: 4637, Loss: 0.2540
Epoch: 4638, Loss: 0.2449
Epoch: 4639, Loss: 0.2498
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37422.54it/s]
Epoch: 4640, Loss: 0.2506, Train: 0.9209, Val: 0.9202, test: 0.9184
Epoch: 4641, Loss: 0.2470
Epoch: 4642, Loss: 0.2484
Epoch: 4643, Loss: 0.2510
Epoch: 4644, Loss: 0.2477
Epoch: 4645, Loss: 0.2480
Epoch: 4646, Loss: 0.2495
Epoch: 4647, Loss: 0.2453
Epoch: 4648, Loss: 0.2478
Epoch: 4649, Loss: 0.2475
Epoch: 4650, Loss: 0.2490
Epoch: 4651, Loss: 0.2484
Epoch: 4652, Loss: 0.2474
Epoch: 4653, Loss: 0.2474
Epoch: 4654, Loss: 0.2609
Epoch: 4655, Loss: 0.2485
Epoch: 4656, Loss: 0.2465
Epoch: 4657, Loss: 0.2492
Epoch: 4658, Loss: 0.2478
Epoch: 4659, Loss: 0.2479
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29366.28it/s]
Epoch: 4660, Loss: 0.2510, Train: 0.9212, Val: 0.9202, test: 0.9200
Epoch: 4661, Loss: 0.2486
Epoch: 4662, Loss: 0.2467
Epoch: 4663, Loss: 0.2527
Epoch: 4664, Loss: 0.2542
Epoch: 4665, Loss: 0.2748
Epoch: 4666, Loss: 0.2469
Epoch: 4667, Loss: 0.2506
Epoch: 4668, Loss: 0.2472
Epoch: 4669, Loss: 0.2488
Epoch: 4670, Loss: 0.2478
Epoch: 4671, Loss: 0.2468
Epoch: 4672, Loss: 0.2535
Epoch: 4673, Loss: 0.2540
Epoch: 4674, Loss: 0.2477
Epoch: 4675, Loss: 0.2571
Epoch: 4676, Loss: 0.2494
Epoch: 4677, Loss: 0.2497
Epoch: 4678, Loss: 0.2476
Epoch: 4679, Loss: 0.2523
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30411.45it/s]
Epoch: 4680, Loss: 0.2488, Train: 0.9208, Val: 0.9184, test: 0.9196
Epoch: 4681, Loss: 0.2515
Epoch: 4682, Loss: 0.2485
Epoch: 4683, Loss: 0.2492
Epoch: 4684, Loss: 0.2476
Epoch: 4685, Loss: 0.2497
Epoch: 4686, Loss: 0.2504
Epoch: 4687, Loss: 0.2464
Epoch: 4688, Loss: 0.2460
Epoch: 4689, Loss: 0.2492
Epoch: 4690, Loss: 0.2496
Epoch: 4691, Loss: 0.2455
Epoch: 4692, Loss: 0.2495
Epoch: 4693, Loss: 0.2500
Epoch: 4694, Loss: 0.2479
Epoch: 4695, Loss: 0.2491
Epoch: 4696, Loss: 0.2461
Epoch: 4697, Loss: 0.2452
Epoch: 4698, Loss: 0.2463
Epoch: 4699, Loss: 0.2480
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37030.91it/s]
Epoch: 4700, Loss: 0.2507, Train: 0.9205, Val: 0.9181, test: 0.9171
Epoch: 4701, Loss: 0.2511
Epoch: 4702, Loss: 0.2451
Epoch: 4703, Loss: 0.2498
Epoch: 4704, Loss: 0.2474
Epoch: 4705, Loss: 0.2512
Epoch: 4706, Loss: 0.2488
Epoch: 4707, Loss: 0.2471
Epoch: 4708, Loss: 0.2465
Epoch: 4709, Loss: 0.2478
Epoch: 4710, Loss: 0.2466
Epoch: 4711, Loss: 0.2461
Epoch: 4712, Loss: 0.2469
Epoch: 4713, Loss: 0.2483
Epoch: 4714, Loss: 0.2469
Epoch: 4715, Loss: 0.2499
Epoch: 4716, Loss: 0.2472
Epoch: 4717, Loss: 0.2452
Epoch: 4718, Loss: 0.2478
Epoch: 4719, Loss: 0.2467
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35145.92it/s]
Epoch: 4720, Loss: 0.2609, Train: 0.9206, Val: 0.9187, test: 0.9190
Epoch: 4721, Loss: 0.2465
Epoch: 4722, Loss: 0.2467
Epoch: 4723, Loss: 0.2544
Epoch: 4724, Loss: 0.2466
Epoch: 4725, Loss: 0.2467
Epoch: 4726, Loss: 0.2456
Epoch: 4727, Loss: 0.2492
Epoch: 4728, Loss: 0.2478
Epoch: 4729, Loss: 0.2503
Epoch: 4730, Loss: 0.2531
Epoch: 4731, Loss: 0.2488
Epoch: 4732, Loss: 0.2468
Epoch: 4733, Loss: 0.2461
Epoch: 4734, Loss: 0.2446
Epoch: 4735, Loss: 0.2453
Epoch: 4736, Loss: 0.2473
Epoch: 4737, Loss: 0.2497
Epoch: 4738, Loss: 0.2458
Epoch: 4739, Loss: 0.2446
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35765.89it/s]
Epoch: 4740, Loss: 0.2454, Train: 0.9215, Val: 0.9205, test: 0.9196
Epoch: 4741, Loss: 0.2491
Epoch: 4742, Loss: 0.2453
Epoch: 4743, Loss: 0.2501
Epoch: 4744, Loss: 0.2458
Epoch: 4745, Loss: 0.2459
Epoch: 4746, Loss: 0.2478
Epoch: 4747, Loss: 0.2519
Epoch: 4748, Loss: 0.2530
Epoch: 4749, Loss: 0.2463
Epoch: 4750, Loss: 0.2478
Epoch: 4751, Loss: 0.2476
Epoch: 4752, Loss: 0.2736
Epoch: 4753, Loss: 0.2490
Epoch: 4754, Loss: 0.2502
Epoch: 4755, Loss: 0.2483
Epoch: 4756, Loss: 0.2477
Epoch: 4757, Loss: 0.2543
Epoch: 4758, Loss: 0.2522
Epoch: 4759, Loss: 0.2487
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35232.37it/s]
Epoch: 4760, Loss: 0.2447, Train: 0.9205, Val: 0.9190, test: 0.9190
Epoch: 4761, Loss: 0.2511
Epoch: 4762, Loss: 0.2453
Epoch: 4763, Loss: 0.2446
Epoch: 4764, Loss: 0.2477
Epoch: 4765, Loss: 0.2481
Epoch: 4766, Loss: 0.2498
Epoch: 4767, Loss: 0.2505
Epoch: 4768, Loss: 0.2501
Epoch: 4769, Loss: 0.2472
Epoch: 4770, Loss: 0.2477
Epoch: 4771, Loss: 0.2447
Epoch: 4772, Loss: 0.2486
Epoch: 4773, Loss: 0.2466
Epoch: 4774, Loss: 0.2478
Epoch: 4775, Loss: 0.2475
Epoch: 4776, Loss: 0.2482
Epoch: 4777, Loss: 0.2454
Epoch: 4778, Loss: 0.2493
Epoch: 4779, Loss: 0.2435
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35112.94it/s]
Epoch: 4780, Loss: 0.2465, Train: 0.9215, Val: 0.9199, test: 0.9196
Epoch: 4781, Loss: 0.2468
Epoch: 4782, Loss: 0.2487
Epoch: 4783, Loss: 0.2450
Epoch: 4784, Loss: 0.2469
Epoch: 4785, Loss: 0.2481
Epoch: 4786, Loss: 0.2455
Epoch: 4787, Loss: 0.2470
Epoch: 4788, Loss: 0.2500
Epoch: 4789, Loss: 0.2463
Epoch: 4790, Loss: 0.2440
Epoch: 4791, Loss: 0.2492
Epoch: 4792, Loss: 0.2630
Epoch: 4793, Loss: 0.2756
Epoch: 4794, Loss: 0.2749
Epoch: 4795, Loss: 0.2632
Epoch: 4796, Loss: 0.2584
Epoch: 4797, Loss: 0.2584
Epoch: 4798, Loss: 0.2613
Epoch: 4799, Loss: 0.2755
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35075.15it/s]
Epoch: 4800, Loss: 0.2789, Train: 0.9208, Val: 0.9181, test: 0.9203
Epoch: 4801, Loss: 0.2766
Epoch: 4802, Loss: 0.2863
Epoch: 4803, Loss: 0.2683
Epoch: 4804, Loss: 0.2673
Epoch: 4805, Loss: 0.3469
Epoch: 4806, Loss: 0.2699
Epoch: 4807, Loss: 0.2903
Epoch: 4808, Loss: 0.2797
Epoch: 4809, Loss: 0.2793
Epoch: 4810, Loss: 0.2918
Epoch: 4811, Loss: 0.2826
Epoch: 4812, Loss: 0.2734
Epoch: 4813, Loss: 0.2686
Epoch: 4814, Loss: 0.3162
Epoch: 4815, Loss: 0.2918
Epoch: 4816, Loss: 0.2696
Epoch: 4817, Loss: 0.2620
Epoch: 4818, Loss: 0.2613
Epoch: 4819, Loss: 0.2653
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36070.19it/s]
Epoch: 4820, Loss: 0.2586, Train: 0.9213, Val: 0.9205, test: 0.9200
Epoch: 4821, Loss: 0.2577
Epoch: 4822, Loss: 0.2534
Epoch: 4823, Loss: 0.2923
Epoch: 4824, Loss: 0.2629
Epoch: 4825, Loss: 0.2533
Epoch: 4826, Loss: 0.2504
Epoch: 4827, Loss: 0.2495
Epoch: 4828, Loss: 0.2523
Epoch: 4829, Loss: 0.2500
Epoch: 4830, Loss: 0.2520
Epoch: 4831, Loss: 0.2522
Epoch: 4832, Loss: 0.2504
Epoch: 4833, Loss: 0.2473
Epoch: 4834, Loss: 0.2461
Epoch: 4835, Loss: 0.2539
Epoch: 4836, Loss: 0.2581
Epoch: 4837, Loss: 0.2519
Epoch: 4838, Loss: 0.2751
Epoch: 4839, Loss: 0.2419
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36407.80it/s]
Epoch: 4840, Loss: 0.2550, Train: 0.9217, Val: 0.9205, test: 0.9200
Epoch: 4841, Loss: 0.2485
Epoch: 4842, Loss: 0.2504
Epoch: 4843, Loss: 0.2606
Epoch: 4844, Loss: 0.2495
Epoch: 4845, Loss: 0.2899
Epoch: 4846, Loss: 0.2487
Epoch: 4847, Loss: 0.2448
Epoch: 4848, Loss: 0.2485
Epoch: 4849, Loss: 0.2476
Epoch: 4850, Loss: 0.2500
Epoch: 4851, Loss: 0.2467
Epoch: 4852, Loss: 0.2803
Epoch: 4853, Loss: 0.2473
Epoch: 4854, Loss: 0.2458
Epoch: 4855, Loss: 0.2509
Epoch: 4856, Loss: 0.2442
Epoch: 4857, Loss: 0.2485
Epoch: 4858, Loss: 0.2461
Epoch: 4859, Loss: 0.2584
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37755.84it/s]
Epoch: 4860, Loss: 0.2470, Train: 0.9213, Val: 0.9196, test: 0.9203
Epoch: 4861, Loss: 0.2481
Epoch: 4862, Loss: 0.2455
Epoch: 4863, Loss: 0.2491
Epoch: 4864, Loss: 0.2475
Epoch: 4865, Loss: 0.2633
Epoch: 4866, Loss: 0.2483
Epoch: 4867, Loss: 0.2447
Epoch: 4868, Loss: 0.2501
Epoch: 4869, Loss: 0.2769
Epoch: 4870, Loss: 0.2522
Epoch: 4871, Loss: 0.2547
Epoch: 4872, Loss: 0.2503
Epoch: 4873, Loss: 0.2517
Epoch: 4874, Loss: 0.2455
Epoch: 4875, Loss: 0.2468
Epoch: 4876, Loss: 0.2453
Epoch: 4877, Loss: 0.2488
Epoch: 4878, Loss: 0.2461
Epoch: 4879, Loss: 0.2441
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37953.54it/s]
Epoch: 4880, Loss: 0.2479, Train: 0.9214, Val: 0.9199, test: 0.9184
Epoch: 4881, Loss: 0.2482
Epoch: 4882, Loss: 0.2469
Epoch: 4883, Loss: 0.2453
Epoch: 4884, Loss: 0.2417
Epoch: 4885, Loss: 0.2438
Epoch: 4886, Loss: 0.2483
Epoch: 4887, Loss: 0.2485
Epoch: 4888, Loss: 0.2448
Epoch: 4889, Loss: 0.2586
Epoch: 4890, Loss: 0.2466
Epoch: 4891, Loss: 0.2495
Epoch: 4892, Loss: 0.2478
Epoch: 4893, Loss: 0.2457
Epoch: 4894, Loss: 0.3212
Epoch: 4895, Loss: 0.2446
Epoch: 4896, Loss: 0.2546
Epoch: 4897, Loss: 0.2462
Epoch: 4898, Loss: 0.2524
Epoch: 4899, Loss: 0.2540
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30510.20it/s]
Epoch: 4900, Loss: 0.2614, Train: 0.9221, Val: 0.9193, test: 0.9209
Epoch: 4901, Loss: 0.2496
Epoch: 4902, Loss: 0.2506
Epoch: 4903, Loss: 0.2482
Epoch: 4904, Loss: 0.2480
Epoch: 4905, Loss: 0.2480
Epoch: 4906, Loss: 0.2461
Epoch: 4907, Loss: 0.2522
Epoch: 4908, Loss: 0.2452
Epoch: 4909, Loss: 0.2527
Epoch: 4910, Loss: 0.2479
Epoch: 4911, Loss: 0.2498
Epoch: 4912, Loss: 0.2467
Epoch: 4913, Loss: 0.2465
Epoch: 4914, Loss: 0.2470
Epoch: 4915, Loss: 0.2499
Epoch: 4916, Loss: 0.2501
Epoch: 4917, Loss: 0.2490
Epoch: 4918, Loss: 0.2488
Epoch: 4919, Loss: 0.2456
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29930.62it/s]
Epoch: 4920, Loss: 0.2477, Train: 0.9222, Val: 0.9205, test: 0.9209
Epoch: 4921, Loss: 0.2486
Epoch: 4922, Loss: 0.2439
Epoch: 4923, Loss: 0.2477
Epoch: 4924, Loss: 0.2463
Epoch: 4925, Loss: 0.2455
Epoch: 4926, Loss: 0.2475
Epoch: 4927, Loss: 0.2431
Epoch: 4928, Loss: 0.2432
Epoch: 4929, Loss: 0.2435
Epoch: 4930, Loss: 0.2435
Epoch: 4931, Loss: 0.2449
Epoch: 4932, Loss: 0.2525
Epoch: 4933, Loss: 0.2427
Epoch: 4934, Loss: 0.2427
Epoch: 4935, Loss: 0.2441
Epoch: 4936, Loss: 0.2439
Epoch: 4937, Loss: 0.2456
Epoch: 4938, Loss: 0.2454
Epoch: 4939, Loss: 0.2426
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29633.84it/s]
Epoch: 4940, Loss: 0.2440, Train: 0.9221, Val: 0.9202, test: 0.9206
Epoch: 4941, Loss: 0.2424
Epoch: 4942, Loss: 0.2459
Epoch: 4943, Loss: 0.2430
Epoch: 4944, Loss: 0.2445
Epoch: 4945, Loss: 0.2466
Epoch: 4946, Loss: 0.2446
Epoch: 4947, Loss: 0.2460
Epoch: 4948, Loss: 0.2435
Epoch: 4949, Loss: 0.2447
Epoch: 4950, Loss: 0.2443
Epoch: 4951, Loss: 0.2436
Epoch: 4952, Loss: 0.2432
Epoch: 4953, Loss: 0.2441
Epoch: 4954, Loss: 0.2416
Epoch: 4955, Loss: 0.2411
Epoch: 4956, Loss: 0.2442
Epoch: 4957, Loss: 0.2505
Epoch: 4958, Loss: 0.2406
Epoch: 4959, Loss: 0.2497
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29722.58it/s]
Epoch: 4960, Loss: 0.2412, Train: 0.9222, Val: 0.9199, test: 0.9209
Epoch: 4961, Loss: 0.2399
Epoch: 4962, Loss: 0.2413
Epoch: 4963, Loss: 0.2402
Epoch: 4964, Loss: 0.2428
Epoch: 4965, Loss: 0.2403
Epoch: 4966, Loss: 0.2406
Epoch: 4967, Loss: 0.2444
Epoch: 4968, Loss: 0.2418
Epoch: 4969, Loss: 0.2422
Epoch: 4970, Loss: 0.2419
Epoch: 4971, Loss: 0.2458
Epoch: 4972, Loss: 0.2434
Epoch: 4973, Loss: 0.2448
Epoch: 4974, Loss: 0.2422
Epoch: 4975, Loss: 0.2413
Epoch: 4976, Loss: 0.2432
Epoch: 4977, Loss: 0.2437
Epoch: 4978, Loss: 0.2412
Epoch: 4979, Loss: 0.2415
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30423.50it/s]
Epoch: 4980, Loss: 0.2397, Train: 0.9211, Val: 0.9202, test: 0.9193
Epoch: 4981, Loss: 0.2463
Epoch: 4982, Loss: 0.2431
Epoch: 4983, Loss: 0.2391
Epoch: 4984, Loss: 0.2394
Epoch: 4985, Loss: 0.2397
Epoch: 4986, Loss: 0.2399
Epoch: 4987, Loss: 0.2421
Epoch: 4988, Loss: 0.2407
Epoch: 4989, Loss: 0.2421
Epoch: 4990, Loss: 0.2432
Epoch: 4991, Loss: 0.2413
Epoch: 4992, Loss: 0.2424
Epoch: 4993, Loss: 0.2427
Epoch: 4994, Loss: 0.2413
Epoch: 4995, Loss: 0.2430
Epoch: 4996, Loss: 0.2462
Epoch: 4997, Loss: 0.2426
Epoch: 4998, Loss: 0.2402
Epoch: 4999, Loss: 0.2496
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30461.79it/s]
Epoch: 5000, Loss: 0.2428, Train: 0.9217, Val: 0.9212, test: 0.9193
Epoch: 5001, Loss: 0.2407
Epoch: 5002, Loss: 0.2413
Epoch: 5003, Loss: 0.2433
Epoch: 5004, Loss: 0.2450
Epoch: 5005, Loss: 0.2404
Epoch: 5006, Loss: 0.2410
Epoch: 5007, Loss: 0.2431
Epoch: 5008, Loss: 0.2413
Epoch: 5009, Loss: 0.2426
Epoch: 5010, Loss: 0.2440
Epoch: 5011, Loss: 0.2433
Epoch: 5012, Loss: 0.2415
Epoch: 5013, Loss: 0.2432
Epoch: 5014, Loss: 0.2478
Epoch: 5015, Loss: 0.2435
Epoch: 5016, Loss: 0.2431
Epoch: 5017, Loss: 0.2424
Epoch: 5018, Loss: 0.2439
Epoch: 5019, Loss: 0.2426
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37736.04it/s]
Epoch: 5020, Loss: 0.2425, Train: 0.9215, Val: 0.9190, test: 0.9200
Epoch: 5021, Loss: 0.2441
Epoch: 5022, Loss: 0.2395
Epoch: 5023, Loss: 0.2417
Epoch: 5024, Loss: 0.2408
Epoch: 5025, Loss: 0.2425
Epoch: 5026, Loss: 0.2421
Epoch: 5027, Loss: 0.2399
Epoch: 5028, Loss: 0.2436
Epoch: 5029, Loss: 0.2413
Epoch: 5030, Loss: 0.2428
Epoch: 5031, Loss: 0.2434
Epoch: 5032, Loss: 0.2423
Epoch: 5033, Loss: 0.2426
Epoch: 5034, Loss: 0.2425
Epoch: 5035, Loss: 0.2405
Epoch: 5036, Loss: 0.2419
Epoch: 5037, Loss: 0.2431
Epoch: 5038, Loss: 0.2441
Epoch: 5039, Loss: 0.2381
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30147.60it/s]
Epoch: 5040, Loss: 0.2395, Train: 0.9218, Val: 0.9202, test: 0.9200
Epoch: 5041, Loss: 0.2424
Epoch: 5042, Loss: 0.2377
Epoch: 5043, Loss: 0.2432
Epoch: 5044, Loss: 0.2425
Epoch: 5045, Loss: 0.2398
Epoch: 5046, Loss: 0.2391
Epoch: 5047, Loss: 0.2408
Epoch: 5048, Loss: 0.2427
Epoch: 5049, Loss: 0.2432
Epoch: 5050, Loss: 0.2402
Epoch: 5051, Loss: 0.2391
Epoch: 5052, Loss: 0.2387
Epoch: 5053, Loss: 0.2418
Epoch: 5054, Loss: 0.2397
Epoch: 5055, Loss: 0.2553
Epoch: 5056, Loss: 0.2394
Epoch: 5057, Loss: 0.2411
Epoch: 5058, Loss: 0.2431
Epoch: 5059, Loss: 0.2425
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30519.24it/s]
Epoch: 5060, Loss: 0.2465, Train: 0.9222, Val: 0.9209, test: 0.9209
Epoch: 5061, Loss: 0.2448
Epoch: 5062, Loss: 0.2414
Epoch: 5063, Loss: 0.2399
Epoch: 5064, Loss: 0.2433
Epoch: 5065, Loss: 0.2408
Epoch: 5066, Loss: 0.2430
Epoch: 5067, Loss: 0.2395
Epoch: 5068, Loss: 0.2406
Epoch: 5069, Loss: 0.2415
Epoch: 5070, Loss: 0.2421
Epoch: 5071, Loss: 0.2431
Epoch: 5072, Loss: 0.2423
Epoch: 5073, Loss: 0.2402
Epoch: 5074, Loss: 0.2426
Epoch: 5075, Loss: 0.2390
Epoch: 5076, Loss: 0.2373
Epoch: 5077, Loss: 0.2415
Epoch: 5078, Loss: 0.2416
Epoch: 5079, Loss: 0.2396
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30007.52it/s]
Epoch: 5080, Loss: 0.2436, Train: 0.9213, Val: 0.9193, test: 0.9190
Epoch: 5081, Loss: 0.2425
Epoch: 5082, Loss: 0.2492
Epoch: 5083, Loss: 0.2412
Epoch: 5084, Loss: 0.2405
Epoch: 5085, Loss: 0.2418
Epoch: 5086, Loss: 0.2379
Epoch: 5087, Loss: 0.2433
Epoch: 5088, Loss: 0.2410
Epoch: 5089, Loss: 0.2396
Epoch: 5090, Loss: 0.2423
Epoch: 5091, Loss: 0.2430
Epoch: 5092, Loss: 0.2400
Epoch: 5093, Loss: 0.2407
Epoch: 5094, Loss: 0.2451
Epoch: 5095, Loss: 0.2388
Epoch: 5096, Loss: 0.2385
Epoch: 5097, Loss: 0.2418
Epoch: 5098, Loss: 0.2399
Epoch: 5099, Loss: 0.2392
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35942.42it/s]
Epoch: 5100, Loss: 0.2404, Train: 0.9228, Val: 0.9215, test: 0.9206
Epoch: 5101, Loss: 0.2410
Epoch: 5102, Loss: 0.2426
Epoch: 5103, Loss: 0.2404
Epoch: 5104, Loss: 0.2389
Epoch: 5105, Loss: 0.2398
Epoch: 5106, Loss: 0.2393
Epoch: 5107, Loss: 0.2361
Epoch: 5108, Loss: 0.2398
Epoch: 5109, Loss: 0.2504
Epoch: 5110, Loss: 0.2421
Epoch: 5111, Loss: 0.2383
Epoch: 5112, Loss: 0.2413
Epoch: 5113, Loss: 0.2393
Epoch: 5114, Loss: 0.2392
Epoch: 5115, Loss: 0.2425
Epoch: 5116, Loss: 0.2418
Epoch: 5117, Loss: 0.2449
Epoch: 5118, Loss: 0.2403
Epoch: 5119, Loss: 0.2421
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37479.89it/s]
Epoch: 5120, Loss: 0.2412, Train: 0.9220, Val: 0.9209, test: 0.9212
Epoch: 5121, Loss: 0.2397
Epoch: 5122, Loss: 0.2431
Epoch: 5123, Loss: 0.2445
Epoch: 5124, Loss: 0.2388
Epoch: 5125, Loss: 0.2402
Epoch: 5126, Loss: 0.2420
Epoch: 5127, Loss: 0.2413
Epoch: 5128, Loss: 0.2415
Epoch: 5129, Loss: 0.2518
Epoch: 5130, Loss: 0.2690
Epoch: 5131, Loss: 0.2421
Epoch: 5132, Loss: 0.2427
Epoch: 5133, Loss: 0.2401
Epoch: 5134, Loss: 0.2470
Epoch: 5135, Loss: 0.2470
Epoch: 5136, Loss: 0.2443
Epoch: 5137, Loss: 0.2444
Epoch: 5138, Loss: 0.2463
Epoch: 5139, Loss: 0.2449
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36798.24it/s]
Epoch: 5140, Loss: 0.2445, Train: 0.9219, Val: 0.9205, test: 0.9200
Epoch: 5141, Loss: 0.2458
Epoch: 5142, Loss: 0.2479
Epoch: 5143, Loss: 0.2413
Epoch: 5144, Loss: 0.2432
Epoch: 5145, Loss: 0.2422
Epoch: 5146, Loss: 0.2414
Epoch: 5147, Loss: 0.2377
Epoch: 5148, Loss: 0.2402
Epoch: 5149, Loss: 0.2386
Epoch: 5150, Loss: 0.2402
Epoch: 5151, Loss: 0.2507
Epoch: 5152, Loss: 0.2384
Epoch: 5153, Loss: 0.2395
Epoch: 5154, Loss: 0.2420
Epoch: 5155, Loss: 0.2426
Epoch: 5156, Loss: 0.2459
Epoch: 5157, Loss: 0.2426
Epoch: 5158, Loss: 0.2414
Epoch: 5159, Loss: 0.2437
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37406.50it/s]
Epoch: 5160, Loss: 0.2405, Train: 0.9230, Val: 0.9215, test: 0.9203
Epoch: 5161, Loss: 0.2412
Epoch: 5162, Loss: 0.2421
Epoch: 5163, Loss: 0.2406
Epoch: 5164, Loss: 0.2417
Epoch: 5165, Loss: 0.2379
Epoch: 5166, Loss: 0.2396
Epoch: 5167, Loss: 0.2420
Epoch: 5168, Loss: 0.2475
Epoch: 5169, Loss: 0.2400
Epoch: 5170, Loss: 0.2405
Epoch: 5171, Loss: 0.2402
Epoch: 5172, Loss: 0.2428
Epoch: 5173, Loss: 0.2392
Epoch: 5174, Loss: 0.2415
Epoch: 5175, Loss: 0.2362
Epoch: 5176, Loss: 0.2407
Epoch: 5177, Loss: 0.2392
Epoch: 5178, Loss: 0.2424
Epoch: 5179, Loss: 0.2383
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37825.43it/s]
Epoch: 5180, Loss: 0.2392, Train: 0.9223, Val: 0.9205, test: 0.9200
Epoch: 5181, Loss: 0.2404
Epoch: 5182, Loss: 0.2369
Epoch: 5183, Loss: 0.2370
Epoch: 5184, Loss: 0.2378
Epoch: 5185, Loss: 0.2419
Epoch: 5186, Loss: 0.2424
Epoch: 5187, Loss: 0.2444
Epoch: 5188, Loss: 0.2458
Epoch: 5189, Loss: 0.2389
Epoch: 5190, Loss: 0.2425
Epoch: 5191, Loss: 0.2374
Epoch: 5192, Loss: 0.2424
Epoch: 5193, Loss: 0.2388
Epoch: 5194, Loss: 0.2401
Epoch: 5195, Loss: 0.2434
Epoch: 5196, Loss: 0.2401
Epoch: 5197, Loss: 0.2399
Epoch: 5198, Loss: 0.2402
Epoch: 5199, Loss: 0.2397
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36881.66it/s]
Epoch: 5200, Loss: 0.2416, Train: 0.9229, Val: 0.9205, test: 0.9203
Epoch: 5201, Loss: 0.2398
Epoch: 5202, Loss: 0.2370
Epoch: 5203, Loss: 0.2411
Epoch: 5204, Loss: 0.2382
Epoch: 5205, Loss: 0.2429
Epoch: 5206, Loss: 0.2388
Epoch: 5207, Loss: 0.2448
Epoch: 5208, Loss: 0.2387
Epoch: 5209, Loss: 0.2362
Epoch: 5210, Loss: 0.2388
Epoch: 5211, Loss: 0.2386
Epoch: 5212, Loss: 0.2380
Epoch: 5213, Loss: 0.2399
Epoch: 5214, Loss: 0.2378
Epoch: 5215, Loss: 0.2369
Epoch: 5216, Loss: 0.2726
Epoch: 5217, Loss: 0.2387
Epoch: 5218, Loss: 0.2412
Epoch: 5219, Loss: 0.2460
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36325.04it/s]
Epoch: 5220, Loss: 0.2392, Train: 0.9232, Val: 0.9221, test: 0.9212
Epoch: 5221, Loss: 0.2387
Epoch: 5222, Loss: 0.2382
Epoch: 5223, Loss: 0.2493
Epoch: 5224, Loss: 0.2426
Epoch: 5225, Loss: 0.2458
Epoch: 5226, Loss: 0.2458
Epoch: 5227, Loss: 0.2360
Epoch: 5228, Loss: 0.2435
Epoch: 5229, Loss: 0.2394
Epoch: 5230, Loss: 0.2363
Epoch: 5231, Loss: 0.2432
Epoch: 5232, Loss: 0.2431
Epoch: 5233, Loss: 0.2422
Epoch: 5234, Loss: 0.2445
Epoch: 5235, Loss: 0.2435
Epoch: 5236, Loss: 0.2422
Epoch: 5237, Loss: 0.2380
Epoch: 5238, Loss: 0.2402
Epoch: 5239, Loss: 0.2397
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35442.45it/s]
Epoch: 5240, Loss: 0.2412, Train: 0.9236, Val: 0.9221, test: 0.9218
Epoch: 5241, Loss: 0.2405
Epoch: 5242, Loss: 0.2425
Epoch: 5243, Loss: 0.2406
Epoch: 5244, Loss: 0.2373
Epoch: 5245, Loss: 0.2398
Epoch: 5246, Loss: 0.2390
Epoch: 5247, Loss: 0.2361
Epoch: 5248, Loss: 0.2419
Epoch: 5249, Loss: 0.2411
Epoch: 5250, Loss: 0.2361
Epoch: 5251, Loss: 0.2393
Epoch: 5252, Loss: 0.2421
Epoch: 5253, Loss: 0.2410
Epoch: 5254, Loss: 0.2357
Epoch: 5255, Loss: 0.2373
Epoch: 5256, Loss: 0.2382
Epoch: 5257, Loss: 0.2379
Epoch: 5258, Loss: 0.2381
Epoch: 5259, Loss: 0.2378
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36211.04it/s]
Epoch: 5260, Loss: 0.2373, Train: 0.9239, Val: 0.9230, test: 0.9215
Epoch: 5261, Loss: 0.2344
Epoch: 5262, Loss: 0.2360
Epoch: 5263, Loss: 0.2378
Epoch: 5264, Loss: 0.2359
Epoch: 5265, Loss: 0.2359
Epoch: 5266, Loss: 0.2368
Epoch: 5267, Loss: 0.2343
Epoch: 5268, Loss: 0.2373
Epoch: 5269, Loss: 0.2332
Epoch: 5270, Loss: 0.2381
Epoch: 5271, Loss: 0.2337
Epoch: 5272, Loss: 0.2382
Epoch: 5273, Loss: 0.2380
Epoch: 5274, Loss: 0.2405
Epoch: 5275, Loss: 0.2385
Epoch: 5276, Loss: 0.2371
Epoch: 5277, Loss: 0.2385
Epoch: 5278, Loss: 0.2392
Epoch: 5279, Loss: 0.2381
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35424.53it/s]
Epoch: 5280, Loss: 0.2416, Train: 0.9229, Val: 0.9221, test: 0.9206
Epoch: 5281, Loss: 0.2352
Epoch: 5282, Loss: 0.2396
Epoch: 5283, Loss: 0.2366
Epoch: 5284, Loss: 0.2339
Epoch: 5285, Loss: 0.2368
Epoch: 5286, Loss: 0.2405
Epoch: 5287, Loss: 0.2376
Epoch: 5288, Loss: 0.2380
Epoch: 5289, Loss: 0.2394
Epoch: 5290, Loss: 0.2383
Epoch: 5291, Loss: 0.2362
Epoch: 5292, Loss: 0.2351
Epoch: 5293, Loss: 0.2405
Epoch: 5294, Loss: 0.2372
Epoch: 5295, Loss: 0.2350
Epoch: 5296, Loss: 0.2322
Epoch: 5297, Loss: 0.2384
Epoch: 5298, Loss: 0.2393
Epoch: 5299, Loss: 0.2459
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37301.17it/s]
Epoch: 5300, Loss: 0.2388, Train: 0.9236, Val: 0.9221, test: 0.9225
Epoch: 5301, Loss: 0.2374
Epoch: 5302, Loss: 0.2353
Epoch: 5303, Loss: 0.2361
Epoch: 5304, Loss: 0.2364
Epoch: 5305, Loss: 0.2360
Epoch: 5306, Loss: 0.2369
Epoch: 5307, Loss: 0.2360
Epoch: 5308, Loss: 0.2351
Epoch: 5309, Loss: 0.2355
Epoch: 5310, Loss: 0.2374
Epoch: 5311, Loss: 0.2414
Epoch: 5312, Loss: 0.2361
Epoch: 5313, Loss: 0.2379
Epoch: 5314, Loss: 0.2379
Epoch: 5315, Loss: 0.2345
Epoch: 5316, Loss: 0.2379
Epoch: 5317, Loss: 0.2379
Epoch: 5318, Loss: 0.2366
Epoch: 5319, Loss: 0.2417
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37798.48it/s]
Epoch: 5320, Loss: 0.2412, Train: 0.9239, Val: 0.9230, test: 0.9209
Epoch: 5321, Loss: 0.2390
Epoch: 5322, Loss: 0.2430
Epoch: 5323, Loss: 0.2373
Epoch: 5324, Loss: 0.2365
Epoch: 5325, Loss: 0.2347
Epoch: 5326, Loss: 0.2366
Epoch: 5327, Loss: 0.2404
Epoch: 5328, Loss: 0.2448
Epoch: 5329, Loss: 0.2400
Epoch: 5330, Loss: 0.2377
Epoch: 5331, Loss: 0.2383
Epoch: 5332, Loss: 0.2402
Epoch: 5333, Loss: 0.2374
Epoch: 5334, Loss: 0.2381
Epoch: 5335, Loss: 0.2385
Epoch: 5336, Loss: 0.2382
Epoch: 5337, Loss: 0.2395
Epoch: 5338, Loss: 0.2377
Epoch: 5339, Loss: 0.2365
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35813.93it/s]
Epoch: 5340, Loss: 0.2400, Train: 0.9207, Val: 0.9190, test: 0.9190
Epoch: 5341, Loss: 0.2382
Epoch: 5342, Loss: 0.2388
Epoch: 5343, Loss: 0.2390
Epoch: 5344, Loss: 0.2391
Epoch: 5345, Loss: 0.2366
Epoch: 5346, Loss: 0.2365
Epoch: 5347, Loss: 0.2368
Epoch: 5348, Loss: 0.2362
Epoch: 5349, Loss: 0.2376
Epoch: 5350, Loss: 0.2452
Epoch: 5351, Loss: 0.2378
Epoch: 5352, Loss: 0.2519
Epoch: 5353, Loss: 0.2384
Epoch: 5354, Loss: 0.2352
Epoch: 5355, Loss: 0.2378
Epoch: 5356, Loss: 0.2369
Epoch: 5357, Loss: 0.2360
Epoch: 5358, Loss: 0.2356
Epoch: 5359, Loss: 0.2401
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38053.95it/s]
Epoch: 5360, Loss: 0.2365, Train: 0.9224, Val: 0.9202, test: 0.9200
Epoch: 5361, Loss: 0.2403
Epoch: 5362, Loss: 0.2379
Epoch: 5363, Loss: 0.2376
Epoch: 5364, Loss: 0.2386
Epoch: 5365, Loss: 0.2352
Epoch: 5366, Loss: 0.2357
Epoch: 5367, Loss: 0.2367
Epoch: 5368, Loss: 0.2397
Epoch: 5369, Loss: 0.2380
Epoch: 5370, Loss: 0.2401
Epoch: 5371, Loss: 0.2361
Epoch: 5372, Loss: 0.2401
Epoch: 5373, Loss: 0.2390
Epoch: 5374, Loss: 0.2369
Epoch: 5375, Loss: 0.2370
Epoch: 5376, Loss: 0.2367
Epoch: 5377, Loss: 0.2359
Epoch: 5378, Loss: 0.2358
Epoch: 5379, Loss: 0.2372
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36931.87it/s]
Epoch: 5380, Loss: 0.2373, Train: 0.9210, Val: 0.9184, test: 0.9196
Epoch: 5381, Loss: 0.3259
Epoch: 5382, Loss: 0.2455
Epoch: 5383, Loss: 0.2421
Epoch: 5384, Loss: 0.2499
Epoch: 5385, Loss: 0.2469
Epoch: 5386, Loss: 0.2543
Epoch: 5387, Loss: 0.2625
Epoch: 5388, Loss: 0.2478
Epoch: 5389, Loss: 0.2575
Epoch: 5390, Loss: 0.2549
Epoch: 5391, Loss: 0.2465
Epoch: 5392, Loss: 0.2430
Epoch: 5393, Loss: 0.2440
Epoch: 5394, Loss: 0.2490
Epoch: 5395, Loss: 0.2510
Epoch: 5396, Loss: 0.2476
Epoch: 5397, Loss: 0.2534
Epoch: 5398, Loss: 0.2443
Epoch: 5399, Loss: 0.2438
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37093.70it/s]
Epoch: 5400, Loss: 0.2426, Train: 0.9221, Val: 0.9196, test: 0.9203
Epoch: 5401, Loss: 0.2392
Epoch: 5402, Loss: 0.2391
Epoch: 5403, Loss: 0.2410
Epoch: 5404, Loss: 0.2373
Epoch: 5405, Loss: 0.2499
Epoch: 5406, Loss: 0.2407
Epoch: 5407, Loss: 0.2604
Epoch: 5408, Loss: 0.2450
Epoch: 5409, Loss: 0.2383
Epoch: 5410, Loss: 0.2434
Epoch: 5411, Loss: 0.2430
Epoch: 5412, Loss: 0.2531
Epoch: 5413, Loss: 0.2460
Epoch: 5414, Loss: 0.2378
Epoch: 5415, Loss: 0.2437
Epoch: 5416, Loss: 0.2384
Epoch: 5417, Loss: 0.2384
Epoch: 5418, Loss: 0.2410
Epoch: 5419, Loss: 0.2380
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36630.30it/s]
Epoch: 5420, Loss: 0.2368, Train: 0.9217, Val: 0.9196, test: 0.9203
Epoch: 5421, Loss: 0.2357
Epoch: 5422, Loss: 0.2370
Epoch: 5423, Loss: 0.2374
Epoch: 5424, Loss: 0.2383
Epoch: 5425, Loss: 0.2375
Epoch: 5426, Loss: 0.2362
Epoch: 5427, Loss: 0.2387
Epoch: 5428, Loss: 0.2386
Epoch: 5429, Loss: 0.2384
Epoch: 5430, Loss: 0.2391
Epoch: 5431, Loss: 0.2385
Epoch: 5432, Loss: 0.2360
Epoch: 5433, Loss: 0.2434
Epoch: 5434, Loss: 0.2461
Epoch: 5435, Loss: 0.2380
Epoch: 5436, Loss: 0.2373
Epoch: 5437, Loss: 0.2384
Epoch: 5438, Loss: 0.2373
Epoch: 5439, Loss: 0.2439
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36595.19it/s]
Epoch: 5440, Loss: 0.2412, Train: 0.9244, Val: 0.9224, test: 0.9212
Epoch: 5441, Loss: 0.2403
Epoch: 5442, Loss: 0.2394
Epoch: 5443, Loss: 0.2367
Epoch: 5444, Loss: 0.2509
Epoch: 5445, Loss: 0.2442
Epoch: 5446, Loss: 0.2496
Epoch: 5447, Loss: 0.2531
Epoch: 5448, Loss: 0.2422
Epoch: 5449, Loss: 0.2508
Epoch: 5450, Loss: 0.2475
Epoch: 5451, Loss: 0.2523
Epoch: 5452, Loss: 0.2478
Epoch: 5453, Loss: 0.2552
Epoch: 5454, Loss: 0.2747
Epoch: 5455, Loss: 0.2522
Epoch: 5456, Loss: 0.2479
Epoch: 5457, Loss: 0.2455
Epoch: 5458, Loss: 0.2466
Epoch: 5459, Loss: 0.2466
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36670.02it/s]
Epoch: 5460, Loss: 0.2488, Train: 0.9240, Val: 0.9227, test: 0.9203
Epoch: 5461, Loss: 0.2465
Epoch: 5462, Loss: 0.2422
Epoch: 5463, Loss: 0.2539
Epoch: 5464, Loss: 0.2453
Epoch: 5465, Loss: 0.2450
Epoch: 5466, Loss: 0.2391
Epoch: 5467, Loss: 0.2417
Epoch: 5468, Loss: 0.2383
Epoch: 5469, Loss: 0.2464
Epoch: 5470, Loss: 0.2378
Epoch: 5471, Loss: 0.2440
Epoch: 5472, Loss: 0.2409
Epoch: 5473, Loss: 0.2399
Epoch: 5474, Loss: 0.2376
Epoch: 5475, Loss: 0.2381
Epoch: 5476, Loss: 0.2363
Epoch: 5477, Loss: 0.2374
Epoch: 5478, Loss: 0.2409
Epoch: 5479, Loss: 0.2466
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36362.37it/s]
Epoch: 5480, Loss: 0.2395, Train: 0.9232, Val: 0.9212, test: 0.9203
Epoch: 5481, Loss: 0.2394
Epoch: 5482, Loss: 0.2388
Epoch: 5483, Loss: 0.2384
Epoch: 5484, Loss: 0.2368
Epoch: 5485, Loss: 0.2371
Epoch: 5486, Loss: 0.2394
Epoch: 5487, Loss: 0.2362
Epoch: 5488, Loss: 0.2371
Epoch: 5489, Loss: 0.2353
Epoch: 5490, Loss: 0.2403
Epoch: 5491, Loss: 0.2366
Epoch: 5492, Loss: 0.2409
Epoch: 5493, Loss: 0.2374
Epoch: 5494, Loss: 0.2384
Epoch: 5495, Loss: 0.2351
Epoch: 5496, Loss: 0.2357
Epoch: 5497, Loss: 0.2338
Epoch: 5498, Loss: 0.2366
Epoch: 5499, Loss: 0.2361
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34764.52it/s]
Epoch: 5500, Loss: 0.2353, Train: 0.9239, Val: 0.9215, test: 0.9218
Epoch: 5501, Loss: 0.2380
Epoch: 5502, Loss: 0.2380
Epoch: 5503, Loss: 0.2371
Epoch: 5504, Loss: 0.2356
Epoch: 5505, Loss: 0.2367
Epoch: 5506, Loss: 0.2390
Epoch: 5507, Loss: 0.2351
Epoch: 5508, Loss: 0.2362
Epoch: 5509, Loss: 0.2365
Epoch: 5510, Loss: 0.2405
Epoch: 5511, Loss: 0.2336
Epoch: 5512, Loss: 0.2385
Epoch: 5513, Loss: 0.2384
Epoch: 5514, Loss: 0.2353
Epoch: 5515, Loss: 0.2400
Epoch: 5516, Loss: 0.2354
Epoch: 5517, Loss: 0.2371
Epoch: 5518, Loss: 0.2360
Epoch: 5519, Loss: 0.2372
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37209.48it/s]
Epoch: 5520, Loss: 0.2365, Train: 0.9211, Val: 0.9193, test: 0.9193
Epoch: 5521, Loss: 0.2400
Epoch: 5522, Loss: 0.2389
Epoch: 5523, Loss: 0.2359
Epoch: 5524, Loss: 0.2349
Epoch: 5525, Loss: 0.2371
Epoch: 5526, Loss: 0.2436
Epoch: 5527, Loss: 0.2389
Epoch: 5528, Loss: 0.2399
Epoch: 5529, Loss: 0.2407
Epoch: 5530, Loss: 0.2405
Epoch: 5531, Loss: 0.2393
Epoch: 5532, Loss: 0.2401
Epoch: 5533, Loss: 0.2387
Epoch: 5534, Loss: 0.2388
Epoch: 5535, Loss: 0.2396
Epoch: 5536, Loss: 0.2416
Epoch: 5537, Loss: 0.2409
Epoch: 5538, Loss: 0.2355
Epoch: 5539, Loss: 0.2368
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37675.99it/s]
Epoch: 5540, Loss: 0.2420, Train: 0.9213, Val: 0.9187, test: 0.9209
Epoch: 5541, Loss: 0.2426
Epoch: 5542, Loss: 0.2388
Epoch: 5543, Loss: 0.2421
Epoch: 5544, Loss: 0.2372
Epoch: 5545, Loss: 0.2389
Epoch: 5546, Loss: 0.2430
Epoch: 5547, Loss: 0.2395
Epoch: 5548, Loss: 0.2360
Epoch: 5549, Loss: 0.2362
Epoch: 5550, Loss: 0.2365
Epoch: 5551, Loss: 0.2399
Epoch: 5552, Loss: 0.2380
Epoch: 5553, Loss: 0.2377
Epoch: 5554, Loss: 0.2366
Epoch: 5555, Loss: 0.2373
Epoch: 5556, Loss: 0.2382
Epoch: 5557, Loss: 0.2385
Epoch: 5558, Loss: 0.2386
Epoch: 5559, Loss: 0.2410
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35920.52it/s]
Epoch: 5560, Loss: 0.2398, Train: 0.9218, Val: 0.9202, test: 0.9200
Epoch: 5561, Loss: 0.2383
Epoch: 5562, Loss: 0.2405
Epoch: 5563, Loss: 0.2400
Epoch: 5564, Loss: 0.2370
Epoch: 5565, Loss: 0.2359
Epoch: 5566, Loss: 0.2355
Epoch: 5567, Loss: 0.2354
Epoch: 5568, Loss: 0.2389
Epoch: 5569, Loss: 0.2377
Epoch: 5570, Loss: 0.2362
Epoch: 5571, Loss: 0.2324
Epoch: 5572, Loss: 0.2339
Epoch: 5573, Loss: 0.2370
Epoch: 5574, Loss: 0.2401
Epoch: 5575, Loss: 0.2366
Epoch: 5576, Loss: 0.2370
Epoch: 5577, Loss: 0.2365
Epoch: 5578, Loss: 0.2372
Epoch: 5579, Loss: 0.2385
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35868.47it/s]
Epoch: 5580, Loss: 0.2363, Train: 0.9227, Val: 0.9209, test: 0.9196
Epoch: 5581, Loss: 0.2365
Epoch: 5582, Loss: 0.2368
Epoch: 5583, Loss: 0.2357
Epoch: 5584, Loss: 0.2351
Epoch: 5585, Loss: 0.2347
Epoch: 5586, Loss: 0.2337
Epoch: 5587, Loss: 0.2370
Epoch: 5588, Loss: 0.2371
Epoch: 5589, Loss: 0.2353
Epoch: 5590, Loss: 0.2364
Epoch: 5591, Loss: 0.2348
Epoch: 5592, Loss: 0.2360
Epoch: 5593, Loss: 0.2344
Epoch: 5594, Loss: 0.2372
Epoch: 5595, Loss: 0.2357
Epoch: 5596, Loss: 0.2514
Epoch: 5597, Loss: 0.2338
Epoch: 5598, Loss: 0.2352
Epoch: 5599, Loss: 0.2368
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36513.50it/s]
Epoch: 5600, Loss: 0.2348, Train: 0.9220, Val: 0.9202, test: 0.9203
Epoch: 5601, Loss: 0.2349
Epoch: 5602, Loss: 0.2316
Epoch: 5603, Loss: 0.2343
Epoch: 5604, Loss: 0.2357
Epoch: 5605, Loss: 0.2345
Epoch: 5606, Loss: 0.2339
Epoch: 5607, Loss: 0.2372
Epoch: 5608, Loss: 0.2348
Epoch: 5609, Loss: 0.2327
Epoch: 5610, Loss: 0.2374
Epoch: 5611, Loss: 0.2384
Epoch: 5612, Loss: 0.2350
Epoch: 5613, Loss: 0.2411
Epoch: 5614, Loss: 0.2357
Epoch: 5615, Loss: 0.2363
Epoch: 5616, Loss: 0.2344
Epoch: 5617, Loss: 0.2347
Epoch: 5618, Loss: 0.2345
Epoch: 5619, Loss: 0.2338
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37519.06it/s]
Epoch: 5620, Loss: 0.2387, Train: 0.9224, Val: 0.9202, test: 0.9190
Epoch: 5621, Loss: 0.2401
Epoch: 5622, Loss: 0.2327
Epoch: 5623, Loss: 0.2344
Epoch: 5624, Loss: 0.2364
Epoch: 5625, Loss: 0.2349
Epoch: 5626, Loss: 0.2366
Epoch: 5627, Loss: 0.2326
Epoch: 5628, Loss: 0.2325
Epoch: 5629, Loss: 0.2358
Epoch: 5630, Loss: 0.2328
Epoch: 5631, Loss: 0.2340
Epoch: 5632, Loss: 0.2334
Epoch: 5633, Loss: 0.2353
Epoch: 5634, Loss: 0.2351
Epoch: 5635, Loss: 0.2341
Epoch: 5636, Loss: 0.2357
Epoch: 5637, Loss: 0.2353
Epoch: 5638, Loss: 0.2335
Epoch: 5639, Loss: 0.2359
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29666.50it/s]
Epoch: 5640, Loss: 0.2405, Train: 0.9230, Val: 0.9199, test: 0.9215
Epoch: 5641, Loss: 0.2361
Epoch: 5642, Loss: 0.2354
Epoch: 5643, Loss: 0.2366
Epoch: 5644, Loss: 0.2385
Epoch: 5645, Loss: 0.2403
Epoch: 5646, Loss: 0.2344
Epoch: 5647, Loss: 0.2373
Epoch: 5648, Loss: 0.2339
Epoch: 5649, Loss: 0.2348
Epoch: 5650, Loss: 0.2363
Epoch: 5651, Loss: 0.2356
Epoch: 5652, Loss: 0.2365
Epoch: 5653, Loss: 0.2335
Epoch: 5654, Loss: 0.2358
Epoch: 5655, Loss: 0.2341
Epoch: 5656, Loss: 0.2356
Epoch: 5657, Loss: 0.2334
Epoch: 5658, Loss: 0.2375
Epoch: 5659, Loss: 0.2388
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29608.51it/s]
Epoch: 5660, Loss: 0.2330, Train: 0.9232, Val: 0.9215, test: 0.9209
Epoch: 5661, Loss: 0.2343
Epoch: 5662, Loss: 0.2334
Epoch: 5663, Loss: 0.2353
Epoch: 5664, Loss: 0.2338
Epoch: 5665, Loss: 0.2350
Epoch: 5666, Loss: 0.2348
Epoch: 5667, Loss: 0.2350
Epoch: 5668, Loss: 0.2345
Epoch: 5669, Loss: 0.2324
Epoch: 5670, Loss: 0.2360
Epoch: 5671, Loss: 0.2371
Epoch: 5672, Loss: 0.2327
Epoch: 5673, Loss: 0.2342
Epoch: 5674, Loss: 0.2334
Epoch: 5675, Loss: 0.2328
Epoch: 5676, Loss: 0.2329
Epoch: 5677, Loss: 0.2348
Epoch: 5678, Loss: 0.2332
Epoch: 5679, Loss: 0.2357
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36413.46it/s]
Epoch: 5680, Loss: 0.2327, Train: 0.9229, Val: 0.9202, test: 0.9212
Epoch: 5681, Loss: 0.2326
Epoch: 5682, Loss: 0.2341
Epoch: 5683, Loss: 0.2332
Epoch: 5684, Loss: 0.2361
Epoch: 5685, Loss: 0.2327
Epoch: 5686, Loss: 0.2333
Epoch: 5687, Loss: 0.2326
Epoch: 5688, Loss: 0.2329
Epoch: 5689, Loss: 0.2344
Epoch: 5690, Loss: 0.2327
Epoch: 5691, Loss: 0.2346
Epoch: 5692, Loss: 0.2345
Epoch: 5693, Loss: 0.2370
Epoch: 5694, Loss: 0.2326
Epoch: 5695, Loss: 0.2392
Epoch: 5696, Loss: 0.2339
Epoch: 5697, Loss: 0.2370
Epoch: 5698, Loss: 0.2379
Epoch: 5699, Loss: 0.2320
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37869.50it/s]
Epoch: 5700, Loss: 0.2333, Train: 0.9228, Val: 0.9224, test: 0.9209
Epoch: 5701, Loss: 0.2349
Epoch: 5702, Loss: 0.2327
Epoch: 5703, Loss: 0.2357
Epoch: 5704, Loss: 0.2322
Epoch: 5705, Loss: 0.2337
Epoch: 5706, Loss: 0.2347
Epoch: 5707, Loss: 0.2358
Epoch: 5708, Loss: 0.2381
Epoch: 5709, Loss: 0.2315
Epoch: 5710, Loss: 0.2337
Epoch: 5711, Loss: 0.2361
Epoch: 5712, Loss: 0.2477
Epoch: 5713, Loss: 0.2337
Epoch: 5714, Loss: 0.2353
Epoch: 5715, Loss: 0.2344
Epoch: 5716, Loss: 0.2326
Epoch: 5717, Loss: 0.2310
Epoch: 5718, Loss: 0.2345
Epoch: 5719, Loss: 0.2348
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35645.60it/s]
Epoch: 5720, Loss: 0.2378, Train: 0.9235, Val: 0.9218, test: 0.9215
Epoch: 5721, Loss: 0.2354
Epoch: 5722, Loss: 0.2311
Epoch: 5723, Loss: 0.2324
Epoch: 5724, Loss: 0.2313
Epoch: 5725, Loss: 0.2348
Epoch: 5726, Loss: 0.2322
Epoch: 5727, Loss: 0.2314
Epoch: 5728, Loss: 0.2321
Epoch: 5729, Loss: 0.2328
Epoch: 5730, Loss: 0.2342
Epoch: 5731, Loss: 0.2332
Epoch: 5732, Loss: 0.2313
Epoch: 5733, Loss: 0.2331
Epoch: 5734, Loss: 0.2334
Epoch: 5735, Loss: 0.2338
Epoch: 5736, Loss: 0.2353
Epoch: 5737, Loss: 0.2292
Epoch: 5738, Loss: 0.2300
Epoch: 5739, Loss: 0.2290
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35017.80it/s]
Epoch: 5740, Loss: 0.2316, Train: 0.9238, Val: 0.9212, test: 0.9203
Epoch: 5741, Loss: 0.2304
Epoch: 5742, Loss: 0.2320
Epoch: 5743, Loss: 0.2311
Epoch: 5744, Loss: 0.2332
Epoch: 5745, Loss: 0.2330
Epoch: 5746, Loss: 0.2308
Epoch: 5747, Loss: 0.2323
Epoch: 5748, Loss: 0.2311
Epoch: 5749, Loss: 0.2380
Epoch: 5750, Loss: 0.2323
Epoch: 5751, Loss: 0.2343
Epoch: 5752, Loss: 0.2315
Epoch: 5753, Loss: 0.2304
Epoch: 5754, Loss: 0.2339
Epoch: 5755, Loss: 0.2332
Epoch: 5756, Loss: 0.2349
Epoch: 5757, Loss: 0.2312
Epoch: 5758, Loss: 0.2354
Epoch: 5759, Loss: 0.2341
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36098.63it/s]
Epoch: 5760, Loss: 0.2357, Train: 0.9236, Val: 0.9205, test: 0.9212
Epoch: 5761, Loss: 0.2335
Epoch: 5762, Loss: 0.2327
Epoch: 5763, Loss: 0.2340
Epoch: 5764, Loss: 0.2320
Epoch: 5765, Loss: 0.2333
Epoch: 5766, Loss: 0.2306
Epoch: 5767, Loss: 0.2344
Epoch: 5768, Loss: 0.2328
Epoch: 5769, Loss: 0.2331
Epoch: 5770, Loss: 0.2338
Epoch: 5771, Loss: 0.2326
Epoch: 5772, Loss: 0.2326
Epoch: 5773, Loss: 0.2314
Epoch: 5774, Loss: 0.2339
Epoch: 5775, Loss: 0.2327
Epoch: 5776, Loss: 0.2338
Epoch: 5777, Loss: 0.2321
Epoch: 5778, Loss: 0.2321
Epoch: 5779, Loss: 0.2295
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36123.83it/s]
Epoch: 5780, Loss: 0.2326, Train: 0.9258, Val: 0.9230, test: 0.9234
Epoch: 5781, Loss: 0.2303
Epoch: 5782, Loss: 0.2294
Epoch: 5783, Loss: 0.2332
Epoch: 5784, Loss: 0.2309
Epoch: 5785, Loss: 0.2345
Epoch: 5786, Loss: 0.2325
Epoch: 5787, Loss: 0.2325
Epoch: 5788, Loss: 0.2330
Epoch: 5789, Loss: 0.2334
Epoch: 5790, Loss: 0.2330
Epoch: 5791, Loss: 0.2343
Epoch: 5792, Loss: 0.2390
Epoch: 5793, Loss: 0.2303
Epoch: 5794, Loss: 0.2338
Epoch: 5795, Loss: 0.2342
Epoch: 5796, Loss: 0.2311
Epoch: 5797, Loss: 0.2362
Epoch: 5798, Loss: 0.2345
Epoch: 5799, Loss: 0.2335
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36496.69it/s]
Epoch: 5800, Loss: 0.2312, Train: 0.9234, Val: 0.9230, test: 0.9196
Epoch: 5801, Loss: 0.2340
Epoch: 5802, Loss: 0.2329
Epoch: 5803, Loss: 0.2302
Epoch: 5804, Loss: 0.2313
Epoch: 5805, Loss: 0.2315
Epoch: 5806, Loss: 0.2327
Epoch: 5807, Loss: 0.2317
Epoch: 5808, Loss: 0.2327
Epoch: 5809, Loss: 0.2321
Epoch: 5810, Loss: 0.2351
Epoch: 5811, Loss: 0.2318
Epoch: 5812, Loss: 0.2324
Epoch: 5813, Loss: 0.2332
Epoch: 5814, Loss: 0.2289
Epoch: 5815, Loss: 0.2356
Epoch: 5816, Loss: 0.2335
Epoch: 5817, Loss: 0.2307
Epoch: 5818, Loss: 0.2333
Epoch: 5819, Loss: 0.2329
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29539.65it/s]
Epoch: 5820, Loss: 0.2318, Train: 0.9237, Val: 0.9230, test: 0.9203
Epoch: 5821, Loss: 0.2325
Epoch: 5822, Loss: 0.2295
Epoch: 5823, Loss: 0.2318
Epoch: 5824, Loss: 0.2313
Epoch: 5825, Loss: 0.2304
Epoch: 5826, Loss: 0.2339
Epoch: 5827, Loss: 0.2328
Epoch: 5828, Loss: 0.2348
Epoch: 5829, Loss: 0.2302
Epoch: 5830, Loss: 0.2335
Epoch: 5831, Loss: 0.2305
Epoch: 5832, Loss: 0.2318
Epoch: 5833, Loss: 0.2336
Epoch: 5834, Loss: 0.2354
Epoch: 5835, Loss: 0.2377
Epoch: 5836, Loss: 0.2293
Epoch: 5837, Loss: 0.2343
Epoch: 5838, Loss: 0.2348
Epoch: 5839, Loss: 0.2326
Evaluating: 100%|| 64484/64484 [00:02<00:00, 27444.56it/s]
Epoch: 5840, Loss: 0.2370, Train: 0.9255, Val: 0.9227, test: 0.9237
Epoch: 5841, Loss: 0.2324
Epoch: 5842, Loss: 0.2418
Epoch: 5843, Loss: 0.2312
Epoch: 5844, Loss: 0.2426
Epoch: 5845, Loss: 0.2310
Epoch: 5846, Loss: 0.2391
Epoch: 5847, Loss: 0.2345
Epoch: 5848, Loss: 0.2331
Epoch: 5849, Loss: 0.2347
Epoch: 5850, Loss: 0.2295
Epoch: 5851, Loss: 0.2311
Epoch: 5852, Loss: 0.2355
Epoch: 5853, Loss: 0.2335
Epoch: 5854, Loss: 0.2289
Epoch: 5855, Loss: 0.2340
Epoch: 5856, Loss: 0.2294
Epoch: 5857, Loss: 0.2275
Epoch: 5858, Loss: 0.2301
Epoch: 5859, Loss: 0.2328
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36615.65it/s]
Epoch: 5860, Loss: 0.2319, Train: 0.9245, Val: 0.9212, test: 0.9218
Epoch: 5861, Loss: 0.2287
Epoch: 5862, Loss: 0.2299
Epoch: 5863, Loss: 0.2300
Epoch: 5864, Loss: 0.2347
Epoch: 5865, Loss: 0.2311
Epoch: 5866, Loss: 0.2296
Epoch: 5867, Loss: 0.2306
Epoch: 5868, Loss: 0.2306
Epoch: 5869, Loss: 0.2302
Epoch: 5870, Loss: 0.2310
Epoch: 5871, Loss: 0.2339
Epoch: 5872, Loss: 0.2306
Epoch: 5873, Loss: 0.2289
Epoch: 5874, Loss: 0.2293
Epoch: 5875, Loss: 0.2313
Epoch: 5876, Loss: 0.2317
Epoch: 5877, Loss: 0.2337
Epoch: 5878, Loss: 0.2296
Epoch: 5879, Loss: 0.2366
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36144.57it/s]
Epoch: 5880, Loss: 0.2325, Train: 0.9257, Val: 0.9230, test: 0.9225
Epoch: 5881, Loss: 0.2349
Epoch: 5882, Loss: 0.2300
Epoch: 5883, Loss: 0.2328
Epoch: 5884, Loss: 0.2321
Epoch: 5885, Loss: 0.2338
Epoch: 5886, Loss: 0.2309
Epoch: 5887, Loss: 0.2326
Epoch: 5888, Loss: 0.2333
Epoch: 5889, Loss: 0.2332
Epoch: 5890, Loss: 0.2289
Epoch: 5891, Loss: 0.2289
Epoch: 5892, Loss: 0.2299
Epoch: 5893, Loss: 0.2308
Epoch: 5894, Loss: 0.2279
Epoch: 5895, Loss: 0.2335
Epoch: 5896, Loss: 0.2353
Epoch: 5897, Loss: 0.2317
Epoch: 5898, Loss: 0.2326
Epoch: 5899, Loss: 0.2353
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36592.91it/s]
Epoch: 5900, Loss: 0.2351, Train: 0.9248, Val: 0.9215, test: 0.9228
Epoch: 5901, Loss: 0.2356
Epoch: 5902, Loss: 0.2341
Epoch: 5903, Loss: 0.2346
Epoch: 5904, Loss: 0.2299
Epoch: 5905, Loss: 0.2306
Epoch: 5906, Loss: 0.2285
Epoch: 5907, Loss: 0.2317
Epoch: 5908, Loss: 0.2309
Epoch: 5909, Loss: 0.2308
Epoch: 5910, Loss: 0.2323
Epoch: 5911, Loss: 0.2340
Epoch: 5912, Loss: 0.2309
Epoch: 5913, Loss: 0.2296
Epoch: 5914, Loss: 0.2333
Epoch: 5915, Loss: 0.2327
Epoch: 5916, Loss: 0.2335
Epoch: 5917, Loss: 0.2286
Epoch: 5918, Loss: 0.2331
Epoch: 5919, Loss: 0.2349
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35953.73it/s]
Epoch: 5920, Loss: 0.2315, Train: 0.9243, Val: 0.9221, test: 0.9212
Epoch: 5921, Loss: 0.2325
Epoch: 5922, Loss: 0.2297
Epoch: 5923, Loss: 0.2325
Epoch: 5924, Loss: 0.2316
Epoch: 5925, Loss: 0.2325
Epoch: 5926, Loss: 0.2341
Epoch: 5927, Loss: 0.2324
Epoch: 5928, Loss: 0.2292
Epoch: 5929, Loss: 0.2329
Epoch: 5930, Loss: 0.2280
Epoch: 5931, Loss: 0.2313
Epoch: 5932, Loss: 0.2324
Epoch: 5933, Loss: 0.2302
Epoch: 5934, Loss: 0.2312
Epoch: 5935, Loss: 0.2287
Epoch: 5936, Loss: 0.2311
Epoch: 5937, Loss: 0.2316
Epoch: 5938, Loss: 0.2297
Epoch: 5939, Loss: 0.2332
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36164.79it/s]
Epoch: 5940, Loss: 0.2321, Train: 0.9236, Val: 0.9218, test: 0.9209
Epoch: 5941, Loss: 0.2319
Epoch: 5942, Loss: 0.2343
Epoch: 5943, Loss: 0.2325
Epoch: 5944, Loss: 0.2322
Epoch: 5945, Loss: 0.2340
Epoch: 5946, Loss: 0.2292
Epoch: 5947, Loss: 0.2314
Epoch: 5948, Loss: 0.2294
Epoch: 5949, Loss: 0.2312
Epoch: 5950, Loss: 0.2299
Epoch: 5951, Loss: 0.2419
Epoch: 5952, Loss: 0.2327
Epoch: 5953, Loss: 0.2275
Epoch: 5954, Loss: 0.2308
Epoch: 5955, Loss: 0.2313
Epoch: 5956, Loss: 0.2312
Epoch: 5957, Loss: 0.2306
Epoch: 5958, Loss: 0.2314
Epoch: 5959, Loss: 0.2279
Evaluating: 100%|| 64484/64484 [00:02<00:00, 31074.82it/s]
Epoch: 5960, Loss: 0.2288, Train: 0.9243, Val: 0.9218, test: 0.9212
Epoch: 5961, Loss: 0.2299
Epoch: 5962, Loss: 0.2301
Epoch: 5963, Loss: 0.2288
Epoch: 5964, Loss: 0.2286
Epoch: 5965, Loss: 0.2298
Epoch: 5966, Loss: 0.2312
Epoch: 5967, Loss: 0.2306
Epoch: 5968, Loss: 0.2308
Epoch: 5969, Loss: 0.2297
Epoch: 5970, Loss: 0.2291
Epoch: 5971, Loss: 0.2283
Epoch: 5972, Loss: 0.2295
Epoch: 5973, Loss: 0.2279
Epoch: 5974, Loss: 0.2268
Epoch: 5975, Loss: 0.2262
Epoch: 5976, Loss: 0.2346
Epoch: 5977, Loss: 0.2294
Epoch: 5978, Loss: 0.2287
Epoch: 5979, Loss: 0.2276
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36991.16it/s]
Epoch: 5980, Loss: 0.2299, Train: 0.9256, Val: 0.9240, test: 0.9231
Epoch: 5981, Loss: 0.2307
Epoch: 5982, Loss: 0.2264
Epoch: 5983, Loss: 0.2311
Epoch: 5984, Loss: 0.2301
Epoch: 5985, Loss: 0.2284
Epoch: 5986, Loss: 0.2316
Epoch: 5987, Loss: 0.2252
Epoch: 5988, Loss: 0.2282
Epoch: 5989, Loss: 0.2305
Epoch: 5990, Loss: 0.2278
Epoch: 5991, Loss: 0.2286
Epoch: 5992, Loss: 0.2290
Epoch: 5993, Loss: 0.2286
Epoch: 5994, Loss: 0.2309
Epoch: 5995, Loss: 0.2291
Epoch: 5996, Loss: 0.2274
Epoch: 5997, Loss: 0.2348
Epoch: 5998, Loss: 0.2339
Epoch: 5999, Loss: 0.2291
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37501.71it/s]
Epoch: 6000, Loss: 0.2308, Train: 0.9249, Val: 0.9243, test: 0.9225
Epoch: 6001, Loss: 0.2290
Epoch: 6002, Loss: 0.2295
Epoch: 6003, Loss: 0.2283
Epoch: 6004, Loss: 0.2323
Epoch: 6005, Loss: 0.2330
Epoch: 6006, Loss: 0.2312
Epoch: 6007, Loss: 0.2336
Epoch: 6008, Loss: 0.2291
Epoch: 6009, Loss: 0.2338
Epoch: 6010, Loss: 0.2381
Epoch: 6011, Loss: 0.2332
Epoch: 6012, Loss: 0.2294
Epoch: 6013, Loss: 0.2301
Epoch: 6014, Loss: 0.2308
Epoch: 6015, Loss: 0.2308
Epoch: 6016, Loss: 0.2313
Epoch: 6017, Loss: 0.2288
Epoch: 6018, Loss: 0.2284
Epoch: 6019, Loss: 0.2311
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37723.26it/s]
Epoch: 6020, Loss: 0.2263, Train: 0.9261, Val: 0.9233, test: 0.9231
Epoch: 6021, Loss: 0.2316
Epoch: 6022, Loss: 0.2309
Epoch: 6023, Loss: 0.2283
Epoch: 6024, Loss: 0.2283
Epoch: 6025, Loss: 0.2279
Epoch: 6026, Loss: 0.2297
Epoch: 6027, Loss: 0.2281
Epoch: 6028, Loss: 0.2287
Epoch: 6029, Loss: 0.2266
Epoch: 6030, Loss: 0.2266
Epoch: 6031, Loss: 0.2283
Epoch: 6032, Loss: 0.2251
Epoch: 6033, Loss: 0.2304
Epoch: 6034, Loss: 0.2287
Epoch: 6035, Loss: 0.2290
Epoch: 6036, Loss: 0.2331
Epoch: 6037, Loss: 0.2324
Epoch: 6038, Loss: 0.2290
Epoch: 6039, Loss: 0.2312
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35828.39it/s]
Epoch: 6040, Loss: 0.2301, Train: 0.9258, Val: 0.9215, test: 0.9240
Epoch: 6041, Loss: 0.2319
Epoch: 6042, Loss: 0.2301
Epoch: 6043, Loss: 0.2300
Epoch: 6044, Loss: 0.2348
Epoch: 6045, Loss: 0.2286
Epoch: 6046, Loss: 0.2311
Epoch: 6047, Loss: 0.2320
Epoch: 6048, Loss: 0.2310
Epoch: 6049, Loss: 0.2348
Epoch: 6050, Loss: 0.2325
Epoch: 6051, Loss: 0.2309
Epoch: 6052, Loss: 0.2294
Epoch: 6053, Loss: 0.2309
Epoch: 6054, Loss: 0.2320
Epoch: 6055, Loss: 0.2304
Epoch: 6056, Loss: 0.2312
Epoch: 6057, Loss: 0.2299
Epoch: 6058, Loss: 0.2342
Epoch: 6059, Loss: 0.2307
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37960.19it/s]
Epoch: 6060, Loss: 0.2282, Train: 0.9259, Val: 0.9246, test: 0.9212
Epoch: 6061, Loss: 0.2418
Epoch: 6062, Loss: 0.2286
Epoch: 6063, Loss: 0.2275
Epoch: 6064, Loss: 0.2295
Epoch: 6065, Loss: 0.2321
Epoch: 6066, Loss: 0.2355
Epoch: 6067, Loss: 0.2282
Epoch: 6068, Loss: 0.2320
Epoch: 6069, Loss: 0.2280
Epoch: 6070, Loss: 0.2279
Epoch: 6071, Loss: 0.2287
Epoch: 6072, Loss: 0.2275
Epoch: 6073, Loss: 0.2294
Epoch: 6074, Loss: 0.2343
Epoch: 6075, Loss: 0.2279
Epoch: 6076, Loss: 0.2280
Epoch: 6077, Loss: 0.2315
Epoch: 6078, Loss: 0.2280
Epoch: 6079, Loss: 0.2303
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35594.07it/s]
Epoch: 6080, Loss: 0.2304, Train: 0.9254, Val: 0.9221, test: 0.9215
Epoch: 6081, Loss: 0.2284
Epoch: 6082, Loss: 0.2317
Epoch: 6083, Loss: 0.2294
Epoch: 6084, Loss: 0.2275
Epoch: 6085, Loss: 0.2293
Epoch: 6086, Loss: 0.2306
Epoch: 6087, Loss: 0.2421
Epoch: 6088, Loss: 0.2291
Epoch: 6089, Loss: 0.2298
Epoch: 6090, Loss: 0.2264
Epoch: 6091, Loss: 0.2290
Epoch: 6092, Loss: 0.2278
Epoch: 6093, Loss: 0.2264
Epoch: 6094, Loss: 0.2297
Epoch: 6095, Loss: 0.2287
Epoch: 6096, Loss: 0.2289
Epoch: 6097, Loss: 0.2269
Epoch: 6098, Loss: 0.2264
Epoch: 6099, Loss: 0.2263
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35526.55it/s]
Epoch: 6100, Loss: 0.2287, Train: 0.9255, Val: 0.9240, test: 0.9218
Epoch: 6101, Loss: 0.2276
Epoch: 6102, Loss: 0.2299
Epoch: 6103, Loss: 0.2274
Epoch: 6104, Loss: 0.2274
Epoch: 6105, Loss: 0.2290
Epoch: 6106, Loss: 0.2292
Epoch: 6107, Loss: 0.2287
Epoch: 6108, Loss: 0.2263
Epoch: 6109, Loss: 0.2282
Epoch: 6110, Loss: 0.2270
Epoch: 6111, Loss: 0.2289
Epoch: 6112, Loss: 0.2319
Epoch: 6113, Loss: 0.2228
Epoch: 6114, Loss: 0.2284
Epoch: 6115, Loss: 0.2278
Epoch: 6116, Loss: 0.2301
Epoch: 6117, Loss: 0.2286
Epoch: 6118, Loss: 0.2283
Epoch: 6119, Loss: 0.2315
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36221.72it/s]
Epoch: 6120, Loss: 0.2289, Train: 0.9249, Val: 0.9236, test: 0.9212
Epoch: 6121, Loss: 0.2278
Epoch: 6122, Loss: 0.2299
Epoch: 6123, Loss: 0.2272
Epoch: 6124, Loss: 0.2263
Epoch: 6125, Loss: 0.2306
Epoch: 6126, Loss: 0.2320
Epoch: 6127, Loss: 0.2281
Epoch: 6128, Loss: 0.2256
Epoch: 6129, Loss: 0.2299
Epoch: 6130, Loss: 0.2279
Epoch: 6131, Loss: 0.2272
Epoch: 6132, Loss: 0.2297
Epoch: 6133, Loss: 0.2268
Epoch: 6134, Loss: 0.2298
Epoch: 6135, Loss: 0.2269
Epoch: 6136, Loss: 0.2295
Epoch: 6137, Loss: 0.2266
Epoch: 6138, Loss: 0.2293
Epoch: 6139, Loss: 0.2283
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37781.40it/s]
Epoch: 6140, Loss: 0.2276, Train: 0.9241, Val: 0.9218, test: 0.9218
Epoch: 6141, Loss: 0.2268
Epoch: 6142, Loss: 0.2245
Epoch: 6143, Loss: 0.2292
Epoch: 6144, Loss: 0.2273
Epoch: 6145, Loss: 0.2261
Epoch: 6146, Loss: 0.2281
Epoch: 6147, Loss: 0.2312
Epoch: 6148, Loss: 0.2265
Epoch: 6149, Loss: 0.2278
Epoch: 6150, Loss: 0.2284
Epoch: 6151, Loss: 0.2325
Epoch: 6152, Loss: 0.2260
Epoch: 6153, Loss: 0.2246
Epoch: 6154, Loss: 0.2253
Epoch: 6155, Loss: 0.2282
Epoch: 6156, Loss: 0.2298
Epoch: 6157, Loss: 0.2273
Epoch: 6158, Loss: 0.2273
Epoch: 6159, Loss: 0.2285
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35872.93it/s]
Epoch: 6160, Loss: 0.2281, Train: 0.9242, Val: 0.9230, test: 0.9215
Epoch: 6161, Loss: 0.2250
Epoch: 6162, Loss: 0.2275
Epoch: 6163, Loss: 0.2277
Epoch: 6164, Loss: 0.2286
Epoch: 6165, Loss: 0.2273
Epoch: 6166, Loss: 0.2284
Epoch: 6167, Loss: 0.2263
Epoch: 6168, Loss: 0.2295
Epoch: 6169, Loss: 0.2251
Epoch: 6170, Loss: 0.2293
Epoch: 6171, Loss: 0.2273
Epoch: 6172, Loss: 0.2279
Epoch: 6173, Loss: 0.2270
Epoch: 6174, Loss: 0.2288
Epoch: 6175, Loss: 0.2287
Epoch: 6176, Loss: 0.2279
Epoch: 6177, Loss: 0.2268
Epoch: 6178, Loss: 0.2262
Epoch: 6179, Loss: 0.2258
Evaluating: 100%|| 64484/64484 [00:01<00:00, 33115.30it/s]
Epoch: 6180, Loss: 0.2274, Train: 0.9246, Val: 0.9246, test: 0.9225
Epoch: 6181, Loss: 0.2282
Epoch: 6182, Loss: 0.2296
Epoch: 6183, Loss: 0.2263
Epoch: 6184, Loss: 0.2289
Epoch: 6185, Loss: 0.2273
Epoch: 6186, Loss: 0.2285
Epoch: 6187, Loss: 0.2396
Epoch: 6188, Loss: 0.2283
Epoch: 6189, Loss: 0.2277
Epoch: 6190, Loss: 0.2298
Epoch: 6191, Loss: 0.2306
Epoch: 6192, Loss: 0.2279
Epoch: 6193, Loss: 0.2263
Epoch: 6194, Loss: 0.2279
Epoch: 6195, Loss: 0.2236
Epoch: 6196, Loss: 0.2279
Epoch: 6197, Loss: 0.2290
Epoch: 6198, Loss: 0.2270
Epoch: 6199, Loss: 0.2281
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35446.60it/s]
Epoch: 6200, Loss: 0.2274, Train: 0.9257, Val: 0.9236, test: 0.9206
Epoch: 6201, Loss: 0.2263
Epoch: 6202, Loss: 0.2278
Epoch: 6203, Loss: 0.2289
Epoch: 6204, Loss: 0.2266
Epoch: 6205, Loss: 0.2306
Epoch: 6206, Loss: 0.2298
Epoch: 6207, Loss: 0.2297
Epoch: 6208, Loss: 0.2295
Epoch: 6209, Loss: 0.2283
Epoch: 6210, Loss: 0.2310
Epoch: 6211, Loss: 0.2293
Epoch: 6212, Loss: 0.2322
Epoch: 6213, Loss: 0.2352
Epoch: 6214, Loss: 0.2296
Epoch: 6215, Loss: 0.2387
Epoch: 6216, Loss: 0.2437
Epoch: 6217, Loss: 0.2296
Epoch: 6218, Loss: 0.2522
Epoch: 6219, Loss: 0.2600
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36461.67it/s]
Epoch: 6220, Loss: 0.2479, Train: 0.9203, Val: 0.9227, test: 0.9153
Epoch: 6221, Loss: 0.2537
Epoch: 6222, Loss: 0.2315
Epoch: 6223, Loss: 0.2407
Epoch: 6224, Loss: 0.2306
Epoch: 6225, Loss: 0.2373
Epoch: 6226, Loss: 0.2414
Epoch: 6227, Loss: 0.2389
Epoch: 6228, Loss: 0.2356
Epoch: 6229, Loss: 0.2329
Epoch: 6230, Loss: 0.2342
Epoch: 6231, Loss: 0.2324
Epoch: 6232, Loss: 0.2384
Epoch: 6233, Loss: 0.2267
Epoch: 6234, Loss: 0.2340
Epoch: 6235, Loss: 0.2335
Epoch: 6236, Loss: 0.2331
Epoch: 6237, Loss: 0.2327
Epoch: 6238, Loss: 0.2344
Epoch: 6239, Loss: 0.2271
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36642.95it/s]
Epoch: 6240, Loss: 0.2328, Train: 0.9251, Val: 0.9243, test: 0.9234
Epoch: 6241, Loss: 0.2263
Epoch: 6242, Loss: 0.2314
Epoch: 6243, Loss: 0.2317
Epoch: 6244, Loss: 0.2301
Epoch: 6245, Loss: 0.2280
Epoch: 6246, Loss: 0.2303
Epoch: 6247, Loss: 0.2286
Epoch: 6248, Loss: 0.2290
Epoch: 6249, Loss: 0.2314
Epoch: 6250, Loss: 0.2293
Epoch: 6251, Loss: 0.2317
Epoch: 6252, Loss: 0.2288
Epoch: 6253, Loss: 0.2299
Epoch: 6254, Loss: 0.2295
Epoch: 6255, Loss: 0.2279
Epoch: 6256, Loss: 0.2298
Epoch: 6257, Loss: 0.2316
Epoch: 6258, Loss: 0.2267
Epoch: 6259, Loss: 0.2286
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35478.30it/s]
Epoch: 6260, Loss: 0.2299, Train: 0.9248, Val: 0.9227, test: 0.9234
Epoch: 6261, Loss: 0.2297
Epoch: 6262, Loss: 0.2305
Epoch: 6263, Loss: 0.2267
Epoch: 6264, Loss: 0.2293
Epoch: 6265, Loss: 0.2314
Epoch: 6266, Loss: 0.2259
Epoch: 6267, Loss: 0.2407
Epoch: 6268, Loss: 0.2288
Epoch: 6269, Loss: 0.2268
Epoch: 6270, Loss: 0.2274
Epoch: 6271, Loss: 0.2323
Epoch: 6272, Loss: 0.2325
Epoch: 6273, Loss: 0.2322
Epoch: 6274, Loss: 0.2316
Epoch: 6275, Loss: 0.2305
Epoch: 6276, Loss: 0.2273
Epoch: 6277, Loss: 0.2295
Epoch: 6278, Loss: 0.2323
Epoch: 6279, Loss: 0.2291
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29545.90it/s]
Epoch: 6280, Loss: 0.2277, Train: 0.9254, Val: 0.9233, test: 0.9237
Epoch: 6281, Loss: 0.2273
Epoch: 6282, Loss: 0.2242
Epoch: 6283, Loss: 0.2294
Epoch: 6284, Loss: 0.2313
Epoch: 6285, Loss: 0.2307
Epoch: 6286, Loss: 0.2286
Epoch: 6287, Loss: 0.2340
Epoch: 6288, Loss: 0.2293
Epoch: 6289, Loss: 0.2291
Epoch: 6290, Loss: 0.2275
Epoch: 6291, Loss: 0.2289
Epoch: 6292, Loss: 0.2314
Epoch: 6293, Loss: 0.2321
Epoch: 6294, Loss: 0.2267
Epoch: 6295, Loss: 0.2285
Epoch: 6296, Loss: 0.2303
Epoch: 6297, Loss: 0.2266
Epoch: 6298, Loss: 0.2308
Epoch: 6299, Loss: 0.2299
Evaluating: 100%|| 64484/64484 [00:02<00:00, 32019.01it/s]
Epoch: 6300, Loss: 0.2347, Train: 0.9252, Val: 0.9230, test: 0.9228
Epoch: 6301, Loss: 0.2286
Epoch: 6302, Loss: 0.2270
Epoch: 6303, Loss: 0.2311
Epoch: 6304, Loss: 0.2301
Epoch: 6305, Loss: 0.2295
Epoch: 6306, Loss: 0.2294
Epoch: 6307, Loss: 0.2296
Epoch: 6308, Loss: 0.2280
Epoch: 6309, Loss: 0.2283
Epoch: 6310, Loss: 0.2275
Epoch: 6311, Loss: 0.2272
Epoch: 6312, Loss: 0.2275
Epoch: 6313, Loss: 0.2268
Epoch: 6314, Loss: 0.2262
Epoch: 6315, Loss: 0.2302
Epoch: 6316, Loss: 0.2294
Epoch: 6317, Loss: 0.2246
Epoch: 6318, Loss: 0.2248
Epoch: 6319, Loss: 0.2342
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37717.50it/s]
Epoch: 6320, Loss: 0.2290, Train: 0.9261, Val: 0.9255, test: 0.9222
Epoch: 6321, Loss: 0.2250
Epoch: 6322, Loss: 0.2282
Epoch: 6323, Loss: 0.2288
Epoch: 6324, Loss: 0.2292
Epoch: 6325, Loss: 0.2310
Epoch: 6326, Loss: 0.2296
Epoch: 6327, Loss: 0.2273
Epoch: 6328, Loss: 0.2270
Epoch: 6329, Loss: 0.2293
Epoch: 6330, Loss: 0.2297
Epoch: 6331, Loss: 0.2281
Epoch: 6332, Loss: 0.2304
Epoch: 6333, Loss: 0.2290
Epoch: 6334, Loss: 0.2278
Epoch: 6335, Loss: 0.2318
Epoch: 6336, Loss: 0.2303
Epoch: 6337, Loss: 0.2293
Epoch: 6338, Loss: 0.2278
Epoch: 6339, Loss: 0.2281
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37781.52it/s]
Epoch: 6340, Loss: 0.2291, Train: 0.9255, Val: 0.9240, test: 0.9222
Epoch: 6341, Loss: 0.2290
Epoch: 6342, Loss: 0.2283
Epoch: 6343, Loss: 0.2284
Epoch: 6344, Loss: 0.2261
Epoch: 6345, Loss: 0.2234
Epoch: 6346, Loss: 0.2291
Epoch: 6347, Loss: 0.2339
Epoch: 6348, Loss: 0.2322
Epoch: 6349, Loss: 0.2324
Epoch: 6350, Loss: 0.2283
Epoch: 6351, Loss: 0.2282
Epoch: 6352, Loss: 0.2310
Epoch: 6353, Loss: 0.2310
Epoch: 6354, Loss: 0.2309
Epoch: 6355, Loss: 0.2289
Epoch: 6356, Loss: 0.2372
Epoch: 6357, Loss: 0.2247
Epoch: 6358, Loss: 0.2322
Epoch: 6359, Loss: 0.2310
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35366.61it/s]
Epoch: 6360, Loss: 0.2294, Train: 0.9251, Val: 0.9221, test: 0.9218
Epoch: 6361, Loss: 0.2291
Epoch: 6362, Loss: 0.2290
Epoch: 6363, Loss: 0.2274
Epoch: 6364, Loss: 0.2268
Epoch: 6365, Loss: 0.2279
Epoch: 6366, Loss: 0.2257
Epoch: 6367, Loss: 0.2285
Epoch: 6368, Loss: 0.2285
Epoch: 6369, Loss: 0.2303
Epoch: 6370, Loss: 0.2291
Epoch: 6371, Loss: 0.2269
Epoch: 6372, Loss: 0.2260
Epoch: 6373, Loss: 0.2270
Epoch: 6374, Loss: 0.2283
Epoch: 6375, Loss: 0.2288
Epoch: 6376, Loss: 0.2259
Epoch: 6377, Loss: 0.2276
Epoch: 6378, Loss: 0.2270
Epoch: 6379, Loss: 0.2248
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36422.43it/s]
Epoch: 6380, Loss: 0.2297, Train: 0.9270, Val: 0.9243, test: 0.9222
Epoch: 6381, Loss: 0.2227
Epoch: 6382, Loss: 0.2260
Epoch: 6383, Loss: 0.2253
Epoch: 6384, Loss: 0.2230
Epoch: 6385, Loss: 0.2289
Epoch: 6386, Loss: 0.2239
Epoch: 6387, Loss: 0.2266
Epoch: 6388, Loss: 0.2331
Epoch: 6389, Loss: 0.2301
Epoch: 6390, Loss: 0.2246
Epoch: 6391, Loss: 0.2293
Epoch: 6392, Loss: 0.2286
Epoch: 6393, Loss: 0.2273
Epoch: 6394, Loss: 0.2270
Epoch: 6395, Loss: 0.2259
Epoch: 6396, Loss: 0.2245
Epoch: 6397, Loss: 0.2258
Epoch: 6398, Loss: 0.2281
Epoch: 6399, Loss: 0.2266
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36817.90it/s]
Epoch: 6400, Loss: 0.2250, Train: 0.9257, Val: 0.9249, test: 0.9231
Epoch: 6401, Loss: 0.2251
Epoch: 6402, Loss: 0.2246
Epoch: 6403, Loss: 0.2250
Epoch: 6404, Loss: 0.2275
Epoch: 6405, Loss: 0.2267
Epoch: 6406, Loss: 0.2271
Epoch: 6407, Loss: 0.2241
Epoch: 6408, Loss: 0.2252
Epoch: 6409, Loss: 0.2309
Epoch: 6410, Loss: 0.2240
Epoch: 6411, Loss: 0.2356
Epoch: 6412, Loss: 0.2260
Epoch: 6413, Loss: 0.2256
Epoch: 6414, Loss: 0.2319
Epoch: 6415, Loss: 0.2274
Epoch: 6416, Loss: 0.2307
Epoch: 6417, Loss: 0.2301
Epoch: 6418, Loss: 0.2293
Epoch: 6419, Loss: 0.2309
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30272.04it/s]
Epoch: 6420, Loss: 0.2284, Train: 0.9259, Val: 0.9255, test: 0.9234
Epoch: 6421, Loss: 0.2281
Epoch: 6422, Loss: 0.2269
Epoch: 6423, Loss: 0.2287
Epoch: 6424, Loss: 0.2284
Epoch: 6425, Loss: 0.2288
Epoch: 6426, Loss: 0.2302
Epoch: 6427, Loss: 0.2268
Epoch: 6428, Loss: 0.2266
Epoch: 6429, Loss: 0.2295
Epoch: 6430, Loss: 0.2293
Epoch: 6431, Loss: 0.2292
Epoch: 6432, Loss: 0.2284
Epoch: 6433, Loss: 0.2310
Epoch: 6434, Loss: 0.2246
Epoch: 6435, Loss: 0.2272
Epoch: 6436, Loss: 0.2277
Epoch: 6437, Loss: 0.2290
Epoch: 6438, Loss: 0.2272
Epoch: 6439, Loss: 0.2271
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36494.79it/s]
Epoch: 6440, Loss: 0.2359, Train: 0.9254, Val: 0.9240, test: 0.9237
Epoch: 6441, Loss: 0.2280
Epoch: 6442, Loss: 0.2288
Epoch: 6443, Loss: 0.2281
Epoch: 6444, Loss: 0.2249
Epoch: 6445, Loss: 0.2282
Epoch: 6446, Loss: 0.2290
Epoch: 6447, Loss: 0.2234
Epoch: 6448, Loss: 0.2301
Epoch: 6449, Loss: 0.2251
Epoch: 6450, Loss: 0.2264
Epoch: 6451, Loss: 0.2264
Epoch: 6452, Loss: 0.2274
Epoch: 6453, Loss: 0.2283
Epoch: 6454, Loss: 0.2259
Epoch: 6455, Loss: 0.2268
Epoch: 6456, Loss: 0.2291
Epoch: 6457, Loss: 0.2301
Epoch: 6458, Loss: 0.2282
Epoch: 6459, Loss: 0.2272
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37917.36it/s]
Epoch: 6460, Loss: 0.2260, Train: 0.9246, Val: 0.9227, test: 0.9222
Epoch: 6461, Loss: 0.2275
Epoch: 6462, Loss: 0.2309
Epoch: 6463, Loss: 0.2290
Epoch: 6464, Loss: 0.2282
Epoch: 6465, Loss: 0.2282
Epoch: 6466, Loss: 0.2327
Epoch: 6467, Loss: 0.2282
Epoch: 6468, Loss: 0.2278
Epoch: 6469, Loss: 0.2275
Epoch: 6470, Loss: 0.2292
Epoch: 6471, Loss: 0.2262
Epoch: 6472, Loss: 0.2261
Epoch: 6473, Loss: 0.2272
Epoch: 6474, Loss: 0.2263
Epoch: 6475, Loss: 0.2265
Epoch: 6476, Loss: 0.2257
Epoch: 6477, Loss: 0.2251
Epoch: 6478, Loss: 0.2240
Epoch: 6479, Loss: 0.2262
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37648.41it/s]
Epoch: 6480, Loss: 0.2269, Train: 0.9249, Val: 0.9224, test: 0.9218
Epoch: 6481, Loss: 0.2268
Epoch: 6482, Loss: 0.2224
Epoch: 6483, Loss: 0.2269
Epoch: 6484, Loss: 0.2253
Epoch: 6485, Loss: 0.2261
Epoch: 6486, Loss: 0.2249
Epoch: 6487, Loss: 0.2279
Epoch: 6488, Loss: 0.2267
Epoch: 6489, Loss: 0.2302
Epoch: 6490, Loss: 0.2258
Epoch: 6491, Loss: 0.2277
Epoch: 6492, Loss: 0.2266
Epoch: 6493, Loss: 0.2262
Epoch: 6494, Loss: 0.2274
Epoch: 6495, Loss: 0.2264
Epoch: 6496, Loss: 0.2263
Epoch: 6497, Loss: 0.2265
Epoch: 6498, Loss: 0.2255
Epoch: 6499, Loss: 0.2286
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37631.89it/s]
Epoch: 6500, Loss: 0.2261, Train: 0.9248, Val: 0.9224, test: 0.9222
Epoch: 6501, Loss: 0.2256
Epoch: 6502, Loss: 0.2255
Epoch: 6503, Loss: 0.2250
Epoch: 6504, Loss: 0.2263
Epoch: 6505, Loss: 0.2232
Epoch: 6506, Loss: 0.2262
Epoch: 6507, Loss: 0.2256
Epoch: 6508, Loss: 0.2261
Epoch: 6509, Loss: 0.2238
Epoch: 6510, Loss: 0.2252
Epoch: 6511, Loss: 0.2224
Epoch: 6512, Loss: 0.2293
Epoch: 6513, Loss: 0.2255
Epoch: 6514, Loss: 0.2217
Epoch: 6515, Loss: 0.2228
Epoch: 6516, Loss: 0.2230
Epoch: 6517, Loss: 0.2249
Epoch: 6518, Loss: 0.2292
Epoch: 6519, Loss: 0.2257
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30037.25it/s]
Epoch: 6520, Loss: 0.2228, Train: 0.9269, Val: 0.9261, test: 0.9218
Epoch: 6521, Loss: 0.2244
Epoch: 6522, Loss: 0.2271
Epoch: 6523, Loss: 0.2265
Epoch: 6524, Loss: 0.2235
Epoch: 6525, Loss: 0.2305
Epoch: 6526, Loss: 0.2259
Epoch: 6527, Loss: 0.2280
Epoch: 6528, Loss: 0.2239
Epoch: 6529, Loss: 0.2256
Epoch: 6530, Loss: 0.2247
Epoch: 6531, Loss: 0.2253
Epoch: 6532, Loss: 0.2265
Epoch: 6533, Loss: 0.2237
Epoch: 6534, Loss: 0.2243
Epoch: 6535, Loss: 0.2310
Epoch: 6536, Loss: 0.2259
Epoch: 6537, Loss: 0.2259
Epoch: 6538, Loss: 0.2236
Epoch: 6539, Loss: 0.2242
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37318.58it/s]
Epoch: 6540, Loss: 0.2272, Train: 0.9254, Val: 0.9233, test: 0.9222
Epoch: 6541, Loss: 0.2266
Epoch: 6542, Loss: 0.2270
Epoch: 6543, Loss: 0.2260
Epoch: 6544, Loss: 0.2542
Epoch: 6545, Loss: 0.2412
Epoch: 6546, Loss: 0.2294
Epoch: 6547, Loss: 0.2389
Epoch: 6548, Loss: 0.2379
Epoch: 6549, Loss: 0.2346
Epoch: 6550, Loss: 0.2356
Epoch: 6551, Loss: 0.2353
Epoch: 6552, Loss: 0.2362
Epoch: 6553, Loss: 0.2371
Epoch: 6554, Loss: 0.2347
Epoch: 6555, Loss: 0.2353
Epoch: 6556, Loss: 0.2349
Epoch: 6557, Loss: 0.2328
Epoch: 6558, Loss: 0.2354
Epoch: 6559, Loss: 0.2333
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36736.17it/s]
Epoch: 6560, Loss: 0.2273, Train: 0.9247, Val: 0.9249, test: 0.9215
Epoch: 6561, Loss: 0.2284
Epoch: 6562, Loss: 0.2469
Epoch: 6563, Loss: 0.2302
Epoch: 6564, Loss: 0.2308
Epoch: 6565, Loss: 0.2271
Epoch: 6566, Loss: 0.2341
Epoch: 6567, Loss: 0.2272
Epoch: 6568, Loss: 0.2301
Epoch: 6569, Loss: 0.2281
Epoch: 6570, Loss: 0.2299
Epoch: 6571, Loss: 0.2305
Epoch: 6572, Loss: 0.2291
Epoch: 6573, Loss: 0.2289
Epoch: 6574, Loss: 0.2258
Epoch: 6575, Loss: 0.2283
Epoch: 6576, Loss: 0.2301
Epoch: 6577, Loss: 0.2315
Epoch: 6578, Loss: 0.2310
Epoch: 6579, Loss: 0.2274
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38007.80it/s]
Epoch: 6580, Loss: 0.2287, Train: 0.9256, Val: 0.9243, test: 0.9218
Epoch: 6581, Loss: 0.2271
Epoch: 6582, Loss: 0.2275
Epoch: 6583, Loss: 0.2230
Epoch: 6584, Loss: 0.2307
Epoch: 6585, Loss: 0.2243
Epoch: 6586, Loss: 0.2261
Epoch: 6587, Loss: 0.2274
Epoch: 6588, Loss: 0.2307
Epoch: 6589, Loss: 0.2291
Epoch: 6590, Loss: 0.2269
Epoch: 6591, Loss: 0.2246
Epoch: 6592, Loss: 0.2288
Epoch: 6593, Loss: 0.2256
Epoch: 6594, Loss: 0.2283
Epoch: 6595, Loss: 0.2281
Epoch: 6596, Loss: 0.2267
Epoch: 6597, Loss: 0.2316
Epoch: 6598, Loss: 0.2261
Epoch: 6599, Loss: 0.2309
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37920.55it/s]
Epoch: 6600, Loss: 0.2317, Train: 0.9275, Val: 0.9277, test: 0.9234
Epoch: 6601, Loss: 0.2245
Epoch: 6602, Loss: 0.2266
Epoch: 6603, Loss: 0.2255
Epoch: 6604, Loss: 0.2242
Epoch: 6605, Loss: 0.2212
Epoch: 6606, Loss: 0.2265
Epoch: 6607, Loss: 0.2301
Epoch: 6608, Loss: 0.2254
Epoch: 6609, Loss: 0.2262
Epoch: 6610, Loss: 0.2255
Epoch: 6611, Loss: 0.2261
Epoch: 6612, Loss: 0.2242
Epoch: 6613, Loss: 0.2266
Epoch: 6614, Loss: 0.2290
Epoch: 6615, Loss: 0.2254
Epoch: 6616, Loss: 0.2269
Epoch: 6617, Loss: 0.2251
Epoch: 6618, Loss: 0.2278
Epoch: 6619, Loss: 0.2266
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30031.92it/s]
Epoch: 6620, Loss: 0.2283, Train: 0.9256, Val: 0.9240, test: 0.9237
Epoch: 6621, Loss: 0.2293
Epoch: 6622, Loss: 0.2280
Epoch: 6623, Loss: 0.2273
Epoch: 6624, Loss: 0.2296
Epoch: 6625, Loss: 0.2222
Epoch: 6626, Loss: 0.2280
Epoch: 6627, Loss: 0.2290
Epoch: 6628, Loss: 0.2277
Epoch: 6629, Loss: 0.2257
Epoch: 6630, Loss: 0.2259
Epoch: 6631, Loss: 0.2263
Epoch: 6632, Loss: 0.2281
Epoch: 6633, Loss: 0.2239
Epoch: 6634, Loss: 0.2285
Epoch: 6635, Loss: 0.2311
Epoch: 6636, Loss: 0.2356
Epoch: 6637, Loss: 0.2397
Epoch: 6638, Loss: 0.2338
Epoch: 6639, Loss: 0.2365
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36304.44it/s]
Epoch: 6640, Loss: 0.2370, Train: 0.9255, Val: 0.9258, test: 0.9228
Epoch: 6641, Loss: 0.2338
Epoch: 6642, Loss: 0.2424
Epoch: 6643, Loss: 0.2442
Epoch: 6644, Loss: 0.2381
Epoch: 6645, Loss: 0.2329
Epoch: 6646, Loss: 0.2379
Epoch: 6647, Loss: 0.2347
Epoch: 6648, Loss: 0.2330
Epoch: 6649, Loss: 0.2283
Epoch: 6650, Loss: 0.2263
Epoch: 6651, Loss: 0.2290
Epoch: 6652, Loss: 0.2309
Epoch: 6653, Loss: 0.2264
Epoch: 6654, Loss: 0.2264
Epoch: 6655, Loss: 0.2319
Epoch: 6656, Loss: 0.2259
Epoch: 6657, Loss: 0.2283
Epoch: 6658, Loss: 0.2309
Epoch: 6659, Loss: 0.2287
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36678.99it/s]
Epoch: 6660, Loss: 0.2245, Train: 0.9277, Val: 0.9261, test: 0.9247
Epoch: 6661, Loss: 0.2251
Epoch: 6662, Loss: 0.2270
Epoch: 6663, Loss: 0.2233
Epoch: 6664, Loss: 0.2271
Epoch: 6665, Loss: 0.2252
Epoch: 6666, Loss: 0.2272
Epoch: 6667, Loss: 0.2264
Epoch: 6668, Loss: 0.2277
Epoch: 6669, Loss: 0.2281
Epoch: 6670, Loss: 0.2279
Epoch: 6671, Loss: 0.2250
Epoch: 6672, Loss: 0.2267
Epoch: 6673, Loss: 0.2252
Epoch: 6674, Loss: 0.2256
Epoch: 6675, Loss: 0.2262
Epoch: 6676, Loss: 0.2255
Epoch: 6677, Loss: 0.2273
Epoch: 6678, Loss: 0.2278
Epoch: 6679, Loss: 0.2287
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36823.38it/s]
Epoch: 6680, Loss: 0.2294, Train: 0.9256, Val: 0.9240, test: 0.9240
Epoch: 6681, Loss: 0.2326
Epoch: 6682, Loss: 0.2263
Epoch: 6683, Loss: 0.2289
Epoch: 6684, Loss: 0.2278
Epoch: 6685, Loss: 0.2270
Epoch: 6686, Loss: 0.2268
Epoch: 6687, Loss: 0.2253
Epoch: 6688, Loss: 0.2294
Epoch: 6689, Loss: 0.2306
Epoch: 6690, Loss: 0.2274
Epoch: 6691, Loss: 0.2280
Epoch: 6692, Loss: 0.2315
Epoch: 6693, Loss: 0.2238
Epoch: 6694, Loss: 0.2304
Epoch: 6695, Loss: 0.2289
Epoch: 6696, Loss: 0.2267
Epoch: 6697, Loss: 0.2306
Epoch: 6698, Loss: 0.2319
Epoch: 6699, Loss: 0.2269
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35684.94it/s]
Epoch: 6700, Loss: 0.2322, Train: 0.9261, Val: 0.9240, test: 0.9231
Epoch: 6701, Loss: 0.2338
Epoch: 6702, Loss: 0.2302
Epoch: 6703, Loss: 0.2401
Epoch: 6704, Loss: 0.2307
Epoch: 6705, Loss: 0.2299
Epoch: 6706, Loss: 0.2337
Epoch: 6707, Loss: 0.2243
Epoch: 6708, Loss: 0.2319
Epoch: 6709, Loss: 0.2290
Epoch: 6710, Loss: 0.2307
Epoch: 6711, Loss: 0.2340
Epoch: 6712, Loss: 0.2307
Epoch: 6713, Loss: 0.2340
Epoch: 6714, Loss: 0.2292
Epoch: 6715, Loss: 0.2288
Epoch: 6716, Loss: 0.2316
Epoch: 6717, Loss: 0.2255
Epoch: 6718, Loss: 0.2301
Epoch: 6719, Loss: 0.2253
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36514.29it/s]
Epoch: 6720, Loss: 0.2304, Train: 0.9272, Val: 0.9261, test: 0.9269
Epoch: 6721, Loss: 0.2250
Epoch: 6722, Loss: 0.2253
Epoch: 6723, Loss: 0.2269
Epoch: 6724, Loss: 0.2279
Epoch: 6725, Loss: 0.2278
Epoch: 6726, Loss: 0.2260
Epoch: 6727, Loss: 0.2256
Epoch: 6728, Loss: 0.2232
Epoch: 6729, Loss: 0.2277
Epoch: 6730, Loss: 0.2281
Epoch: 6731, Loss: 0.2249
Epoch: 6732, Loss: 0.2237
Epoch: 6733, Loss: 0.2265
Epoch: 6734, Loss: 0.2253
Epoch: 6735, Loss: 0.2248
Epoch: 6736, Loss: 0.2260
Epoch: 6737, Loss: 0.2233
Epoch: 6738, Loss: 0.2226
Epoch: 6739, Loss: 0.2236
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30330.52it/s]
Epoch: 6740, Loss: 0.2246, Train: 0.9273, Val: 0.9261, test: 0.9253
Epoch: 6741, Loss: 0.2242
Epoch: 6742, Loss: 0.2260
Epoch: 6743, Loss: 0.2259
Epoch: 6744, Loss: 0.2267
Epoch: 6745, Loss: 0.2254
Epoch: 6746, Loss: 0.2262
Epoch: 6747, Loss: 0.2265
Epoch: 6748, Loss: 0.2253
Epoch: 6749, Loss: 0.2232
Epoch: 6750, Loss: 0.2288
Epoch: 6751, Loss: 0.2242
Epoch: 6752, Loss: 0.2238
Epoch: 6753, Loss: 0.2247
Epoch: 6754, Loss: 0.2228
Epoch: 6755, Loss: 0.2253
Epoch: 6756, Loss: 0.2234
Epoch: 6757, Loss: 0.2235
Epoch: 6758, Loss: 0.2240
Epoch: 6759, Loss: 0.2238
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30368.15it/s]
Epoch: 6760, Loss: 0.2268, Train: 0.9279, Val: 0.9261, test: 0.9250
Epoch: 6761, Loss: 0.2245
Epoch: 6762, Loss: 0.2249
Epoch: 6763, Loss: 0.2231
Epoch: 6764, Loss: 0.2275
Epoch: 6765, Loss: 0.2203
Epoch: 6766, Loss: 0.2211
Epoch: 6767, Loss: 0.2270
Epoch: 6768, Loss: 0.2259
Epoch: 6769, Loss: 0.2270
Epoch: 6770, Loss: 0.2256
Epoch: 6771, Loss: 0.2264
Epoch: 6772, Loss: 0.2256
Epoch: 6773, Loss: 0.2251
Epoch: 6774, Loss: 0.2266
Epoch: 6775, Loss: 0.2255
Epoch: 6776, Loss: 0.2228
Epoch: 6777, Loss: 0.2242
Epoch: 6778, Loss: 0.2220
Epoch: 6779, Loss: 0.2257
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37670.52it/s]
Epoch: 6780, Loss: 0.2222, Train: 0.9261, Val: 0.9236, test: 0.9234
Epoch: 6781, Loss: 0.2237
Epoch: 6782, Loss: 0.2217
Epoch: 6783, Loss: 0.2261
Epoch: 6784, Loss: 0.2241
Epoch: 6785, Loss: 0.2208
Epoch: 6786, Loss: 0.2243
Epoch: 6787, Loss: 0.2228
Epoch: 6788, Loss: 0.2252
Epoch: 6789, Loss: 0.2223
Epoch: 6790, Loss: 0.2269
Epoch: 6791, Loss: 0.2219
Epoch: 6792, Loss: 0.2328
Epoch: 6793, Loss: 0.2229
Epoch: 6794, Loss: 0.2249
Epoch: 6795, Loss: 0.2216
Epoch: 6796, Loss: 0.2225
Epoch: 6797, Loss: 0.2261
Epoch: 6798, Loss: 0.2235
Epoch: 6799, Loss: 0.2240
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34719.47it/s]
Epoch: 6800, Loss: 0.2258, Train: 0.9259, Val: 0.9246, test: 0.9215
Epoch: 6801, Loss: 0.2240
Epoch: 6802, Loss: 0.2243
Epoch: 6803, Loss: 0.2238
Epoch: 6804, Loss: 0.2248
Epoch: 6805, Loss: 0.2253
Epoch: 6806, Loss: 0.2226
Epoch: 6807, Loss: 0.2250
Epoch: 6808, Loss: 0.2214
Epoch: 6809, Loss: 0.2222
Epoch: 6810, Loss: 0.2249
Epoch: 6811, Loss: 0.2234
Epoch: 6812, Loss: 0.2244
Epoch: 6813, Loss: 0.2230
Epoch: 6814, Loss: 0.2224
Epoch: 6815, Loss: 0.2244
Epoch: 6816, Loss: 0.2255
Epoch: 6817, Loss: 0.2267
Epoch: 6818, Loss: 0.2207
Epoch: 6819, Loss: 0.2246
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36521.98it/s]
Epoch: 6820, Loss: 0.2235, Train: 0.9272, Val: 0.9252, test: 0.9234
Epoch: 6821, Loss: 0.2237
Epoch: 6822, Loss: 0.2227
Epoch: 6823, Loss: 0.2213
Epoch: 6824, Loss: 0.2248
Epoch: 6825, Loss: 0.2213
Epoch: 6826, Loss: 0.2230
Epoch: 6827, Loss: 0.2229
Epoch: 6828, Loss: 0.2218
Epoch: 6829, Loss: 0.2223
Epoch: 6830, Loss: 0.2217
Epoch: 6831, Loss: 0.2265
Epoch: 6832, Loss: 0.2244
Epoch: 6833, Loss: 0.2234
Epoch: 6834, Loss: 0.2231
Epoch: 6835, Loss: 0.2246
Epoch: 6836, Loss: 0.2224
Epoch: 6837, Loss: 0.2199
Epoch: 6838, Loss: 0.2235
Epoch: 6839, Loss: 0.2226
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36974.02it/s]
Epoch: 6840, Loss: 0.2223, Train: 0.9260, Val: 0.9246, test: 0.9231
Epoch: 6841, Loss: 0.2256
Epoch: 6842, Loss: 0.2237
Epoch: 6843, Loss: 0.2247
Epoch: 6844, Loss: 0.2247
Epoch: 6845, Loss: 0.2253
Epoch: 6846, Loss: 0.2239
Epoch: 6847, Loss: 0.2238
Epoch: 6848, Loss: 0.2253
Epoch: 6849, Loss: 0.2240
Epoch: 6850, Loss: 0.2244
Epoch: 6851, Loss: 0.2256
Epoch: 6852, Loss: 0.2225
Epoch: 6853, Loss: 0.2245
Epoch: 6854, Loss: 0.2231
Epoch: 6855, Loss: 0.2251
Epoch: 6856, Loss: 0.2243
Epoch: 6857, Loss: 0.2234
Epoch: 6858, Loss: 0.2220
Epoch: 6859, Loss: 0.2218
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36963.56it/s]
Epoch: 6860, Loss: 0.2206, Train: 0.9278, Val: 0.9261, test: 0.9240
Epoch: 6861, Loss: 0.2249
Epoch: 6862, Loss: 0.2222
Epoch: 6863, Loss: 0.2234
Epoch: 6864, Loss: 0.2199
Epoch: 6865, Loss: 0.2211
Epoch: 6866, Loss: 0.2242
Epoch: 6867, Loss: 0.2222
Epoch: 6868, Loss: 0.2221
Epoch: 6869, Loss: 0.2215
Epoch: 6870, Loss: 0.2214
Epoch: 6871, Loss: 0.2219
Epoch: 6872, Loss: 0.2189
Epoch: 6873, Loss: 0.2189
Epoch: 6874, Loss: 0.2232
Epoch: 6875, Loss: 0.2222
Epoch: 6876, Loss: 0.2236
Epoch: 6877, Loss: 0.2232
Epoch: 6878, Loss: 0.2218
Epoch: 6879, Loss: 0.2218
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37862.00it/s]
Epoch: 6880, Loss: 0.2200, Train: 0.9273, Val: 0.9255, test: 0.9225
Epoch: 6881, Loss: 0.2203
Epoch: 6882, Loss: 0.2215
Epoch: 6883, Loss: 0.2229
Epoch: 6884, Loss: 0.2229
Epoch: 6885, Loss: 0.2219
Epoch: 6886, Loss: 0.2233
Epoch: 6887, Loss: 0.2222
Epoch: 6888, Loss: 0.2248
Epoch: 6889, Loss: 0.2212
Epoch: 6890, Loss: 0.2207
Epoch: 6891, Loss: 0.2208
Epoch: 6892, Loss: 0.2228
Epoch: 6893, Loss: 0.2219
Epoch: 6894, Loss: 0.2185
Epoch: 6895, Loss: 0.2214
Epoch: 6896, Loss: 0.2214
Epoch: 6897, Loss: 0.2230
Epoch: 6898, Loss: 0.2230
Epoch: 6899, Loss: 0.2247
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36253.10it/s]
Epoch: 6900, Loss: 0.2262, Train: 0.9263, Val: 0.9249, test: 0.9228
Epoch: 6901, Loss: 0.2230
Epoch: 6902, Loss: 0.2224
Epoch: 6903, Loss: 0.2221
Epoch: 6904, Loss: 0.2238
Epoch: 6905, Loss: 0.2249
Epoch: 6906, Loss: 0.2240
Epoch: 6907, Loss: 0.2210
Epoch: 6908, Loss: 0.2196
Epoch: 6909, Loss: 0.2231
Epoch: 6910, Loss: 0.2213
Epoch: 6911, Loss: 0.2222
Epoch: 6912, Loss: 0.2202
Epoch: 6913, Loss: 0.2222
Epoch: 6914, Loss: 0.2235
Epoch: 6915, Loss: 0.2217
Epoch: 6916, Loss: 0.2239
Epoch: 6917, Loss: 0.2246
Epoch: 6918, Loss: 0.2230
Epoch: 6919, Loss: 0.2229
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35433.29it/s]
Epoch: 6920, Loss: 0.2221, Train: 0.9269, Val: 0.9249, test: 0.9244
Epoch: 6921, Loss: 0.2241
Epoch: 6922, Loss: 0.2222
Epoch: 6923, Loss: 0.2240
Epoch: 6924, Loss: 0.2265
Epoch: 6925, Loss: 0.2221
Epoch: 6926, Loss: 0.2262
Epoch: 6927, Loss: 0.2233
Epoch: 6928, Loss: 0.2212
Epoch: 6929, Loss: 0.2288
Epoch: 6930, Loss: 0.2270
Epoch: 6931, Loss: 0.2200
Epoch: 6932, Loss: 0.2288
Epoch: 6933, Loss: 0.2317
Epoch: 6934, Loss: 0.2246
Epoch: 6935, Loss: 0.2327
Epoch: 6936, Loss: 0.2320
Epoch: 6937, Loss: 0.2308
Epoch: 6938, Loss: 0.2330
Epoch: 6939, Loss: 0.2283
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30324.97it/s]
Epoch: 6940, Loss: 0.2339, Train: 0.9240, Val: 0.9264, test: 0.9196
Epoch: 6941, Loss: 0.2308
Epoch: 6942, Loss: 0.2261
Epoch: 6943, Loss: 0.2269
Epoch: 6944, Loss: 0.2271
Epoch: 6945, Loss: 0.2252
Epoch: 6946, Loss: 0.2304
Epoch: 6947, Loss: 0.2296
Epoch: 6948, Loss: 0.2294
Epoch: 6949, Loss: 0.2279
Epoch: 6950, Loss: 0.2292
Epoch: 6951, Loss: 0.2251
Epoch: 6952, Loss: 0.2264
Epoch: 6953, Loss: 0.2256
Epoch: 6954, Loss: 0.2273
Epoch: 6955, Loss: 0.2253
Epoch: 6956, Loss: 0.2319
Epoch: 6957, Loss: 0.2286
Epoch: 6958, Loss: 0.2239
Epoch: 6959, Loss: 0.2239
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29697.33it/s]
Epoch: 6960, Loss: 0.2240, Train: 0.9249, Val: 0.9249, test: 0.9222
Epoch: 6961, Loss: 0.2253
Epoch: 6962, Loss: 0.2241
Epoch: 6963, Loss: 0.2251
Epoch: 6964, Loss: 0.2231
Epoch: 6965, Loss: 0.2246
Epoch: 6966, Loss: 0.2290
Epoch: 6967, Loss: 0.2264
Epoch: 6968, Loss: 0.2232
Epoch: 6969, Loss: 0.2241
Epoch: 6970, Loss: 0.2229
Epoch: 6971, Loss: 0.2278
Epoch: 6972, Loss: 0.2249
Epoch: 6973, Loss: 0.2239
Epoch: 6974, Loss: 0.2217
Epoch: 6975, Loss: 0.2211
Epoch: 6976, Loss: 0.2257
Epoch: 6977, Loss: 0.2232
Epoch: 6978, Loss: 0.2233
Epoch: 6979, Loss: 0.2271
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29823.25it/s]
Epoch: 6980, Loss: 0.2235, Train: 0.9248, Val: 0.9230, test: 0.9218
Epoch: 6981, Loss: 0.2239
Epoch: 6982, Loss: 0.2240
Epoch: 6983, Loss: 0.2247
Epoch: 6984, Loss: 0.2233
Epoch: 6985, Loss: 0.2226
Epoch: 6986, Loss: 0.2263
Epoch: 6987, Loss: 0.2237
Epoch: 6988, Loss: 0.2332
Epoch: 6989, Loss: 0.2381
Epoch: 6990, Loss: 0.2339
Epoch: 6991, Loss: 0.2305
Epoch: 6992, Loss: 0.2334
Epoch: 6993, Loss: 0.2267
Epoch: 6994, Loss: 0.2267
Epoch: 6995, Loss: 0.2293
Epoch: 6996, Loss: 0.2233
Epoch: 6997, Loss: 0.2267
Epoch: 6998, Loss: 0.2253
Epoch: 6999, Loss: 0.2239
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36637.35it/s]
Epoch: 7000, Loss: 0.2254, Train: 0.9261, Val: 0.9255, test: 0.9237
Epoch: 7001, Loss: 0.2256
Epoch: 7002, Loss: 0.2243
Epoch: 7003, Loss: 0.2260
Epoch: 7004, Loss: 0.2227
Epoch: 7005, Loss: 0.2240
Epoch: 7006, Loss: 0.2228
Epoch: 7007, Loss: 0.2226
Epoch: 7008, Loss: 0.2232
Epoch: 7009, Loss: 0.2207
Epoch: 7010, Loss: 0.2249
Epoch: 7011, Loss: 0.2200
Epoch: 7012, Loss: 0.2219
Epoch: 7013, Loss: 0.2240
Epoch: 7014, Loss: 0.2238
Epoch: 7015, Loss: 0.2214
Epoch: 7016, Loss: 0.2213
Epoch: 7017, Loss: 0.2221
Epoch: 7018, Loss: 0.2232
Epoch: 7019, Loss: 0.2217
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37329.94it/s]
Epoch: 7020, Loss: 0.2181, Train: 0.9285, Val: 0.9286, test: 0.9244
Epoch: 7021, Loss: 0.2222
Epoch: 7022, Loss: 0.2267
Epoch: 7023, Loss: 0.2195
Epoch: 7024, Loss: 0.2210
Epoch: 7025, Loss: 0.2220
Epoch: 7026, Loss: 0.2213
Epoch: 7027, Loss: 0.2243
Epoch: 7028, Loss: 0.2208
Epoch: 7029, Loss: 0.2285
Epoch: 7030, Loss: 0.2218
Epoch: 7031, Loss: 0.2240
Epoch: 7032, Loss: 0.2231
Epoch: 7033, Loss: 0.2260
Epoch: 7034, Loss: 0.2202
Epoch: 7035, Loss: 0.2285
Epoch: 7036, Loss: 0.2261
Epoch: 7037, Loss: 0.2254
Epoch: 7038, Loss: 0.2228
Epoch: 7039, Loss: 0.2235
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37527.85it/s]
Epoch: 7040, Loss: 0.2267, Train: 0.9261, Val: 0.9258, test: 0.9203
Epoch: 7041, Loss: 0.2226
Epoch: 7042, Loss: 0.2228
Epoch: 7043, Loss: 0.2233
Epoch: 7044, Loss: 0.2253
Epoch: 7045, Loss: 0.2269
Epoch: 7046, Loss: 0.2272
Epoch: 7047, Loss: 0.2280
Epoch: 7048, Loss: 0.2254
Epoch: 7049, Loss: 0.2226
Epoch: 7050, Loss: 0.2255
Epoch: 7051, Loss: 0.2243
Epoch: 7052, Loss: 0.2241
Epoch: 7053, Loss: 0.2230
Epoch: 7054, Loss: 0.2245
Epoch: 7055, Loss: 0.2278
Epoch: 7056, Loss: 0.2259
Epoch: 7057, Loss: 0.2209
Epoch: 7058, Loss: 0.2248
Epoch: 7059, Loss: 0.2283
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37783.00it/s]
Epoch: 7060, Loss: 0.2212, Train: 0.9254, Val: 0.9252, test: 0.9218
Epoch: 7061, Loss: 0.2278
Epoch: 7062, Loss: 0.2240
Epoch: 7063, Loss: 0.2234
Epoch: 7064, Loss: 0.2232
Epoch: 7065, Loss: 0.2217
Epoch: 7066, Loss: 0.2261
Epoch: 7067, Loss: 0.2247
Epoch: 7068, Loss: 0.2243
Epoch: 7069, Loss: 0.2241
Epoch: 7070, Loss: 0.2236
Epoch: 7071, Loss: 0.2226
Epoch: 7072, Loss: 0.2230
Epoch: 7073, Loss: 0.2262
Epoch: 7074, Loss: 0.2239
Epoch: 7075, Loss: 0.2228
Epoch: 7076, Loss: 0.2255
Epoch: 7077, Loss: 0.2227
Epoch: 7078, Loss: 0.2267
Epoch: 7079, Loss: 0.2240
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35631.05it/s]
Epoch: 7080, Loss: 0.2243, Train: 0.9257, Val: 0.9252, test: 0.9206
Epoch: 7081, Loss: 0.2234
Epoch: 7082, Loss: 0.2234
Epoch: 7083, Loss: 0.2229
Epoch: 7084, Loss: 0.2224
Epoch: 7085, Loss: 0.2219
Epoch: 7086, Loss: 0.2247
Epoch: 7087, Loss: 0.2286
Epoch: 7088, Loss: 0.2285
Epoch: 7089, Loss: 0.2290
Epoch: 7090, Loss: 0.2272
Epoch: 7091, Loss: 0.2265
Epoch: 7092, Loss: 0.2263
Epoch: 7093, Loss: 0.2251
Epoch: 7094, Loss: 0.2283
Epoch: 7095, Loss: 0.2267
Epoch: 7096, Loss: 0.2289
Epoch: 7097, Loss: 0.2265
Epoch: 7098, Loss: 0.2247
Epoch: 7099, Loss: 0.2273
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35877.86it/s]
Epoch: 7100, Loss: 0.2623, Train: 0.9252, Val: 0.9227, test: 0.9222
Epoch: 7101, Loss: 0.2310
Epoch: 7102, Loss: 0.2288
Epoch: 7103, Loss: 0.2264
Epoch: 7104, Loss: 0.2297
Epoch: 7105, Loss: 0.2325
Epoch: 7106, Loss: 0.2316
Epoch: 7107, Loss: 0.2320
Epoch: 7108, Loss: 0.2318
Epoch: 7109, Loss: 0.2288
Epoch: 7110, Loss: 0.2261
Epoch: 7111, Loss: 0.2319
Epoch: 7112, Loss: 0.2286
Epoch: 7113, Loss: 0.2336
Epoch: 7114, Loss: 0.2273
Epoch: 7115, Loss: 0.2260
Epoch: 7116, Loss: 0.2262
Epoch: 7117, Loss: 0.2278
Epoch: 7118, Loss: 0.2285
Epoch: 7119, Loss: 0.2252
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36610.18it/s]
Epoch: 7120, Loss: 0.2244, Train: 0.9258, Val: 0.9280, test: 0.9184
Epoch: 7121, Loss: 0.2282
Epoch: 7122, Loss: 0.2272
Epoch: 7123, Loss: 0.2268
Epoch: 7124, Loss: 0.2287
Epoch: 7125, Loss: 0.2269
Epoch: 7126, Loss: 0.2246
Epoch: 7127, Loss: 0.2262
Epoch: 7128, Loss: 0.2268
Epoch: 7129, Loss: 0.2224
Epoch: 7130, Loss: 0.2249
Epoch: 7131, Loss: 0.2270
Epoch: 7132, Loss: 0.2244
Epoch: 7133, Loss: 0.2302
Epoch: 7134, Loss: 0.2351
Epoch: 7135, Loss: 0.2292
Epoch: 7136, Loss: 0.2420
Epoch: 7137, Loss: 0.2453
Epoch: 7138, Loss: 0.2373
Epoch: 7139, Loss: 0.2291
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37723.95it/s]
Epoch: 7140, Loss: 0.2271, Train: 0.9244, Val: 0.9218, test: 0.9222
Epoch: 7141, Loss: 0.2344
Epoch: 7142, Loss: 0.2286
Epoch: 7143, Loss: 0.2376
Epoch: 7144, Loss: 0.2356
Epoch: 7145, Loss: 0.2309
Epoch: 7146, Loss: 0.2345
Epoch: 7147, Loss: 0.2271
Epoch: 7148, Loss: 0.2300
Epoch: 7149, Loss: 0.2323
Epoch: 7150, Loss: 0.2371
Epoch: 7151, Loss: 0.2295
Epoch: 7152, Loss: 0.2277
Epoch: 7153, Loss: 0.2267
Epoch: 7154, Loss: 0.2297
Epoch: 7155, Loss: 0.2261
Epoch: 7156, Loss: 0.2239
Epoch: 7157, Loss: 0.2249
Epoch: 7158, Loss: 0.2267
Epoch: 7159, Loss: 0.2243
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36259.25it/s]
Epoch: 7160, Loss: 0.2272, Train: 0.9273, Val: 0.9268, test: 0.9225
Epoch: 7161, Loss: 0.2259
Epoch: 7162, Loss: 0.2294
Epoch: 7163, Loss: 0.2287
Epoch: 7164, Loss: 0.2276
Epoch: 7165, Loss: 0.2245
Epoch: 7166, Loss: 0.2279
Epoch: 7167, Loss: 0.2244
Epoch: 7168, Loss: 0.2294
Epoch: 7169, Loss: 0.2251
Epoch: 7170, Loss: 0.2264
Epoch: 7171, Loss: 0.2262
Epoch: 7172, Loss: 0.2269
Epoch: 7173, Loss: 0.2263
Epoch: 7174, Loss: 0.2244
Epoch: 7175, Loss: 0.2246
Epoch: 7176, Loss: 0.2253
Epoch: 7177, Loss: 0.2246
Epoch: 7178, Loss: 0.2236
Epoch: 7179, Loss: 0.2251
Evaluating: 100%|| 64484/64484 [00:01<00:00, 32450.11it/s]
Epoch: 7180, Loss: 0.2250, Train: 0.9243, Val: 0.9246, test: 0.9203
Epoch: 7181, Loss: 0.2230
Epoch: 7182, Loss: 0.2247
Epoch: 7183, Loss: 0.2252
Epoch: 7184, Loss: 0.2257
Epoch: 7185, Loss: 0.2246
Epoch: 7186, Loss: 0.2265
Epoch: 7187, Loss: 0.2252
Epoch: 7188, Loss: 0.2247
Epoch: 7189, Loss: 0.2244
Epoch: 7190, Loss: 0.2271
Epoch: 7191, Loss: 0.2221
Epoch: 7192, Loss: 0.2226
Epoch: 7193, Loss: 0.2262
Epoch: 7194, Loss: 0.2241
Epoch: 7195, Loss: 0.2271
Epoch: 7196, Loss: 0.2257
Epoch: 7197, Loss: 0.2242
Epoch: 7198, Loss: 0.2222
Epoch: 7199, Loss: 0.2264
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29893.53it/s]
Epoch: 7200, Loss: 0.2239, Train: 0.9241, Val: 0.9246, test: 0.9209
Epoch: 7201, Loss: 0.2259
Epoch: 7202, Loss: 0.2250
Epoch: 7203, Loss: 0.2253
Epoch: 7204, Loss: 0.2260
Epoch: 7205, Loss: 0.2215
Epoch: 7206, Loss: 0.2213
Epoch: 7207, Loss: 0.2219
Epoch: 7208, Loss: 0.2234
Epoch: 7209, Loss: 0.2265
Epoch: 7210, Loss: 0.2247
Epoch: 7211, Loss: 0.2217
Epoch: 7212, Loss: 0.2224
Epoch: 7213, Loss: 0.2207
Epoch: 7214, Loss: 0.2213
Epoch: 7215, Loss: 0.2231
Epoch: 7216, Loss: 0.2214
Epoch: 7217, Loss: 0.2234
Epoch: 7218, Loss: 0.2199
Epoch: 7219, Loss: 0.2247
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30436.34it/s]
Epoch: 7220, Loss: 0.2256, Train: 0.9253, Val: 0.9240, test: 0.9237
Epoch: 7221, Loss: 0.2241
Epoch: 7222, Loss: 0.2227
Epoch: 7223, Loss: 0.2205
Epoch: 7224, Loss: 0.2236
Epoch: 7225, Loss: 0.2213
Epoch: 7226, Loss: 0.2248
Epoch: 7227, Loss: 0.2201
Epoch: 7228, Loss: 0.2180
Epoch: 7229, Loss: 0.2241
Epoch: 7230, Loss: 0.2245
Epoch: 7231, Loss: 0.2218
Epoch: 7232, Loss: 0.2215
Epoch: 7233, Loss: 0.2210
Epoch: 7234, Loss: 0.2260
Epoch: 7235, Loss: 0.2263
Epoch: 7236, Loss: 0.2236
Epoch: 7237, Loss: 0.2206
Epoch: 7238, Loss: 0.2264
Epoch: 7239, Loss: 0.2206
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30361.78it/s]
Epoch: 7240, Loss: 0.2235, Train: 0.9256, Val: 0.9243, test: 0.9234
Epoch: 7241, Loss: 0.2238
Epoch: 7242, Loss: 0.2228
Epoch: 7243, Loss: 0.2227
Epoch: 7244, Loss: 0.2192
Epoch: 7245, Loss: 0.2228
Epoch: 7246, Loss: 0.2246
Epoch: 7247, Loss: 0.2221
Epoch: 7248, Loss: 0.2239
Epoch: 7249, Loss: 0.2198
Epoch: 7250, Loss: 0.2281
Epoch: 7251, Loss: 0.2234
Epoch: 7252, Loss: 0.2254
Epoch: 7253, Loss: 0.2228
Epoch: 7254, Loss: 0.2249
Epoch: 7255, Loss: 0.2252
Epoch: 7256, Loss: 0.2251
Epoch: 7257, Loss: 0.2241
Epoch: 7258, Loss: 0.2261
Epoch: 7259, Loss: 0.2220
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29997.82it/s]
Epoch: 7260, Loss: 0.2215, Train: 0.9265, Val: 0.9249, test: 0.9234
Epoch: 7261, Loss: 0.2213
Epoch: 7262, Loss: 0.2189
Epoch: 7263, Loss: 0.2248
Epoch: 7264, Loss: 0.2212
Epoch: 7265, Loss: 0.2254
Epoch: 7266, Loss: 0.2212
Epoch: 7267, Loss: 0.2226
Epoch: 7268, Loss: 0.2223
Epoch: 7269, Loss: 0.2252
Epoch: 7270, Loss: 0.2209
Epoch: 7271, Loss: 0.2261
Epoch: 7272, Loss: 0.2213
Epoch: 7273, Loss: 0.2250
Epoch: 7274, Loss: 0.2244
Epoch: 7275, Loss: 0.2275
Epoch: 7276, Loss: 0.2258
Epoch: 7277, Loss: 0.2254
Epoch: 7278, Loss: 0.2234
Epoch: 7279, Loss: 0.2239
Evaluating: 100%|| 64484/64484 [00:02<00:00, 28796.94it/s]
Epoch: 7280, Loss: 0.2269, Train: 0.9250, Val: 0.9246, test: 0.9218
Epoch: 7281, Loss: 0.2266
Epoch: 7282, Loss: 0.2240
Epoch: 7283, Loss: 0.2229
Epoch: 7284, Loss: 0.2230
Epoch: 7285, Loss: 0.2249
Epoch: 7286, Loss: 0.2248
Epoch: 7287, Loss: 0.2232
Epoch: 7288, Loss: 0.2240
Epoch: 7289, Loss: 0.2229
Epoch: 7290, Loss: 0.2221
Epoch: 7291, Loss: 0.2222
Epoch: 7292, Loss: 0.2241
Epoch: 7293, Loss: 0.2229
Epoch: 7294, Loss: 0.2269
Epoch: 7295, Loss: 0.2229
Epoch: 7296, Loss: 0.2240
Epoch: 7297, Loss: 0.2250
Epoch: 7298, Loss: 0.2241
Epoch: 7299, Loss: 0.2205
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29296.34it/s]
Epoch: 7300, Loss: 0.2239, Train: 0.9255, Val: 0.9252, test: 0.9222
Epoch: 7301, Loss: 0.2251
Epoch: 7302, Loss: 0.2239
Epoch: 7303, Loss: 0.2211
Epoch: 7304, Loss: 0.2236
Epoch: 7305, Loss: 0.2223
Epoch: 7306, Loss: 0.2228
Epoch: 7307, Loss: 0.2208
Epoch: 7308, Loss: 0.2213
Epoch: 7309, Loss: 0.2197
Epoch: 7310, Loss: 0.2219
Epoch: 7311, Loss: 0.2206
Epoch: 7312, Loss: 0.2240
Epoch: 7313, Loss: 0.2205
Epoch: 7314, Loss: 0.2216
Epoch: 7315, Loss: 0.2221
Epoch: 7316, Loss: 0.2221
Epoch: 7317, Loss: 0.2228
Epoch: 7318, Loss: 0.2201
Epoch: 7319, Loss: 0.2216
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30189.44it/s]
Epoch: 7320, Loss: 0.2231, Train: 0.9267, Val: 0.9261, test: 0.9237
Epoch: 7321, Loss: 0.2194
Epoch: 7322, Loss: 0.2225
Epoch: 7323, Loss: 0.2267
Epoch: 7324, Loss: 0.2226
Epoch: 7325, Loss: 0.2246
Epoch: 7326, Loss: 0.2182
Epoch: 7327, Loss: 0.2236
Epoch: 7328, Loss: 0.2213
Epoch: 7329, Loss: 0.2197
Epoch: 7330, Loss: 0.2225
Epoch: 7331, Loss: 0.2254
Epoch: 7332, Loss: 0.2250
Epoch: 7333, Loss: 0.2185
Epoch: 7334, Loss: 0.2227
Epoch: 7335, Loss: 0.2196
Epoch: 7336, Loss: 0.2217
Epoch: 7337, Loss: 0.2212
Epoch: 7338, Loss: 0.2201
Epoch: 7339, Loss: 0.2214
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37332.73it/s]
Epoch: 7340, Loss: 0.2210, Train: 0.9254, Val: 0.9243, test: 0.9222
Epoch: 7341, Loss: 0.2236
Epoch: 7342, Loss: 0.2209
Epoch: 7343, Loss: 0.2241
Epoch: 7344, Loss: 0.2227
Epoch: 7345, Loss: 0.2232
Epoch: 7346, Loss: 0.2213
Epoch: 7347, Loss: 0.2228
Epoch: 7348, Loss: 0.2231
Epoch: 7349, Loss: 0.2240
Epoch: 7350, Loss: 0.2231
Epoch: 7351, Loss: 0.2242
Epoch: 7352, Loss: 0.2221
Epoch: 7353, Loss: 0.2218
Epoch: 7354, Loss: 0.2199
Epoch: 7355, Loss: 0.2208
Epoch: 7356, Loss: 0.2226
Epoch: 7357, Loss: 0.2218
Epoch: 7358, Loss: 0.2216
Epoch: 7359, Loss: 0.2218
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37456.45it/s]
Epoch: 7360, Loss: 0.2224, Train: 0.9254, Val: 0.9252, test: 0.9215
Epoch: 7361, Loss: 0.2231
Epoch: 7362, Loss: 0.2213
Epoch: 7363, Loss: 0.2197
Epoch: 7364, Loss: 0.2227
Epoch: 7365, Loss: 0.2201
Epoch: 7366, Loss: 0.2212
Epoch: 7367, Loss: 0.2234
Epoch: 7368, Loss: 0.2221
Epoch: 7369, Loss: 0.2223
Epoch: 7370, Loss: 0.2216
Epoch: 7371, Loss: 0.2217
Epoch: 7372, Loss: 0.2232
Epoch: 7373, Loss: 0.2244
Epoch: 7374, Loss: 0.2232
Epoch: 7375, Loss: 0.2206
Epoch: 7376, Loss: 0.2230
Epoch: 7377, Loss: 0.2204
Epoch: 7378, Loss: 0.2191
Epoch: 7379, Loss: 0.2199
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36042.27it/s]
Epoch: 7380, Loss: 0.2235, Train: 0.9259, Val: 0.9258, test: 0.9200
Epoch: 7381, Loss: 0.2220
Epoch: 7382, Loss: 0.2204
Epoch: 7383, Loss: 0.2235
Epoch: 7384, Loss: 0.2239
Epoch: 7385, Loss: 0.2222
Epoch: 7386, Loss: 0.2208
Epoch: 7387, Loss: 0.2204
Epoch: 7388, Loss: 0.2223
Epoch: 7389, Loss: 0.2236
Epoch: 7390, Loss: 0.2233
Epoch: 7391, Loss: 0.2236
Epoch: 7392, Loss: 0.2282
Epoch: 7393, Loss: 0.2230
Epoch: 7394, Loss: 0.2226
Epoch: 7395, Loss: 0.2233
Epoch: 7396, Loss: 0.2212
Epoch: 7397, Loss: 0.2281
Epoch: 7398, Loss: 0.2258
Epoch: 7399, Loss: 0.2216
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36780.57it/s]
Epoch: 7400, Loss: 0.2310, Train: 0.9259, Val: 0.9221, test: 0.9222
Epoch: 7401, Loss: 0.2315
Epoch: 7402, Loss: 0.2260
Epoch: 7403, Loss: 0.2482
Epoch: 7404, Loss: 0.2571
Epoch: 7405, Loss: 0.2628
Epoch: 7406, Loss: 0.2309
Epoch: 7407, Loss: 0.2452
Epoch: 7408, Loss: 0.2628
Epoch: 7409, Loss: 0.2734
Epoch: 7410, Loss: 0.2321
Epoch: 7411, Loss: 0.3266
Epoch: 7412, Loss: 0.2892
Epoch: 7413, Loss: 0.3868
Epoch: 7414, Loss: 0.3278
Epoch: 7415, Loss: 0.2418
Epoch: 7416, Loss: 0.3586
Epoch: 7417, Loss: 0.2512
Epoch: 7418, Loss: 0.2813
Epoch: 7419, Loss: 0.3018
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35419.71it/s]
Epoch: 7420, Loss: 0.2842, Train: 0.9239, Val: 0.9199, test: 0.9215
Epoch: 7421, Loss: 0.2550
Epoch: 7422, Loss: 0.2580
Epoch: 7423, Loss: 0.2858
Epoch: 7424, Loss: 0.2584
Epoch: 7425, Loss: 0.2517
Epoch: 7426, Loss: 0.2662
Epoch: 7427, Loss: 0.2674
Epoch: 7428, Loss: 0.2541
Epoch: 7429, Loss: 0.2526
Epoch: 7430, Loss: 0.2635
Epoch: 7431, Loss: 0.2575
Epoch: 7432, Loss: 0.2547
Epoch: 7433, Loss: 0.2531
Epoch: 7434, Loss: 0.2545
Epoch: 7435, Loss: 0.2529
Epoch: 7436, Loss: 0.2484
Epoch: 7437, Loss: 0.2528
Epoch: 7438, Loss: 0.2454
Epoch: 7439, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35666.76it/s]
Epoch: 7440, Loss: 0.2489, Train: 0.9239, Val: 0.9209, test: 0.9218
Epoch: 7441, Loss: 0.2491
Epoch: 7442, Loss: 0.2487
Epoch: 7443, Loss: 0.2471
Epoch: 7444, Loss: 0.2466
Epoch: 7445, Loss: 0.2481
Epoch: 7446, Loss: 0.2471
Epoch: 7447, Loss: 0.2507
Epoch: 7448, Loss: 0.2457
Epoch: 7449, Loss: 0.2512
Epoch: 7450, Loss: 0.2497
Epoch: 7451, Loss: 0.2487
Epoch: 7452, Loss: 0.2471
Epoch: 7453, Loss: 0.2455
Epoch: 7454, Loss: 0.2485
Epoch: 7455, Loss: 0.2478
Epoch: 7456, Loss: 0.2451
Epoch: 7457, Loss: 0.2467
Epoch: 7458, Loss: 0.2481
Epoch: 7459, Loss: 0.2477
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35785.72it/s]
Epoch: 7460, Loss: 0.2436, Train: 0.9251, Val: 0.9230, test: 0.9222
Epoch: 7461, Loss: 0.2446
Epoch: 7462, Loss: 0.2503
Epoch: 7463, Loss: 0.2430
Epoch: 7464, Loss: 0.2447
Epoch: 7465, Loss: 0.2481
Epoch: 7466, Loss: 0.2433
Epoch: 7467, Loss: 0.2479
Epoch: 7468, Loss: 0.2447
Epoch: 7469, Loss: 0.2448
Epoch: 7470, Loss: 0.2450
Epoch: 7471, Loss: 0.2459
Epoch: 7472, Loss: 0.2455
Epoch: 7473, Loss: 0.2426
Epoch: 7474, Loss: 0.2470
Epoch: 7475, Loss: 0.2445
Epoch: 7476, Loss: 0.2472
Epoch: 7477, Loss: 0.2467
Epoch: 7478, Loss: 0.2450
Epoch: 7479, Loss: 0.2448
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29536.31it/s]
Epoch: 7480, Loss: 0.2469, Train: 0.9245, Val: 0.9218, test: 0.9200
Epoch: 7481, Loss: 0.2450
Epoch: 7482, Loss: 0.2458
Epoch: 7483, Loss: 0.2421
Epoch: 7484, Loss: 0.2427
Epoch: 7485, Loss: 0.2504
Epoch: 7486, Loss: 0.2437
Epoch: 7487, Loss: 0.2439
Epoch: 7488, Loss: 0.2426
Epoch: 7489, Loss: 0.2464
Epoch: 7490, Loss: 0.2475
Epoch: 7491, Loss: 0.2435
Epoch: 7492, Loss: 0.2443
Epoch: 7493, Loss: 0.2443
Epoch: 7494, Loss: 0.2450
Epoch: 7495, Loss: 0.2443
Epoch: 7496, Loss: 0.2447
Epoch: 7497, Loss: 0.2427
Epoch: 7498, Loss: 0.2449
Epoch: 7499, Loss: 0.2441
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29975.53it/s]
Epoch: 7500, Loss: 0.2414, Train: 0.9253, Val: 0.9233, test: 0.9218
Epoch: 7501, Loss: 0.2409
Epoch: 7502, Loss: 0.2430
Epoch: 7503, Loss: 0.2419
Epoch: 7504, Loss: 0.2449
Epoch: 7505, Loss: 0.2449
Epoch: 7506, Loss: 0.2403
Epoch: 7507, Loss: 0.2406
Epoch: 7508, Loss: 0.2438
Epoch: 7509, Loss: 0.2453
Epoch: 7510, Loss: 0.2396
Epoch: 7511, Loss: 0.2396
Epoch: 7512, Loss: 0.2424
Epoch: 7513, Loss: 0.2412
Epoch: 7514, Loss: 0.2409
Epoch: 7515, Loss: 0.2434
Epoch: 7516, Loss: 0.2418
Epoch: 7517, Loss: 0.2424
Epoch: 7518, Loss: 0.2429
Epoch: 7519, Loss: 0.2425
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35070.98it/s]
Epoch: 7520, Loss: 0.2427, Train: 0.9259, Val: 0.9230, test: 0.9247
Epoch: 7521, Loss: 0.2425
Epoch: 7522, Loss: 0.2410
Epoch: 7523, Loss: 0.2422
Epoch: 7524, Loss: 0.2425
Epoch: 7525, Loss: 0.2400
Epoch: 7526, Loss: 0.2429
Epoch: 7527, Loss: 0.2427
Epoch: 7528, Loss: 0.2421
Epoch: 7529, Loss: 0.2419
Epoch: 7530, Loss: 0.2437
Epoch: 7531, Loss: 0.2409
Epoch: 7532, Loss: 0.2463
Epoch: 7533, Loss: 0.2415
Epoch: 7534, Loss: 0.2418
Epoch: 7535, Loss: 0.2435
Epoch: 7536, Loss: 0.2395
Epoch: 7537, Loss: 0.2420
Epoch: 7538, Loss: 0.2434
Epoch: 7539, Loss: 0.2406
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36607.10it/s]
Epoch: 7540, Loss: 0.2402, Train: 0.9256, Val: 0.9218, test: 0.9244
Epoch: 7541, Loss: 0.2392
Epoch: 7542, Loss: 0.2389
Epoch: 7543, Loss: 0.2413
Epoch: 7544, Loss: 0.2388
Epoch: 7545, Loss: 0.2375
Epoch: 7546, Loss: 0.2407
Epoch: 7547, Loss: 0.2420
Epoch: 7548, Loss: 0.2423
Epoch: 7549, Loss: 0.2422
Epoch: 7550, Loss: 0.2414
Epoch: 7551, Loss: 0.2399
Epoch: 7552, Loss: 0.2432
Epoch: 7553, Loss: 0.2434
Epoch: 7554, Loss: 0.2441
Epoch: 7555, Loss: 0.2423
Epoch: 7556, Loss: 0.2451
Epoch: 7557, Loss: 0.2425
Epoch: 7558, Loss: 0.2405
Epoch: 7559, Loss: 0.2419
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36794.79it/s]
Epoch: 7560, Loss: 0.2406, Train: 0.9262, Val: 0.9227, test: 0.9237
Epoch: 7561, Loss: 0.2411
Epoch: 7562, Loss: 0.2407
Epoch: 7563, Loss: 0.2409
Epoch: 7564, Loss: 0.2404
Epoch: 7565, Loss: 0.2418
Epoch: 7566, Loss: 0.2401
Epoch: 7567, Loss: 0.2430
Epoch: 7568, Loss: 0.2412
Epoch: 7569, Loss: 0.2434
Epoch: 7570, Loss: 0.2406
Epoch: 7571, Loss: 0.2404
Epoch: 7572, Loss: 0.2392
Epoch: 7573, Loss: 0.2387
Epoch: 7574, Loss: 0.2422
Epoch: 7575, Loss: 0.2438
Epoch: 7576, Loss: 0.2419
Epoch: 7577, Loss: 0.2407
Epoch: 7578, Loss: 0.2412
Epoch: 7579, Loss: 0.2443
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35308.45it/s]
Epoch: 7580, Loss: 0.2429, Train: 0.9251, Val: 0.9233, test: 0.9231
Epoch: 7581, Loss: 0.2429
Epoch: 7582, Loss: 0.2436
Epoch: 7583, Loss: 0.2404
Epoch: 7584, Loss: 0.2421
Epoch: 7585, Loss: 0.2407
Epoch: 7586, Loss: 0.2420
Epoch: 7587, Loss: 0.2424
Epoch: 7588, Loss: 0.2414
Epoch: 7589, Loss: 0.2427
Epoch: 7590, Loss: 0.2427
Epoch: 7591, Loss: 0.2380
Epoch: 7592, Loss: 0.2404
Epoch: 7593, Loss: 0.2461
Epoch: 7594, Loss: 0.2456
Epoch: 7595, Loss: 0.2424
Epoch: 7596, Loss: 0.2427
Epoch: 7597, Loss: 0.2401
Epoch: 7598, Loss: 0.2415
Epoch: 7599, Loss: 0.2397
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35951.05it/s]
Epoch: 7600, Loss: 0.2411, Train: 0.9258, Val: 0.9236, test: 0.9234
Epoch: 7601, Loss: 0.2444
Epoch: 7602, Loss: 0.2401
Epoch: 7603, Loss: 0.2411
Epoch: 7604, Loss: 0.2434
Epoch: 7605, Loss: 0.2440
Epoch: 7606, Loss: 0.2410
Epoch: 7607, Loss: 0.2405
Epoch: 7608, Loss: 0.2420
Epoch: 7609, Loss: 0.2429
Epoch: 7610, Loss: 0.2405
Epoch: 7611, Loss: 0.2451
Epoch: 7612, Loss: 0.2452
Epoch: 7613, Loss: 0.2431
Epoch: 7614, Loss: 0.2437
Epoch: 7615, Loss: 0.2403
Epoch: 7616, Loss: 0.2460
Epoch: 7617, Loss: 0.2436
Epoch: 7618, Loss: 0.2416
Epoch: 7619, Loss: 0.2430
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36705.98it/s]
Epoch: 7620, Loss: 0.2411, Train: 0.9256, Val: 0.9224, test: 0.9218
Epoch: 7621, Loss: 0.2495
Epoch: 7622, Loss: 0.2412
Epoch: 7623, Loss: 0.2400
Epoch: 7624, Loss: 0.2414
Epoch: 7625, Loss: 0.2423
Epoch: 7626, Loss: 0.2451
Epoch: 7627, Loss: 0.2431
Epoch: 7628, Loss: 0.2392
Epoch: 7629, Loss: 0.2441
Epoch: 7630, Loss: 0.2451
Epoch: 7631, Loss: 0.2409
Epoch: 7632, Loss: 0.2401
Epoch: 7633, Loss: 0.2603
Epoch: 7634, Loss: 0.2470
Epoch: 7635, Loss: 0.2478
Epoch: 7636, Loss: 0.2514
Epoch: 7637, Loss: 0.2469
Epoch: 7638, Loss: 0.2496
Epoch: 7639, Loss: 0.2486
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35679.35it/s]
Epoch: 7640, Loss: 0.2456, Train: 0.9254, Val: 0.9233, test: 0.9209
Epoch: 7641, Loss: 0.2455
Epoch: 7642, Loss: 0.2465
Epoch: 7643, Loss: 0.2460
Epoch: 7644, Loss: 0.2482
Epoch: 7645, Loss: 0.2446
Epoch: 7646, Loss: 0.2458
Epoch: 7647, Loss: 0.2443
Epoch: 7648, Loss: 0.2408
Epoch: 7649, Loss: 0.2477
Epoch: 7650, Loss: 0.2395
Epoch: 7651, Loss: 0.2412
Epoch: 7652, Loss: 0.2426
Epoch: 7653, Loss: 0.2416
Epoch: 7654, Loss: 0.2442
Epoch: 7655, Loss: 0.2411
Epoch: 7656, Loss: 0.2428
Epoch: 7657, Loss: 0.2389
Epoch: 7658, Loss: 0.2416
Epoch: 7659, Loss: 0.2394
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35226.05it/s]
Epoch: 7660, Loss: 0.2394, Train: 0.9271, Val: 0.9252, test: 0.9231
Epoch: 7661, Loss: 0.2455
Epoch: 7662, Loss: 0.2403
Epoch: 7663, Loss: 0.2395
Epoch: 7664, Loss: 0.2376
Epoch: 7665, Loss: 0.2383
Epoch: 7666, Loss: 0.2401
Epoch: 7667, Loss: 0.2395
Epoch: 7668, Loss: 0.2378
Epoch: 7669, Loss: 0.2386
Epoch: 7670, Loss: 0.2399
Epoch: 7671, Loss: 0.2408
Epoch: 7672, Loss: 0.2374
Epoch: 7673, Loss: 0.2423
Epoch: 7674, Loss: 0.2392
Epoch: 7675, Loss: 0.2417
Epoch: 7676, Loss: 0.2402
Epoch: 7677, Loss: 0.2438
Epoch: 7678, Loss: 0.2414
Epoch: 7679, Loss: 0.2393
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29911.94it/s]
Epoch: 7680, Loss: 0.2381, Train: 0.9251, Val: 0.9215, test: 0.9234
Epoch: 7681, Loss: 0.2402
Epoch: 7682, Loss: 0.2409
Epoch: 7683, Loss: 0.2432
Epoch: 7684, Loss: 0.2403
Epoch: 7685, Loss: 0.2408
Epoch: 7686, Loss: 0.2424
Epoch: 7687, Loss: 0.2405
Epoch: 7688, Loss: 0.2401
Epoch: 7689, Loss: 0.2422
Epoch: 7690, Loss: 0.2390
Epoch: 7691, Loss: 0.2398
Epoch: 7692, Loss: 0.2618
Epoch: 7693, Loss: 0.2401
Epoch: 7694, Loss: 0.2417
Epoch: 7695, Loss: 0.2422
Epoch: 7696, Loss: 0.2456
Epoch: 7697, Loss: 0.2383
Epoch: 7698, Loss: 0.2439
Epoch: 7699, Loss: 0.2419
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35478.07it/s]
Epoch: 7700, Loss: 0.2389, Train: 0.9255, Val: 0.9236, test: 0.9234
Epoch: 7701, Loss: 0.2469
Epoch: 7702, Loss: 0.2414
Epoch: 7703, Loss: 0.2442
Epoch: 7704, Loss: 0.2418
Epoch: 7705, Loss: 0.2418
Epoch: 7706, Loss: 0.2387
Epoch: 7707, Loss: 0.2421
Epoch: 7708, Loss: 0.2431
Epoch: 7709, Loss: 0.2411
Epoch: 7710, Loss: 0.2405
Epoch: 7711, Loss: 0.2433
Epoch: 7712, Loss: 0.2396
Epoch: 7713, Loss: 0.2414
Epoch: 7714, Loss: 0.2395
Epoch: 7715, Loss: 0.2425
Epoch: 7716, Loss: 0.2426
Epoch: 7717, Loss: 0.2445
Epoch: 7718, Loss: 0.2396
Epoch: 7719, Loss: 0.2425
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36101.42it/s]
Epoch: 7720, Loss: 0.2424, Train: 0.9254, Val: 0.9230, test: 0.9222
Epoch: 7721, Loss: 0.2401
Epoch: 7722, Loss: 0.2427
Epoch: 7723, Loss: 0.2421
Epoch: 7724, Loss: 0.2400
Epoch: 7725, Loss: 0.2402
Epoch: 7726, Loss: 0.2429
Epoch: 7727, Loss: 0.2497
Epoch: 7728, Loss: 0.2403
Epoch: 7729, Loss: 0.2409
Epoch: 7730, Loss: 0.2415
Epoch: 7731, Loss: 0.2420
Epoch: 7732, Loss: 0.2389
Epoch: 7733, Loss: 0.2438
Epoch: 7734, Loss: 0.2404
Epoch: 7735, Loss: 0.2416
Epoch: 7736, Loss: 0.2422
Epoch: 7737, Loss: 0.2372
Epoch: 7738, Loss: 0.2387
Epoch: 7739, Loss: 0.2405
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36525.93it/s]
Epoch: 7740, Loss: 0.2420, Train: 0.9251, Val: 0.9218, test: 0.9206
Epoch: 7741, Loss: 0.2415
Epoch: 7742, Loss: 0.2424
Epoch: 7743, Loss: 0.2396
Epoch: 7744, Loss: 0.2385
Epoch: 7745, Loss: 0.2394
Epoch: 7746, Loss: 0.2376
Epoch: 7747, Loss: 0.2407
Epoch: 7748, Loss: 0.2388
Epoch: 7749, Loss: 0.2385
Epoch: 7750, Loss: 0.2405
Epoch: 7751, Loss: 0.2382
Epoch: 7752, Loss: 0.2388
Epoch: 7753, Loss: 0.2378
Epoch: 7754, Loss: 0.2413
Epoch: 7755, Loss: 0.2406
Epoch: 7756, Loss: 0.2386
Epoch: 7757, Loss: 0.2367
Epoch: 7758, Loss: 0.2380
Epoch: 7759, Loss: 0.2416
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37799.02it/s]
Epoch: 7760, Loss: 0.2389, Train: 0.9241, Val: 0.9224, test: 0.9203
Epoch: 7761, Loss: 0.2363
Epoch: 7762, Loss: 0.2385
Epoch: 7763, Loss: 0.2398
Epoch: 7764, Loss: 0.2385
Epoch: 7765, Loss: 0.2368
Epoch: 7766, Loss: 0.2378
Epoch: 7767, Loss: 0.2371
Epoch: 7768, Loss: 0.2383
Epoch: 7769, Loss: 0.2403
Epoch: 7770, Loss: 0.2355
Epoch: 7771, Loss: 0.2357
Epoch: 7772, Loss: 0.2372
Epoch: 7773, Loss: 0.2371
Epoch: 7774, Loss: 0.2388
Epoch: 7775, Loss: 0.2363
Epoch: 7776, Loss: 0.2383
Epoch: 7777, Loss: 0.2401
Epoch: 7778, Loss: 0.2386
Epoch: 7779, Loss: 0.2370
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36425.35it/s]
Epoch: 7780, Loss: 0.2385, Train: 0.9260, Val: 0.9233, test: 0.9222
Epoch: 7781, Loss: 0.2393
Epoch: 7782, Loss: 0.2363
Epoch: 7783, Loss: 0.2379
Epoch: 7784, Loss: 0.2363
Epoch: 7785, Loss: 0.2407
Epoch: 7786, Loss: 0.2393
Epoch: 7787, Loss: 0.2367
Epoch: 7788, Loss: 0.2391
Epoch: 7789, Loss: 0.2411
Epoch: 7790, Loss: 0.2365
Epoch: 7791, Loss: 0.2377
Epoch: 7792, Loss: 0.2391
Epoch: 7793, Loss: 0.2382
Epoch: 7794, Loss: 0.2389
Epoch: 7795, Loss: 0.2385
Epoch: 7796, Loss: 0.2357
Epoch: 7797, Loss: 0.2401
Epoch: 7798, Loss: 0.2371
Epoch: 7799, Loss: 0.2410
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35699.15it/s]
Epoch: 7800, Loss: 0.2381, Train: 0.9260, Val: 0.9224, test: 0.9225
Epoch: 7801, Loss: 0.2388
Epoch: 7802, Loss: 0.2395
Epoch: 7803, Loss: 0.2395
Epoch: 7804, Loss: 0.2390
Epoch: 7805, Loss: 0.2375
Epoch: 7806, Loss: 0.2387
Epoch: 7807, Loss: 0.2369
Epoch: 7808, Loss: 0.2400
Epoch: 7809, Loss: 0.2364
Epoch: 7810, Loss: 0.2383
Epoch: 7811, Loss: 0.2390
Epoch: 7812, Loss: 0.2373
Epoch: 7813, Loss: 0.2376
Epoch: 7814, Loss: 0.2375
Epoch: 7815, Loss: 0.2384
Epoch: 7816, Loss: 0.2362
Epoch: 7817, Loss: 0.2376
Epoch: 7818, Loss: 0.2354
Epoch: 7819, Loss: 0.2354
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29601.56it/s]
Epoch: 7820, Loss: 0.2381, Train: 0.9253, Val: 0.9233, test: 0.9225
Epoch: 7821, Loss: 0.2378
Epoch: 7822, Loss: 0.2376
Epoch: 7823, Loss: 0.2372
Epoch: 7824, Loss: 0.2340
Epoch: 7825, Loss: 0.2363
Epoch: 7826, Loss: 0.2376
Epoch: 7827, Loss: 0.2339
Epoch: 7828, Loss: 0.2332
Epoch: 7829, Loss: 0.2369
Epoch: 7830, Loss: 0.2402
Epoch: 7831, Loss: 0.2380
Epoch: 7832, Loss: 0.2371
Epoch: 7833, Loss: 0.2403
Epoch: 7834, Loss: 0.2407
Epoch: 7835, Loss: 0.2368
Epoch: 7836, Loss: 0.2375
Epoch: 7837, Loss: 0.2373
Epoch: 7838, Loss: 0.2356
Epoch: 7839, Loss: 0.2384
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29579.16it/s]
Epoch: 7840, Loss: 0.2366, Train: 0.9262, Val: 0.9240, test: 0.9228
Epoch: 7841, Loss: 0.2376
Epoch: 7842, Loss: 0.2377
Epoch: 7843, Loss: 0.2373
Epoch: 7844, Loss: 0.2363
Epoch: 7845, Loss: 0.2385
Epoch: 7846, Loss: 0.2387
Epoch: 7847, Loss: 0.2387
Epoch: 7848, Loss: 0.2408
Epoch: 7849, Loss: 0.2382
Epoch: 7850, Loss: 0.2379
Epoch: 7851, Loss: 0.2337
Epoch: 7852, Loss: 0.2365
Epoch: 7853, Loss: 0.2375
Epoch: 7854, Loss: 0.2374
Epoch: 7855, Loss: 0.2350
Epoch: 7856, Loss: 0.2371
Epoch: 7857, Loss: 0.2348
Epoch: 7858, Loss: 0.2354
Epoch: 7859, Loss: 0.2342
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37990.05it/s]
Epoch: 7860, Loss: 0.2365, Train: 0.9256, Val: 0.9236, test: 0.9228
Epoch: 7861, Loss: 0.2367
Epoch: 7862, Loss: 0.2381
Epoch: 7863, Loss: 0.2362
Epoch: 7864, Loss: 0.2412
Epoch: 7865, Loss: 0.2362
Epoch: 7866, Loss: 0.2351
Epoch: 7867, Loss: 0.2354
Epoch: 7868, Loss: 0.2406
Epoch: 7869, Loss: 0.2402
Epoch: 7870, Loss: 0.2376
Epoch: 7871, Loss: 0.2350
Epoch: 7872, Loss: 0.2367
Epoch: 7873, Loss: 0.2323
Epoch: 7874, Loss: 0.2381
Epoch: 7875, Loss: 0.2368
Epoch: 7876, Loss: 0.2399
Epoch: 7877, Loss: 0.2387
Epoch: 7878, Loss: 0.2395
Epoch: 7879, Loss: 0.2365
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35540.32it/s]
Epoch: 7880, Loss: 0.2375, Train: 0.9263, Val: 0.9221, test: 0.9234
Epoch: 7881, Loss: 0.2364
Epoch: 7882, Loss: 0.2364
Epoch: 7883, Loss: 0.2364
Epoch: 7884, Loss: 0.2372
Epoch: 7885, Loss: 0.2360
Epoch: 7886, Loss: 0.2349
Epoch: 7887, Loss: 0.2369
Epoch: 7888, Loss: 0.2565
Epoch: 7889, Loss: 0.2362
Epoch: 7890, Loss: 0.2374
Epoch: 7891, Loss: 0.2350
Epoch: 7892, Loss: 0.2349
Epoch: 7893, Loss: 0.2383
Epoch: 7894, Loss: 0.2408
Epoch: 7895, Loss: 0.2363
Epoch: 7896, Loss: 0.2378
Epoch: 7897, Loss: 0.2351
Epoch: 7898, Loss: 0.2363
Epoch: 7899, Loss: 0.2379
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35808.34it/s]
Epoch: 7900, Loss: 0.2365, Train: 0.9250, Val: 0.9224, test: 0.9234
Epoch: 7901, Loss: 0.2353
Epoch: 7902, Loss: 0.2329
Epoch: 7903, Loss: 0.2354
Epoch: 7904, Loss: 0.2347
Epoch: 7905, Loss: 0.2364
Epoch: 7906, Loss: 0.2353
Epoch: 7907, Loss: 0.2354
Epoch: 7908, Loss: 0.2389
Epoch: 7909, Loss: 0.2357
Epoch: 7910, Loss: 0.2342
Epoch: 7911, Loss: 0.2340
Epoch: 7912, Loss: 0.2362
Epoch: 7913, Loss: 0.2375
Epoch: 7914, Loss: 0.2326
Epoch: 7915, Loss: 0.2376
Epoch: 7916, Loss: 0.2353
Epoch: 7917, Loss: 0.2340
Epoch: 7918, Loss: 0.2363
Epoch: 7919, Loss: 0.2355
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34924.00it/s]
Epoch: 7920, Loss: 0.2346, Train: 0.9252, Val: 0.9218, test: 0.9234
Epoch: 7921, Loss: 0.2396
Epoch: 7922, Loss: 0.2424
Epoch: 7923, Loss: 0.2461
Epoch: 7924, Loss: 0.4333
Epoch: 7925, Loss: 0.2621
Epoch: 7926, Loss: 0.2480
Epoch: 7927, Loss: 0.2596
Epoch: 7928, Loss: 0.2424
Epoch: 7929, Loss: 0.2469
Epoch: 7930, Loss: 0.2481
Epoch: 7931, Loss: 0.2521
Epoch: 7932, Loss: 0.2481
Epoch: 7933, Loss: 0.2500
Epoch: 7934, Loss: 0.2429
Epoch: 7935, Loss: 0.2473
Epoch: 7936, Loss: 0.2465
Epoch: 7937, Loss: 0.2451
Epoch: 7938, Loss: 0.2460
Epoch: 7939, Loss: 0.2421
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34994.58it/s]
Epoch: 7940, Loss: 0.2429, Train: 0.9238, Val: 0.9224, test: 0.9212
Epoch: 7941, Loss: 0.2396
Epoch: 7942, Loss: 0.2408
Epoch: 7943, Loss: 0.2403
Epoch: 7944, Loss: 0.2431
Epoch: 7945, Loss: 0.2397
Epoch: 7946, Loss: 0.2393
Epoch: 7947, Loss: 0.2434
Epoch: 7948, Loss: 0.2403
Epoch: 7949, Loss: 0.2405
Epoch: 7950, Loss: 0.2418
Epoch: 7951, Loss: 0.2407
Epoch: 7952, Loss: 0.2412
Epoch: 7953, Loss: 0.2401
Epoch: 7954, Loss: 0.2401
Epoch: 7955, Loss: 0.2425
Epoch: 7956, Loss: 0.2407
Epoch: 7957, Loss: 0.2373
Epoch: 7958, Loss: 0.2388
Epoch: 7959, Loss: 0.2367
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36660.14it/s]
Epoch: 7960, Loss: 0.2363, Train: 0.9258, Val: 0.9243, test: 0.9228
Epoch: 7961, Loss: 0.2376
Epoch: 7962, Loss: 0.2384
Epoch: 7963, Loss: 0.2376
Epoch: 7964, Loss: 0.2393
Epoch: 7965, Loss: 0.2375
Epoch: 7966, Loss: 0.2397
Epoch: 7967, Loss: 0.2396
Epoch: 7968, Loss: 0.2370
Epoch: 7969, Loss: 0.2381
Epoch: 7970, Loss: 0.2359
Epoch: 7971, Loss: 0.2359
Epoch: 7972, Loss: 0.2378
Epoch: 7973, Loss: 0.2423
Epoch: 7974, Loss: 0.2387
Epoch: 7975, Loss: 0.2379
Epoch: 7976, Loss: 0.2396
Epoch: 7977, Loss: 0.2411
Epoch: 7978, Loss: 0.2391
Epoch: 7979, Loss: 0.2369
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36577.45it/s]
Epoch: 7980, Loss: 0.2368, Train: 0.9244, Val: 0.9233, test: 0.9240
Epoch: 7981, Loss: 0.2367
Epoch: 7982, Loss: 0.2398
Epoch: 7983, Loss: 0.2387
Epoch: 7984, Loss: 0.2394
Epoch: 7985, Loss: 0.2419
Epoch: 7986, Loss: 0.2376
Epoch: 7987, Loss: 0.2395
Epoch: 7988, Loss: 0.2388
Epoch: 7989, Loss: 0.2352
Epoch: 7990, Loss: 0.2390
Epoch: 7991, Loss: 0.2408
Epoch: 7992, Loss: 0.2409
Epoch: 7993, Loss: 0.2372
Epoch: 7994, Loss: 0.2401
Epoch: 7995, Loss: 0.2383
Epoch: 7996, Loss: 0.2392
Epoch: 7997, Loss: 0.2371
Epoch: 7998, Loss: 0.2435
Epoch: 7999, Loss: 0.2387
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35463.71it/s]
Epoch: 8000, Loss: 0.2383, Train: 0.9246, Val: 0.9218, test: 0.9225
Epoch: 8001, Loss: 0.2379
Epoch: 8002, Loss: 0.2369
Epoch: 8003, Loss: 0.2401
Epoch: 8004, Loss: 0.2387
Epoch: 8005, Loss: 0.2359
Epoch: 8006, Loss: 0.2379
Epoch: 8007, Loss: 0.2365
Epoch: 8008, Loss: 0.2374
Epoch: 8009, Loss: 0.2396
Epoch: 8010, Loss: 0.2363
Epoch: 8011, Loss: 0.2352
Epoch: 8012, Loss: 0.2359
Epoch: 8013, Loss: 0.2373
Epoch: 8014, Loss: 0.2371
Epoch: 8015, Loss: 0.2348
Epoch: 8016, Loss: 0.2357
Epoch: 8017, Loss: 0.2393
Epoch: 8018, Loss: 0.2391
Epoch: 8019, Loss: 0.2359
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36611.39it/s]
Epoch: 8020, Loss: 0.2350, Train: 0.9257, Val: 0.9243, test: 0.9228
Epoch: 8021, Loss: 0.2376
Epoch: 8022, Loss: 0.2343
Epoch: 8023, Loss: 0.2373
Epoch: 8024, Loss: 0.2342
Epoch: 8025, Loss: 0.2344
Epoch: 8026, Loss: 0.2366
Epoch: 8027, Loss: 0.2342
Epoch: 8028, Loss: 0.2367
Epoch: 8029, Loss: 0.2352
Epoch: 8030, Loss: 0.2349
Epoch: 8031, Loss: 0.2336
Epoch: 8032, Loss: 0.2358
Epoch: 8033, Loss: 0.2376
Epoch: 8034, Loss: 0.2339
Epoch: 8035, Loss: 0.2314
Epoch: 8036, Loss: 0.2354
Epoch: 8037, Loss: 0.2341
Epoch: 8038, Loss: 0.2330
Epoch: 8039, Loss: 0.2351
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38011.93it/s]
Epoch: 8040, Loss: 0.2335, Train: 0.9258, Val: 0.9246, test: 0.9228
Epoch: 8041, Loss: 0.2355
Epoch: 8042, Loss: 0.2340
Epoch: 8043, Loss: 0.2374
Epoch: 8044, Loss: 0.2364
Epoch: 8045, Loss: 0.2343
Epoch: 8046, Loss: 0.2357
Epoch: 8047, Loss: 0.2333
Epoch: 8048, Loss: 0.2355
Epoch: 8049, Loss: 0.2343
Epoch: 8050, Loss: 0.2368
Epoch: 8051, Loss: 0.2385
Epoch: 8052, Loss: 0.2346
Epoch: 8053, Loss: 0.2377
Epoch: 8054, Loss: 0.2351
Epoch: 8055, Loss: 0.2347
Epoch: 8056, Loss: 0.2360
Epoch: 8057, Loss: 0.2347
Epoch: 8058, Loss: 0.2390
Epoch: 8059, Loss: 0.2364
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35540.89it/s]
Epoch: 8060, Loss: 0.2360, Train: 0.9258, Val: 0.9224, test: 0.9234
Epoch: 8061, Loss: 0.2377
Epoch: 8062, Loss: 0.2342
Epoch: 8063, Loss: 0.2375
Epoch: 8064, Loss: 0.2377
Epoch: 8065, Loss: 0.2336
Epoch: 8066, Loss: 0.2358
Epoch: 8067, Loss: 0.2370
Epoch: 8068, Loss: 0.2383
Epoch: 8069, Loss: 0.2362
Epoch: 8070, Loss: 0.2363
Epoch: 8071, Loss: 0.2351
Epoch: 8072, Loss: 0.2358
Epoch: 8073, Loss: 0.2352
Epoch: 8074, Loss: 0.2350
Epoch: 8075, Loss: 0.2358
Epoch: 8076, Loss: 0.2340
Epoch: 8077, Loss: 0.2323
Epoch: 8078, Loss: 0.2371
Epoch: 8079, Loss: 0.2334
Evaluating: 100%|| 64484/64484 [00:01<00:00, 33313.11it/s]
Epoch: 8080, Loss: 0.2345, Train: 0.9273, Val: 0.9246, test: 0.9222
Epoch: 8081, Loss: 0.2393
Epoch: 8082, Loss: 0.2364
Epoch: 8083, Loss: 0.2380
Epoch: 8084, Loss: 0.2357
Epoch: 8085, Loss: 0.2377
Epoch: 8086, Loss: 0.2351
Epoch: 8087, Loss: 0.2356
Epoch: 8088, Loss: 0.2344
Epoch: 8089, Loss: 0.2353
Epoch: 8090, Loss: 0.2357
Epoch: 8091, Loss: 0.2300
Epoch: 8092, Loss: 0.2355
Epoch: 8093, Loss: 0.2348
Epoch: 8094, Loss: 0.2360
Epoch: 8095, Loss: 0.2354
Epoch: 8096, Loss: 0.2389
Epoch: 8097, Loss: 0.2363
Epoch: 8098, Loss: 0.2399
Epoch: 8099, Loss: 0.2378
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30420.01it/s]
Epoch: 8100, Loss: 0.2434, Train: 0.9261, Val: 0.9258, test: 0.9231
Epoch: 8101, Loss: 0.2373
Epoch: 8102, Loss: 0.2432
Epoch: 8103, Loss: 0.2456
Epoch: 8104, Loss: 0.2421
Epoch: 8105, Loss: 0.2549
Epoch: 8106, Loss: 0.2467
Epoch: 8107, Loss: 0.2481
Epoch: 8108, Loss: 0.2484
Epoch: 8109, Loss: 0.2397
Epoch: 8110, Loss: 0.2467
Epoch: 8111, Loss: 0.2342
Epoch: 8112, Loss: 0.2424
Epoch: 8113, Loss: 0.2385
Epoch: 8114, Loss: 0.5067
Epoch: 8115, Loss: 0.2520
Epoch: 8116, Loss: 0.2537
Epoch: 8117, Loss: 0.2902
Epoch: 8118, Loss: 0.4939
Epoch: 8119, Loss: 0.6128
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29392.96it/s]
Epoch: 8120, Loss: 0.3768, Train: 0.9224, Val: 0.9209, test: 0.9215
Epoch: 8121, Loss: 0.5884
Epoch: 8122, Loss: 0.8523
Epoch: 8123, Loss: 0.5643
Epoch: 8124, Loss: 0.9272
Epoch: 8125, Loss: 0.6080
Epoch: 8126, Loss: 0.5495
Epoch: 8127, Loss: 1.0426
Epoch: 8128, Loss: 0.8546
Epoch: 8129, Loss: 0.7318
Epoch: 8130, Loss: 0.5463
Epoch: 8131, Loss: 0.4980
Epoch: 8132, Loss: 0.6207
Epoch: 8133, Loss: 0.3277
Epoch: 8134, Loss: 0.3453
Epoch: 8135, Loss: 0.5278
Epoch: 8136, Loss: 1.7627
Epoch: 8137, Loss: 0.6540
Epoch: 8138, Loss: 0.4228
Epoch: 8139, Loss: 0.6583
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30207.01it/s]
Epoch: 8140, Loss: 0.4929, Train: 0.9181, Val: 0.9184, test: 0.9146
Epoch: 8141, Loss: 0.3510
Epoch: 8142, Loss: 0.3327
Epoch: 8143, Loss: 0.3386
Epoch: 8144, Loss: 0.3791
Epoch: 8145, Loss: 0.4111
Epoch: 8146, Loss: 0.5269
Epoch: 8147, Loss: 0.5194
Epoch: 8148, Loss: 0.4591
Epoch: 8149, Loss: 0.3350
Epoch: 8150, Loss: 0.3320
Epoch: 8151, Loss: 0.2652
Epoch: 8152, Loss: 0.2784
Epoch: 8153, Loss: 0.2560
Epoch: 8154, Loss: 0.2565
Epoch: 8155, Loss: 0.2597
Epoch: 8156, Loss: 0.2710
Epoch: 8157, Loss: 0.2582
Epoch: 8158, Loss: 0.2577
Epoch: 8159, Loss: 0.2574
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30243.65it/s]
Epoch: 8160, Loss: 0.2620, Train: 0.9194, Val: 0.9202, test: 0.9187
Epoch: 8161, Loss: 0.2619
Epoch: 8162, Loss: 0.2569
Epoch: 8163, Loss: 0.2560
Epoch: 8164, Loss: 0.2567
Epoch: 8165, Loss: 0.2576
Epoch: 8166, Loss: 0.2626
Epoch: 8167, Loss: 0.2548
Epoch: 8168, Loss: 0.2569
Epoch: 8169, Loss: 0.2554
Epoch: 8170, Loss: 0.2538
Epoch: 8171, Loss: 0.2582
Epoch: 8172, Loss: 0.2526
Epoch: 8173, Loss: 0.2539
Epoch: 8174, Loss: 0.2529
Epoch: 8175, Loss: 0.2559
Epoch: 8176, Loss: 0.2521
Epoch: 8177, Loss: 0.2524
Epoch: 8178, Loss: 0.2508
Epoch: 8179, Loss: 0.2530
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35921.90it/s]
Epoch: 8180, Loss: 0.2541, Train: 0.9198, Val: 0.9202, test: 0.9178
Epoch: 8181, Loss: 0.2515
Epoch: 8182, Loss: 0.2514
Epoch: 8183, Loss: 0.2538
Epoch: 8184, Loss: 0.2539
Epoch: 8185, Loss: 0.2535
Epoch: 8186, Loss: 0.2548
Epoch: 8187, Loss: 0.2533
Epoch: 8188, Loss: 0.2508
Epoch: 8189, Loss: 0.2501
Epoch: 8190, Loss: 0.2544
Epoch: 8191, Loss: 0.2498
Epoch: 8192, Loss: 0.2526
Epoch: 8193, Loss: 0.2510
Epoch: 8194, Loss: 0.2509
Epoch: 8195, Loss: 0.2531
Epoch: 8196, Loss: 0.2536
Epoch: 8197, Loss: 0.2509
Epoch: 8198, Loss: 0.2507
Epoch: 8199, Loss: 0.2534
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36441.43it/s]
Epoch: 8200, Loss: 0.2518, Train: 0.9196, Val: 0.9196, test: 0.9168
Epoch: 8201, Loss: 0.2507
Epoch: 8202, Loss: 0.2500
Epoch: 8203, Loss: 0.2538
Epoch: 8204, Loss: 0.2502
Epoch: 8205, Loss: 0.2508
Epoch: 8206, Loss: 0.2516
Epoch: 8207, Loss: 0.2504
Epoch: 8208, Loss: 0.2515
Epoch: 8209, Loss: 0.2505
Epoch: 8210, Loss: 0.2493
Epoch: 8211, Loss: 0.2506
Epoch: 8212, Loss: 0.2496
Epoch: 8213, Loss: 0.2502
Epoch: 8214, Loss: 0.2536
Epoch: 8215, Loss: 0.2476
Epoch: 8216, Loss: 0.2502
Epoch: 8217, Loss: 0.2509
Epoch: 8218, Loss: 0.2494
Epoch: 8219, Loss: 0.2497
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37043.84it/s]
Epoch: 8220, Loss: 0.2497, Train: 0.9203, Val: 0.9209, test: 0.9171
Epoch: 8221, Loss: 0.2482
Epoch: 8222, Loss: 0.2501
Epoch: 8223, Loss: 0.2516
Epoch: 8224, Loss: 0.2508
Epoch: 8225, Loss: 0.2486
Epoch: 8226, Loss: 0.2530
Epoch: 8227, Loss: 0.2485
Epoch: 8228, Loss: 0.2514
Epoch: 8229, Loss: 0.2459
Epoch: 8230, Loss: 0.2501
Epoch: 8231, Loss: 0.2467
Epoch: 8232, Loss: 0.2479
Epoch: 8233, Loss: 0.2466
Epoch: 8234, Loss: 0.2494
Epoch: 8235, Loss: 0.2501
Epoch: 8236, Loss: 0.2466
Epoch: 8237, Loss: 0.2493
Epoch: 8238, Loss: 0.3131
Epoch: 8239, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37224.94it/s]
Epoch: 8240, Loss: 0.2532, Train: 0.9198, Val: 0.9193, test: 0.9165
Epoch: 8241, Loss: 0.2495
Epoch: 8242, Loss: 0.2481
Epoch: 8243, Loss: 0.2489
Epoch: 8244, Loss: 0.2508
Epoch: 8245, Loss: 0.2482
Epoch: 8246, Loss: 0.2536
Epoch: 8247, Loss: 0.2484
Epoch: 8248, Loss: 0.2516
Epoch: 8249, Loss: 0.2493
Epoch: 8250, Loss: 0.2489
Epoch: 8251, Loss: 0.2513
Epoch: 8252, Loss: 0.2493
Epoch: 8253, Loss: 0.2469
Epoch: 8254, Loss: 0.2463
Epoch: 8255, Loss: 0.2505
Epoch: 8256, Loss: 0.2502
Epoch: 8257, Loss: 0.2554
Epoch: 8258, Loss: 0.2491
Epoch: 8259, Loss: 0.2469
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36278.61it/s]
Epoch: 8260, Loss: 0.2521, Train: 0.9205, Val: 0.9209, test: 0.9184
Epoch: 8261, Loss: 0.2461
Epoch: 8262, Loss: 0.2471
Epoch: 8263, Loss: 0.2494
Epoch: 8264, Loss: 0.2457
Epoch: 8265, Loss: 0.2494
Epoch: 8266, Loss: 0.2497
Epoch: 8267, Loss: 0.2485
Epoch: 8268, Loss: 0.2561
Epoch: 8269, Loss: 0.2567
Epoch: 8270, Loss: 0.2487
Epoch: 8271, Loss: 0.2500
Epoch: 8272, Loss: 0.2506
Epoch: 8273, Loss: 0.2526
Epoch: 8274, Loss: 0.2502
Epoch: 8275, Loss: 0.2459
Epoch: 8276, Loss: 0.2470
Epoch: 8277, Loss: 0.2467
Epoch: 8278, Loss: 0.2537
Epoch: 8279, Loss: 0.2497
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36449.70it/s]
Epoch: 8280, Loss: 0.2496, Train: 0.9208, Val: 0.9205, test: 0.9181
Epoch: 8281, Loss: 0.2463
Epoch: 8282, Loss: 0.2494
Epoch: 8283, Loss: 0.2472
Epoch: 8284, Loss: 0.2499
Epoch: 8285, Loss: 0.2478
Epoch: 8286, Loss: 0.2520
Epoch: 8287, Loss: 0.2469
Epoch: 8288, Loss: 0.2467
Epoch: 8289, Loss: 0.2493
Epoch: 8290, Loss: 0.2467
Epoch: 8291, Loss: 0.2465
Epoch: 8292, Loss: 0.2477
Epoch: 8293, Loss: 0.2450
Epoch: 8294, Loss: 0.2474
Epoch: 8295, Loss: 0.2456
Epoch: 8296, Loss: 0.2455
Epoch: 8297, Loss: 0.2448
Epoch: 8298, Loss: 0.2479
Epoch: 8299, Loss: 0.2474
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35536.98it/s]
Epoch: 8300, Loss: 0.2453, Train: 0.9208, Val: 0.9202, test: 0.9187
Epoch: 8301, Loss: 0.2473
Epoch: 8302, Loss: 0.2444
Epoch: 8303, Loss: 0.2459
Epoch: 8304, Loss: 0.2464
Epoch: 8305, Loss: 0.2445
Epoch: 8306, Loss: 0.2471
Epoch: 8307, Loss: 0.2456
Epoch: 8308, Loss: 0.2470
Epoch: 8309, Loss: 0.2443
Epoch: 8310, Loss: 0.2468
Epoch: 8311, Loss: 0.2471
Epoch: 8312, Loss: 0.2481
Epoch: 8313, Loss: 0.2494
Epoch: 8314, Loss: 0.2478
Epoch: 8315, Loss: 0.2454
Epoch: 8316, Loss: 0.2484
Epoch: 8317, Loss: 0.2464
Epoch: 8318, Loss: 0.2480
Epoch: 8319, Loss: 0.2478
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37494.06it/s]
Epoch: 8320, Loss: 0.2451, Train: 0.9194, Val: 0.9190, test: 0.9178
Epoch: 8321, Loss: 0.2472
Epoch: 8322, Loss: 0.2485
Epoch: 8323, Loss: 0.2478
Epoch: 8324, Loss: 0.2445
Epoch: 8325, Loss: 0.2442
Epoch: 8326, Loss: 0.2463
Epoch: 8327, Loss: 0.2463
Epoch: 8328, Loss: 0.2443
Epoch: 8329, Loss: 0.2448
Epoch: 8330, Loss: 0.2479
Epoch: 8331, Loss: 0.2489
Epoch: 8332, Loss: 0.2490
Epoch: 8333, Loss: 0.2462
Epoch: 8334, Loss: 0.2479
Epoch: 8335, Loss: 0.2501
Epoch: 8336, Loss: 0.2455
Epoch: 8337, Loss: 0.2481
Epoch: 8338, Loss: 0.2458
Epoch: 8339, Loss: 0.2483
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37019.66it/s]
Epoch: 8340, Loss: 0.2470, Train: 0.9201, Val: 0.9187, test: 0.9178
Epoch: 8341, Loss: 0.2473
Epoch: 8342, Loss: 0.2473
Epoch: 8343, Loss: 0.2485
Epoch: 8344, Loss: 0.2502
Epoch: 8345, Loss: 0.2490
Epoch: 8346, Loss: 0.2461
Epoch: 8347, Loss: 0.2506
Epoch: 8348, Loss: 0.2479
Epoch: 8349, Loss: 0.2520
Epoch: 8350, Loss: 0.2558
Epoch: 8351, Loss: 0.2536
Epoch: 8352, Loss: 0.2513
Epoch: 8353, Loss: 0.2484
Epoch: 8354, Loss: 0.2504
Epoch: 8355, Loss: 0.2470
Epoch: 8356, Loss: 0.2474
Epoch: 8357, Loss: 0.2489
Epoch: 8358, Loss: 0.2524
Epoch: 8359, Loss: 0.2486
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37213.82it/s]
Epoch: 8360, Loss: 0.2461, Train: 0.9198, Val: 0.9187, test: 0.9184
Epoch: 8361, Loss: 0.2476
Epoch: 8362, Loss: 0.2487
Epoch: 8363, Loss: 0.2492
Epoch: 8364, Loss: 0.2460
Epoch: 8365, Loss: 0.2476
Epoch: 8366, Loss: 0.2504
Epoch: 8367, Loss: 0.2497
Epoch: 8368, Loss: 0.2491
Epoch: 8369, Loss: 0.2502
Epoch: 8370, Loss: 0.2459
Epoch: 8371, Loss: 0.2482
Epoch: 8372, Loss: 0.2459
Epoch: 8373, Loss: 0.2457
Epoch: 8374, Loss: 0.2452
Epoch: 8375, Loss: 0.2461
Epoch: 8376, Loss: 0.2476
Epoch: 8377, Loss: 0.2485
Epoch: 8378, Loss: 0.2447
Epoch: 8379, Loss: 0.2478
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36931.69it/s]
Epoch: 8380, Loss: 0.2449, Train: 0.9205, Val: 0.9205, test: 0.9187
Epoch: 8381, Loss: 0.2478
Epoch: 8382, Loss: 0.2446
Epoch: 8383, Loss: 0.2453
Epoch: 8384, Loss: 0.2471
Epoch: 8385, Loss: 0.2455
Epoch: 8386, Loss: 0.2457
Epoch: 8387, Loss: 0.2465
Epoch: 8388, Loss: 0.2514
Epoch: 8389, Loss: 0.2489
Epoch: 8390, Loss: 0.2453
Epoch: 8391, Loss: 0.2466
Epoch: 8392, Loss: 0.2497
Epoch: 8393, Loss: 0.2461
Epoch: 8394, Loss: 0.2472
Epoch: 8395, Loss: 0.2462
Epoch: 8396, Loss: 0.2476
Epoch: 8397, Loss: 0.2465
Epoch: 8398, Loss: 0.2487
Epoch: 8399, Loss: 0.2463
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35877.64it/s]
Epoch: 8400, Loss: 0.2464, Train: 0.9202, Val: 0.9205, test: 0.9178
Epoch: 8401, Loss: 0.2496
Epoch: 8402, Loss: 0.2456
Epoch: 8403, Loss: 0.2474
Epoch: 8404, Loss: 0.2498
Epoch: 8405, Loss: 0.2475
Epoch: 8406, Loss: 0.2480
Epoch: 8407, Loss: 0.2497
Epoch: 8408, Loss: 0.2492
Epoch: 8409, Loss: 0.2486
Epoch: 8410, Loss: 0.2516
Epoch: 8411, Loss: 0.2474
Epoch: 8412, Loss: 0.2453
Epoch: 8413, Loss: 0.2449
Epoch: 8414, Loss: 0.2512
Epoch: 8415, Loss: 0.2451
Epoch: 8416, Loss: 0.2473
Epoch: 8417, Loss: 0.2478
Epoch: 8418, Loss: 0.2503
Epoch: 8419, Loss: 0.2473
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36208.41it/s]
Epoch: 8420, Loss: 0.2494, Train: 0.9204, Val: 0.9202, test: 0.9181
Epoch: 8421, Loss: 0.2485
Epoch: 8422, Loss: 0.2501
Epoch: 8423, Loss: 0.2472
Epoch: 8424, Loss: 0.2476
Epoch: 8425, Loss: 0.2490
Epoch: 8426, Loss: 0.2501
Epoch: 8427, Loss: 0.2493
Epoch: 8428, Loss: 0.2512
Epoch: 8429, Loss: 0.2513
Epoch: 8430, Loss: 0.2469
Epoch: 8431, Loss: 0.2509
Epoch: 8432, Loss: 0.2539
Epoch: 8433, Loss: 0.2508
Epoch: 8434, Loss: 0.2507
Epoch: 8435, Loss: 0.2475
Epoch: 8436, Loss: 0.2501
Epoch: 8437, Loss: 0.2479
Epoch: 8438, Loss: 0.2488
Epoch: 8439, Loss: 0.2500
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35760.48it/s]
Epoch: 8440, Loss: 0.2461, Train: 0.9197, Val: 0.9199, test: 0.9187
Epoch: 8441, Loss: 0.2460
Epoch: 8442, Loss: 0.2487
Epoch: 8443, Loss: 0.2455
Epoch: 8444, Loss: 0.2475
Epoch: 8445, Loss: 0.2473
Epoch: 8446, Loss: 0.2496
Epoch: 8447, Loss: 0.2474
Epoch: 8448, Loss: 0.2485
Epoch: 8449, Loss: 0.2482
Epoch: 8450, Loss: 0.2494
Epoch: 8451, Loss: 0.2466
Epoch: 8452, Loss: 0.2455
Epoch: 8453, Loss: 0.2473
Epoch: 8454, Loss: 0.2495
Epoch: 8455, Loss: 0.2449
Epoch: 8456, Loss: 0.2465
Epoch: 8457, Loss: 0.2470
Epoch: 8458, Loss: 0.2494
Epoch: 8459, Loss: 0.2466
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37334.09it/s]
Epoch: 8460, Loss: 0.2448, Train: 0.9205, Val: 0.9215, test: 0.9184
Epoch: 8461, Loss: 0.2435
Epoch: 8462, Loss: 0.2429
Epoch: 8463, Loss: 0.2466
Epoch: 8464, Loss: 0.2433
Epoch: 8465, Loss: 0.2464
Epoch: 8466, Loss: 0.2440
Epoch: 8467, Loss: 0.2463
Epoch: 8468, Loss: 0.2446
Epoch: 8469, Loss: 0.2471
Epoch: 8470, Loss: 0.2443
Epoch: 8471, Loss: 0.2463
Epoch: 8472, Loss: 1.5493
Epoch: 8473, Loss: 0.2665
Epoch: 8474, Loss: 0.2941
Epoch: 8475, Loss: 0.2814
Epoch: 8476, Loss: 0.3207
Epoch: 8477, Loss: 0.2900
Epoch: 8478, Loss: 0.2974
Epoch: 8479, Loss: 0.3111
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35718.78it/s]
Epoch: 8480, Loss: 0.2966, Train: 0.9173, Val: 0.9146, test: 0.9168
Epoch: 8481, Loss: 0.2890
Epoch: 8482, Loss: 0.2934
Epoch: 8483, Loss: 0.2975
Epoch: 8484, Loss: 0.2986
Epoch: 8485, Loss: 0.2941
Epoch: 8486, Loss: 0.2836
Epoch: 8487, Loss: 0.2732
Epoch: 8488, Loss: 0.2740
Epoch: 8489, Loss: 0.2730
Epoch: 8490, Loss: 0.2708
Epoch: 8491, Loss: 0.2746
Epoch: 8492, Loss: 0.2766
Epoch: 8493, Loss: 0.2727
Epoch: 8494, Loss: 0.2688
Epoch: 8495, Loss: 0.2631
Epoch: 8496, Loss: 0.2636
Epoch: 8497, Loss: 0.2609
Epoch: 8498, Loss: 0.2633
Epoch: 8499, Loss: 0.2607
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36027.56it/s]
Epoch: 8500, Loss: 0.2590, Train: 0.9184, Val: 0.9187, test: 0.9156
Epoch: 8501, Loss: 0.2621
Epoch: 8502, Loss: 0.2646
Epoch: 8503, Loss: 0.2602
Epoch: 8504, Loss: 0.2663
Epoch: 8505, Loss: 0.2641
Epoch: 8506, Loss: 0.2684
Epoch: 8507, Loss: 0.2606
Epoch: 8508, Loss: 0.2581
Epoch: 8509, Loss: 0.2619
Epoch: 8510, Loss: 0.2630
Epoch: 8511, Loss: 0.2565
Epoch: 8512, Loss: 0.2638
Epoch: 8513, Loss: 0.2579
Epoch: 8514, Loss: 0.2676
Epoch: 8515, Loss: 0.2619
Epoch: 8516, Loss: 0.2616
Epoch: 8517, Loss: 0.2590
Epoch: 8518, Loss: 0.2661
Epoch: 8519, Loss: 0.2615
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36361.73it/s]
Epoch: 8520, Loss: 0.2604, Train: 0.9187, Val: 0.9190, test: 0.9165
Epoch: 8521, Loss: 0.2589
Epoch: 8522, Loss: 0.2582
Epoch: 8523, Loss: 0.2588
Epoch: 8524, Loss: 0.2562
Epoch: 8525, Loss: 0.2569
Epoch: 8526, Loss: 0.2563
Epoch: 8527, Loss: 0.2571
Epoch: 8528, Loss: 0.2578
Epoch: 8529, Loss: 0.2580
Epoch: 8530, Loss: 0.2549
Epoch: 8531, Loss: 0.2581
Epoch: 8532, Loss: 0.2738
Epoch: 8533, Loss: 0.2735
Epoch: 8534, Loss: 0.2604
Epoch: 8535, Loss: 0.2658
Epoch: 8536, Loss: 0.2605
Epoch: 8537, Loss: 0.2641
Epoch: 8538, Loss: 0.2648
Epoch: 8539, Loss: 0.2646
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36304.73it/s]
Epoch: 8540, Loss: 0.2642, Train: 0.9192, Val: 0.9193, test: 0.9178
Epoch: 8541, Loss: 0.2665
Epoch: 8542, Loss: 0.2643
Epoch: 8543, Loss: 0.2627
Epoch: 8544, Loss: 0.2619
Epoch: 8545, Loss: 0.2655
Epoch: 8546, Loss: 0.2592
Epoch: 8547, Loss: 0.2602
Epoch: 8548, Loss: 0.2615
Epoch: 8549, Loss: 0.2612
Epoch: 8550, Loss: 0.2571
Epoch: 8551, Loss: 0.2564
Epoch: 8552, Loss: 0.2586
Epoch: 8553, Loss: 0.2576
Epoch: 8554, Loss: 0.2566
Epoch: 8555, Loss: 0.2533
Epoch: 8556, Loss: 0.2578
Epoch: 8557, Loss: 0.2565
Epoch: 8558, Loss: 0.2669
Epoch: 8559, Loss: 0.2678
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35629.93it/s]
Epoch: 8560, Loss: 0.2863, Train: 0.9193, Val: 0.9193, test: 0.9175
Epoch: 8561, Loss: 0.3061
Epoch: 8562, Loss: 0.2897
Epoch: 8563, Loss: 0.2863
Epoch: 8564, Loss: 0.2699
Epoch: 8565, Loss: 0.2637
Epoch: 8566, Loss: 0.2577
Epoch: 8567, Loss: 0.2597
Epoch: 8568, Loss: 0.2579
Epoch: 8569, Loss: 0.2601
Epoch: 8570, Loss: 0.2576
Epoch: 8571, Loss: 0.2578
Epoch: 8572, Loss: 0.2758
Epoch: 8573, Loss: 0.2614
Epoch: 8574, Loss: 0.2678
Epoch: 8575, Loss: 0.2643
Epoch: 8576, Loss: 0.2698
Epoch: 8577, Loss: 0.2645
Epoch: 8578, Loss: 0.2662
Epoch: 8579, Loss: 0.2634
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35936.99it/s]
Epoch: 8580, Loss: 0.2637, Train: 0.9194, Val: 0.9193, test: 0.9168
Epoch: 8581, Loss: 0.2686
Epoch: 8582, Loss: 0.2593
Epoch: 8583, Loss: 0.2623
Epoch: 8584, Loss: 0.2618
Epoch: 8585, Loss: 0.2577
Epoch: 8586, Loss: 0.2628
Epoch: 8587, Loss: 0.2584
Epoch: 8588, Loss: 0.2611
Epoch: 8589, Loss: 0.2628
Epoch: 8590, Loss: 0.2586
Epoch: 8591, Loss: 0.2621
Epoch: 8592, Loss: 0.2567
Epoch: 8593, Loss: 0.2622
Epoch: 8594, Loss: 0.2576
Epoch: 8595, Loss: 0.2566
Epoch: 8596, Loss: 0.2575
Epoch: 8597, Loss: 0.2584
Epoch: 8598, Loss: 0.2599
Epoch: 8599, Loss: 0.2627
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36871.49it/s]
Epoch: 8600, Loss: 0.2584, Train: 0.9193, Val: 0.9193, test: 0.9156
Epoch: 8601, Loss: 0.2580
Epoch: 8602, Loss: 0.2584
Epoch: 8603, Loss: 0.2540
Epoch: 8604, Loss: 0.2539
Epoch: 8605, Loss: 0.2619
Epoch: 8606, Loss: 0.2632
Epoch: 8607, Loss: 0.2548
Epoch: 8608, Loss: 0.2623
Epoch: 8609, Loss: 0.2599
Epoch: 8610, Loss: 0.2572
Epoch: 8611, Loss: 0.2670
Epoch: 8612, Loss: 0.2592
Epoch: 8613, Loss: 0.2637
Epoch: 8614, Loss: 0.2643
Epoch: 8615, Loss: 0.2560
Epoch: 8616, Loss: 0.2614
Epoch: 8617, Loss: 0.2621
Epoch: 8618, Loss: 0.2641
Epoch: 8619, Loss: 0.2637
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37156.13it/s]
Epoch: 8620, Loss: 0.2627, Train: 0.9195, Val: 0.9199, test: 0.9168
Epoch: 8621, Loss: 0.2611
Epoch: 8622, Loss: 0.2605
Epoch: 8623, Loss: 0.2631
Epoch: 8624, Loss: 0.2619
Epoch: 8625, Loss: 0.2596
Epoch: 8626, Loss: 0.2612
Epoch: 8627, Loss: 0.2615
Epoch: 8628, Loss: 0.2605
Epoch: 8629, Loss: 0.2639
Epoch: 8630, Loss: 0.2666
Epoch: 8631, Loss: 0.2718
Epoch: 8632, Loss: 0.2592
Epoch: 8633, Loss: 0.2616
Epoch: 8634, Loss: 0.2637
Epoch: 8635, Loss: 0.2631
Epoch: 8636, Loss: 0.2740
Epoch: 8637, Loss: 0.2627
Epoch: 8638, Loss: 0.2667
Epoch: 8639, Loss: 0.2581
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36830.63it/s]
Epoch: 8640, Loss: 0.2633, Train: 0.9194, Val: 0.9202, test: 0.9165
Epoch: 8641, Loss: 0.2659
Epoch: 8642, Loss: 0.2590
Epoch: 8643, Loss: 0.2595
Epoch: 8644, Loss: 0.2634
Epoch: 8645, Loss: 0.2601
Epoch: 8646, Loss: 0.2607
Epoch: 8647, Loss: 0.2626
Epoch: 8648, Loss: 0.2596
Epoch: 8649, Loss: 0.2600
Epoch: 8650, Loss: 0.2585
Epoch: 8651, Loss: 0.2596
Epoch: 8652, Loss: 0.2591
Epoch: 8653, Loss: 0.2677
Epoch: 8654, Loss: 0.2664
Epoch: 8655, Loss: 0.2643
Epoch: 8656, Loss: 0.2615
Epoch: 8657, Loss: 0.2583
Epoch: 8658, Loss: 0.2610
Epoch: 8659, Loss: 0.2609
Evaluating: 100%|| 64484/64484 [00:01<00:00, 38109.60it/s]
Epoch: 8660, Loss: 0.2635, Train: 0.9192, Val: 0.9190, test: 0.9165
Epoch: 8661, Loss: 0.2623
Epoch: 8662, Loss: 0.2650
Epoch: 8663, Loss: 0.2663
Epoch: 8664, Loss: 0.2609
Epoch: 8665, Loss: 0.2627
Epoch: 8666, Loss: 0.2600
Epoch: 8667, Loss: 0.2596
Epoch: 8668, Loss: 0.2603
Epoch: 8669, Loss: 0.2604
Epoch: 8670, Loss: 0.2635
Epoch: 8671, Loss: 0.2616
Epoch: 8672, Loss: 0.2624
Epoch: 8673, Loss: 0.2554
Epoch: 8674, Loss: 0.2580
Epoch: 8675, Loss: 0.2643
Epoch: 8676, Loss: 0.2621
Epoch: 8677, Loss: 0.2600
Epoch: 8678, Loss: 0.2608
Epoch: 8679, Loss: 0.2576
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37565.42it/s]
Epoch: 8680, Loss: 0.2602, Train: 0.9196, Val: 0.9205, test: 0.9168
Epoch: 8681, Loss: 0.2648
Epoch: 8682, Loss: 0.2607
Epoch: 8683, Loss: 0.2589
Epoch: 8684, Loss: 0.2597
Epoch: 8685, Loss: 0.2575
Epoch: 8686, Loss: 0.2627
Epoch: 8687, Loss: 0.2622
Epoch: 8688, Loss: 0.2591
Epoch: 8689, Loss: 0.2614
Epoch: 8690, Loss: 0.2621
Epoch: 8691, Loss: 0.2621
Epoch: 8692, Loss: 0.2585
Epoch: 8693, Loss: 0.2624
Epoch: 8694, Loss: 0.2598
Epoch: 8695, Loss: 0.2587
Epoch: 8696, Loss: 0.2577
Epoch: 8697, Loss: 0.2586
Epoch: 8698, Loss: 0.2568
Epoch: 8699, Loss: 0.2586
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36500.86it/s]
Epoch: 8700, Loss: 0.2579, Train: 0.9194, Val: 0.9205, test: 0.9171
Epoch: 8701, Loss: 0.2606
Epoch: 8702, Loss: 0.2527
Epoch: 8703, Loss: 0.2549
Epoch: 8704, Loss: 0.2577
Epoch: 8705, Loss: 0.2550
Epoch: 8706, Loss: 0.2576
Epoch: 8707, Loss: 0.2574
Epoch: 8708, Loss: 0.2602
Epoch: 8709, Loss: 0.2528
Epoch: 8710, Loss: 0.2565
Epoch: 8711, Loss: 0.2530
Epoch: 8712, Loss: 0.2564
Epoch: 8713, Loss: 0.2574
Epoch: 8714, Loss: 0.2535
Epoch: 8715, Loss: 0.2524
Epoch: 8716, Loss: 0.2542
Epoch: 8717, Loss: 0.2568
Epoch: 8718, Loss: 0.2546
Epoch: 8719, Loss: 0.2536
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36496.96it/s]
Epoch: 8720, Loss: 0.2514, Train: 0.9212, Val: 0.9212, test: 0.9181
Epoch: 8721, Loss: 0.2557
Epoch: 8722, Loss: 0.2591
Epoch: 8723, Loss: 0.2529
Epoch: 8724, Loss: 0.2648
Epoch: 8725, Loss: 0.2552
Epoch: 8726, Loss: 0.2633
Epoch: 8727, Loss: 0.2567
Epoch: 8728, Loss: 0.2573
Epoch: 8729, Loss: 0.2618
Epoch: 8730, Loss: 0.2642
Epoch: 8731, Loss: 0.2578
Epoch: 8732, Loss: 0.2639
Epoch: 8733, Loss: 0.2628
Epoch: 8734, Loss: 0.2612
Epoch: 8735, Loss: 0.2758
Epoch: 8736, Loss: 0.2701
Epoch: 8737, Loss: 0.2637
Epoch: 8738, Loss: 0.2752
Epoch: 8739, Loss: 0.2579
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29935.09it/s]
Epoch: 8740, Loss: 0.2659, Train: 0.9205, Val: 0.9215, test: 0.9178
Epoch: 8741, Loss: 0.2598
Epoch: 8742, Loss: 0.2644
Epoch: 8743, Loss: 0.2638
Epoch: 8744, Loss: 0.2611
Epoch: 8745, Loss: 0.2664
Epoch: 8746, Loss: 0.2571
Epoch: 8747, Loss: 0.2642
Epoch: 8748, Loss: 0.2544
Epoch: 8749, Loss: 0.2570
Epoch: 8750, Loss: 0.2731
Epoch: 8751, Loss: 0.2570
Epoch: 8752, Loss: 0.2658
Epoch: 8753, Loss: 0.2548
Epoch: 8754, Loss: 0.2637
Epoch: 8755, Loss: 0.2575
Epoch: 8756, Loss: 0.2608
Epoch: 8757, Loss: 0.2588
Epoch: 8758, Loss: 0.2593
Epoch: 8759, Loss: 0.2574
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29994.80it/s]
Epoch: 8760, Loss: 0.2562, Train: 0.9208, Val: 0.9190, test: 0.9175
Epoch: 8761, Loss: 0.2592
Epoch: 8762, Loss: 0.2555
Epoch: 8763, Loss: 0.2580
Epoch: 8764, Loss: 0.2586
Epoch: 8765, Loss: 0.2568
Epoch: 8766, Loss: 0.2597
Epoch: 8767, Loss: 0.2588
Epoch: 8768, Loss: 0.2588
Epoch: 8769, Loss: 0.2584
Epoch: 8770, Loss: 0.2567
Epoch: 8771, Loss: 0.2569
Epoch: 8772, Loss: 0.2584
Epoch: 8773, Loss: 0.2570
Epoch: 8774, Loss: 0.2527
Epoch: 8775, Loss: 0.2563
Epoch: 8776, Loss: 0.2528
Epoch: 8777, Loss: 0.2576
Epoch: 8778, Loss: 0.2536
Epoch: 8779, Loss: 0.2545
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35492.53it/s]
Epoch: 8780, Loss: 0.2551, Train: 0.9199, Val: 0.9184, test: 0.9159
Epoch: 8781, Loss: 0.2561
Epoch: 8782, Loss: 0.2518
Epoch: 8783, Loss: 0.2520
Epoch: 8784, Loss: 0.2545
Epoch: 8785, Loss: 0.2585
Epoch: 8786, Loss: 0.2631
Epoch: 8787, Loss: 0.2544
Epoch: 8788, Loss: 0.2551
Epoch: 8789, Loss: 0.2558
Epoch: 8790, Loss: 0.2553
Epoch: 8791, Loss: 0.2586
Epoch: 8792, Loss: 0.2577
Epoch: 8793, Loss: 0.2560
Epoch: 8794, Loss: 0.2584
Epoch: 8795, Loss: 0.2546
Epoch: 8796, Loss: 0.2599
Epoch: 8797, Loss: 0.2563
Epoch: 8798, Loss: 0.2814
Epoch: 8799, Loss: 0.2574
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35497.31it/s]
Epoch: 8800, Loss: 0.2576, Train: 0.9206, Val: 0.9202, test: 0.9184
Epoch: 8801, Loss: 0.2555
Epoch: 8802, Loss: 0.2602
Epoch: 8803, Loss: 0.2582
Epoch: 8804, Loss: 0.2599
Epoch: 8805, Loss: 0.2598
Epoch: 8806, Loss: 0.2604
Epoch: 8807, Loss: 0.2582
Epoch: 8808, Loss: 0.2590
Epoch: 8809, Loss: 0.2601
Epoch: 8810, Loss: 0.2612
Epoch: 8811, Loss: 0.2568
Epoch: 8812, Loss: 0.2584
Epoch: 8813, Loss: 0.2577
Epoch: 8814, Loss: 0.2587
Epoch: 8815, Loss: 0.2598
Epoch: 8816, Loss: 0.2629
Epoch: 8817, Loss: 0.2574
Epoch: 8818, Loss: 0.2570
Epoch: 8819, Loss: 0.2590
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35917.86it/s]
Epoch: 8820, Loss: 0.2568, Train: 0.9213, Val: 0.9215, test: 0.9181
Epoch: 8821, Loss: 0.2596
Epoch: 8822, Loss: 0.2586
Epoch: 8823, Loss: 0.2598
Epoch: 8824, Loss: 0.2565
Epoch: 8825, Loss: 0.2595
Epoch: 8826, Loss: 0.2566
Epoch: 8827, Loss: 0.2608
Epoch: 8828, Loss: 0.2550
Epoch: 8829, Loss: 0.2563
Epoch: 8830, Loss: 0.2560
Epoch: 8831, Loss: 0.2555
Epoch: 8832, Loss: 0.2538
Epoch: 8833, Loss: 0.2573
Epoch: 8834, Loss: 0.2569
Epoch: 8835, Loss: 0.2554
Epoch: 8836, Loss: 0.2552
Epoch: 8837, Loss: 0.2555
Epoch: 8838, Loss: 0.2568
Epoch: 8839, Loss: 0.2549
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36428.81it/s]
Epoch: 8840, Loss: 0.2538, Train: 0.9210, Val: 0.9212, test: 0.9175
Epoch: 8841, Loss: 0.2571
Epoch: 8842, Loss: 0.2541
Epoch: 8843, Loss: 0.2531
Epoch: 8844, Loss: 0.2577
Epoch: 8845, Loss: 0.2551
Epoch: 8846, Loss: 0.2551
Epoch: 8847, Loss: 0.2560
Epoch: 8848, Loss: 0.2532
Epoch: 8849, Loss: 0.2573
Epoch: 8850, Loss: 0.2559
Epoch: 8851, Loss: 0.2552
Epoch: 8852, Loss: 0.2579
Epoch: 8853, Loss: 0.2633
Epoch: 8854, Loss: 0.2611
Epoch: 8855, Loss: 0.2591
Epoch: 8856, Loss: 0.2584
Epoch: 8857, Loss: 0.2565
Epoch: 8858, Loss: 0.2574
Epoch: 8859, Loss: 0.2563
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37668.28it/s]
Epoch: 8860, Loss: 0.2553, Train: 0.9209, Val: 0.9202, test: 0.9175
Epoch: 8861, Loss: 0.2556
Epoch: 8862, Loss: 0.2569
Epoch: 8863, Loss: 0.2585
Epoch: 8864, Loss: 0.2564
Epoch: 8865, Loss: 0.2582
Epoch: 8866, Loss: 0.2563
Epoch: 8867, Loss: 0.2567
Epoch: 8868, Loss: 0.2540
Epoch: 8869, Loss: 0.2551
Epoch: 8870, Loss: 0.2562
Epoch: 8871, Loss: 0.2536
Epoch: 8872, Loss: 0.2575
Epoch: 8873, Loss: 0.2561
Epoch: 8874, Loss: 0.2537
Epoch: 8875, Loss: 0.2534
Epoch: 8876, Loss: 0.2558
Epoch: 8877, Loss: 0.2564
Epoch: 8878, Loss: 0.2527
Epoch: 8879, Loss: 0.2535
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35323.28it/s]
Epoch: 8880, Loss: 0.2543, Train: 0.9205, Val: 0.9209, test: 0.9165
Epoch: 8881, Loss: 0.2568
Epoch: 8882, Loss: 0.2565
Epoch: 8883, Loss: 0.2556
Epoch: 8884, Loss: 0.2545
Epoch: 8885, Loss: 0.2520
Epoch: 8886, Loss: 0.2576
Epoch: 8887, Loss: 0.2558
Epoch: 8888, Loss: 0.2532
Epoch: 8889, Loss: 0.2559
Epoch: 8890, Loss: 0.2532
Epoch: 8891, Loss: 0.2557
Epoch: 8892, Loss: 0.2594
Epoch: 8893, Loss: 0.2534
Epoch: 8894, Loss: 0.2596
Epoch: 8895, Loss: 0.2596
Epoch: 8896, Loss: 0.2600
Epoch: 8897, Loss: 0.2558
Epoch: 8898, Loss: 0.2592
Epoch: 8899, Loss: 0.2571
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36421.49it/s]
Epoch: 8900, Loss: 0.2545, Train: 0.9211, Val: 0.9221, test: 0.9175
Epoch: 8901, Loss: 0.2599
Epoch: 8902, Loss: 0.2578
Epoch: 8903, Loss: 0.2536
Epoch: 8904, Loss: 0.2568
Epoch: 8905, Loss: 0.2537
Epoch: 8906, Loss: 0.2546
Epoch: 8907, Loss: 0.2562
Epoch: 8908, Loss: 0.2598
Epoch: 8909, Loss: 0.2562
Epoch: 8910, Loss: 0.2598
Epoch: 8911, Loss: 0.2542
Epoch: 8912, Loss: 0.2575
Epoch: 8913, Loss: 0.2579
Epoch: 8914, Loss: 0.2573
Epoch: 8915, Loss: 0.2556
Epoch: 8916, Loss: 0.2537
Epoch: 8917, Loss: 0.2585
Epoch: 8918, Loss: 0.2546
Epoch: 8919, Loss: 0.2563
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34960.27it/s]
Epoch: 8920, Loss: 0.2536, Train: 0.9203, Val: 0.9224, test: 0.9178
Epoch: 8921, Loss: 0.2527
Epoch: 8922, Loss: 0.2534
Epoch: 8923, Loss: 0.2515
Epoch: 8924, Loss: 0.2542
Epoch: 8925, Loss: 0.2579
Epoch: 8926, Loss: 0.2532
Epoch: 8927, Loss: 0.2552
Epoch: 8928, Loss: 0.2542
Epoch: 8929, Loss: 0.2571
Epoch: 8930, Loss: 0.2545
Epoch: 8931, Loss: 0.2519
Epoch: 8932, Loss: 0.2551
Epoch: 8933, Loss: 0.2529
Epoch: 8934, Loss: 0.2559
Epoch: 8935, Loss: 0.2504
Epoch: 8936, Loss: 0.2501
Epoch: 8937, Loss: 0.2539
Epoch: 8938, Loss: 0.2527
Epoch: 8939, Loss: 0.2531
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35407.18it/s]
Epoch: 8940, Loss: 0.2547, Train: 0.9210, Val: 0.9215, test: 0.9178
Epoch: 8941, Loss: 0.2559
Epoch: 8942, Loss: 0.2502
Epoch: 8943, Loss: 0.2514
Epoch: 8944, Loss: 0.2509
Epoch: 8945, Loss: 0.2549
Epoch: 8946, Loss: 0.2548
Epoch: 8947, Loss: 0.2508
Epoch: 8948, Loss: 0.2530
Epoch: 8949, Loss: 0.2517
Epoch: 8950, Loss: 0.2537
Epoch: 8951, Loss: 0.2537
Epoch: 8952, Loss: 0.2556
Epoch: 8953, Loss: 0.2535
Epoch: 8954, Loss: 0.2534
Epoch: 8955, Loss: 0.2556
Epoch: 8956, Loss: 0.2502
Epoch: 8957, Loss: 0.2512
Epoch: 8958, Loss: 0.2518
Epoch: 8959, Loss: 0.2536
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35397.08it/s]
Epoch: 8960, Loss: 0.2539, Train: 0.9198, Val: 0.9205, test: 0.9178
Epoch: 8961, Loss: 0.2603
Epoch: 8962, Loss: 0.2681
Epoch: 8963, Loss: 0.2720
Epoch: 8964, Loss: 0.2675
Epoch: 8965, Loss: 0.2585
Epoch: 8966, Loss: 0.2536
Epoch: 8967, Loss: 0.2553
Epoch: 8968, Loss: 0.2539
Epoch: 8969, Loss: 0.2546
Epoch: 8970, Loss: 0.2536
Epoch: 8971, Loss: 0.2502
Epoch: 8972, Loss: 0.2542
Epoch: 8973, Loss: 0.2526
Epoch: 8974, Loss: 0.2552
Epoch: 8975, Loss: 0.2554
Epoch: 8976, Loss: 0.2544
Epoch: 8977, Loss: 0.2485
Epoch: 8978, Loss: 0.2550
Epoch: 8979, Loss: 0.2509
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37540.20it/s]
Epoch: 8980, Loss: 0.2529, Train: 0.9210, Val: 0.9205, test: 0.9187
Epoch: 8981, Loss: 0.2547
Epoch: 8982, Loss: 0.2501
Epoch: 8983, Loss: 0.2497
Epoch: 8984, Loss: 0.2526
Epoch: 8985, Loss: 0.2541
Epoch: 8986, Loss: 0.2512
Epoch: 8987, Loss: 0.2503
Epoch: 8988, Loss: 0.2522
Epoch: 8989, Loss: 0.2470
Epoch: 8990, Loss: 0.2507
Epoch: 8991, Loss: 0.2536
Epoch: 8992, Loss: 0.2469
Epoch: 8993, Loss: 0.2517
Epoch: 8994, Loss: 0.2517
Epoch: 8995, Loss: 0.2499
Epoch: 8996, Loss: 0.2467
Epoch: 8997, Loss: 0.2512
Epoch: 8998, Loss: 0.2478
Epoch: 8999, Loss: 0.2498
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37211.56it/s]
Epoch: 9000, Loss: 0.2475, Train: 0.9214, Val: 0.9227, test: 0.9184
Epoch: 9001, Loss: 0.2520
Epoch: 9002, Loss: 0.2508
Epoch: 9003, Loss: 0.2500
Epoch: 9004, Loss: 0.2520
Epoch: 9005, Loss: 0.2472
Epoch: 9006, Loss: 0.2517
Epoch: 9007, Loss: 0.2544
Epoch: 9008, Loss: 0.2492
Epoch: 9009, Loss: 0.2488
Epoch: 9010, Loss: 0.2492
Epoch: 9011, Loss: 0.2568
Epoch: 9012, Loss: 0.2494
Epoch: 9013, Loss: 0.2463
Epoch: 9014, Loss: 0.2535
Epoch: 9015, Loss: 0.2541
Epoch: 9016, Loss: 0.2485
Epoch: 9017, Loss: 0.2494
Epoch: 9018, Loss: 0.2521
Epoch: 9019, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36816.88it/s]
Epoch: 9020, Loss: 0.2491, Train: 0.9205, Val: 0.9193, test: 0.9184
Epoch: 9021, Loss: 0.2513
Epoch: 9022, Loss: 0.2504
Epoch: 9023, Loss: 0.2481
Epoch: 9024, Loss: 0.2453
Epoch: 9025, Loss: 0.2526
Epoch: 9026, Loss: 0.2478
Epoch: 9027, Loss: 0.2509
Epoch: 9028, Loss: 0.2503
Epoch: 9029, Loss: 0.2469
Epoch: 9030, Loss: 0.2513
Epoch: 9031, Loss: 0.2485
Epoch: 9032, Loss: 0.2601
Epoch: 9033, Loss: 0.2500
Epoch: 9034, Loss: 0.2508
Epoch: 9035, Loss: 0.2543
Epoch: 9036, Loss: 0.2536
Epoch: 9037, Loss: 0.2547
Epoch: 9038, Loss: 0.2576
Epoch: 9039, Loss: 0.2553
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37565.24it/s]
Epoch: 9040, Loss: 0.2589, Train: 0.9227, Val: 0.9252, test: 0.9178
Epoch: 9041, Loss: 0.2565
Epoch: 9042, Loss: 0.2559
Epoch: 9043, Loss: 0.2551
Epoch: 9044, Loss: 0.2553
Epoch: 9045, Loss: 0.2561
Epoch: 9046, Loss: 0.2564
Epoch: 9047, Loss: 0.2537
Epoch: 9048, Loss: 0.2553
Epoch: 9049, Loss: 0.2529
Epoch: 9050, Loss: 0.2528
Epoch: 9051, Loss: 0.2540
Epoch: 9052, Loss: 0.2595
Epoch: 9053, Loss: 0.2562
Epoch: 9054, Loss: 0.2547
Epoch: 9055, Loss: 0.2576
Epoch: 9056, Loss: 0.2547
Epoch: 9057, Loss: 0.2567
Epoch: 9058, Loss: 0.2544
Epoch: 9059, Loss: 0.2531
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35054.72it/s]
Epoch: 9060, Loss: 0.2561, Train: 0.9219, Val: 0.9227, test: 0.9171
Epoch: 9061, Loss: 0.2545
Epoch: 9062, Loss: 0.2553
Epoch: 9063, Loss: 0.2521
Epoch: 9064, Loss: 0.2535
Epoch: 9065, Loss: 0.2554
Epoch: 9066, Loss: 0.2539
Epoch: 9067, Loss: 0.2571
Epoch: 9068, Loss: 0.2593
Epoch: 9069, Loss: 0.2731
Epoch: 9070, Loss: 0.2838
Epoch: 9071, Loss: 0.2915
Epoch: 9072, Loss: 0.2564
Epoch: 9073, Loss: 0.5068
Epoch: 9074, Loss: 0.2833
Epoch: 9075, Loss: 0.2690
Epoch: 9076, Loss: 0.2553
Epoch: 9077, Loss: 0.2564
Epoch: 9078, Loss: 0.2540
Epoch: 9079, Loss: 0.2522
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36203.45it/s]
Epoch: 9080, Loss: 0.2528, Train: 0.9220, Val: 0.9221, test: 0.9187
Epoch: 9081, Loss: 0.2530
Epoch: 9082, Loss: 0.2527
Epoch: 9083, Loss: 0.2537
Epoch: 9084, Loss: 0.2517
Epoch: 9085, Loss: 0.2532
Epoch: 9086, Loss: 0.2545
Epoch: 9087, Loss: 0.2487
Epoch: 9088, Loss: 0.2499
Epoch: 9089, Loss: 0.2546
Epoch: 9090, Loss: 0.2560
Epoch: 9091, Loss: 0.2505
Epoch: 9092, Loss: 0.2542
Epoch: 9093, Loss: 0.2584
Epoch: 9094, Loss: 0.2553
Epoch: 9095, Loss: 0.2606
Epoch: 9096, Loss: 0.2607
Epoch: 9097, Loss: 0.2626
Epoch: 9098, Loss: 0.2549
Epoch: 9099, Loss: 0.2546
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36934.14it/s]
Epoch: 9100, Loss: 0.2550, Train: 0.9211, Val: 0.9224, test: 0.9178
Epoch: 9101, Loss: 0.2533
Epoch: 9102, Loss: 0.2527
Epoch: 9103, Loss: 0.2538
Epoch: 9104, Loss: 0.2563
Epoch: 9105, Loss: 0.2565
Epoch: 9106, Loss: 0.2519
Epoch: 9107, Loss: 0.2557
Epoch: 9108, Loss: 0.2508
Epoch: 9109, Loss: 0.2561
Epoch: 9110, Loss: 0.2553
Epoch: 9111, Loss: 0.2554
Epoch: 9112, Loss: 0.2625
Epoch: 9113, Loss: 0.2577
Epoch: 9114, Loss: 0.2588
Epoch: 9115, Loss: 0.2570
Epoch: 9116, Loss: 0.2562
Epoch: 9117, Loss: 0.2586
Epoch: 9118, Loss: 0.2607
Epoch: 9119, Loss: 0.2591
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35637.84it/s]
Epoch: 9120, Loss: 0.2595, Train: 0.9221, Val: 0.9224, test: 0.9187
Epoch: 9121, Loss: 0.2528
Epoch: 9122, Loss: 0.2569
Epoch: 9123, Loss: 0.2555
Epoch: 9124, Loss: 0.2547
Epoch: 9125, Loss: 0.2553
Epoch: 9126, Loss: 0.2527
Epoch: 9127, Loss: 0.2543
Epoch: 9128, Loss: 0.2539
Epoch: 9129, Loss: 0.2559
Epoch: 9130, Loss: 1.4592
Epoch: 9131, Loss: 0.2548
Epoch: 9132, Loss: 0.2532
Epoch: 9133, Loss: 0.2529
Epoch: 9134, Loss: 0.2571
Epoch: 9135, Loss: 0.2563
Epoch: 9136, Loss: 0.2560
Epoch: 9137, Loss: 0.2548
Epoch: 9138, Loss: 0.2562
Epoch: 9139, Loss: 0.2568
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35391.14it/s]
Epoch: 9140, Loss: 0.2547, Train: 0.9214, Val: 0.9205, test: 0.9193
Epoch: 9141, Loss: 0.2601
Epoch: 9142, Loss: 0.2557
Epoch: 9143, Loss: 0.2535
Epoch: 9144, Loss: 0.2538
Epoch: 9145, Loss: 0.2570
Epoch: 9146, Loss: 0.2566
Epoch: 9147, Loss: 0.2514
Epoch: 9148, Loss: 0.2510
Epoch: 9149, Loss: 0.2593
Epoch: 9150, Loss: 0.2551
Epoch: 9151, Loss: 0.2530
Epoch: 9152, Loss: 0.2533
Epoch: 9153, Loss: 0.2553
Epoch: 9154, Loss: 0.2503
Epoch: 9155, Loss: 0.2550
Epoch: 9156, Loss: 0.2555
Epoch: 9157, Loss: 0.2560
Epoch: 9158, Loss: 0.2546
Epoch: 9159, Loss: 0.2586
Evaluating: 100%|| 64484/64484 [00:01<00:00, 34602.25it/s]
Epoch: 9160, Loss: 0.2537, Train: 0.9219, Val: 0.9215, test: 0.9187
Epoch: 9161, Loss: 0.2520
Epoch: 9162, Loss: 0.2514
Epoch: 9163, Loss: 0.2501
Epoch: 9164, Loss: 0.2512
Epoch: 9165, Loss: 0.2524
Epoch: 9166, Loss: 0.2546
Epoch: 9167, Loss: 0.2500
Epoch: 9168, Loss: 0.2540
Epoch: 9169, Loss: 0.2526
Epoch: 9170, Loss: 0.2517
Epoch: 9171, Loss: 0.2552
Epoch: 9172, Loss: 0.2519
Epoch: 9173, Loss: 0.2531
Epoch: 9174, Loss: 0.2524
Epoch: 9175, Loss: 0.2530
Epoch: 9176, Loss: 0.2520
Epoch: 9177, Loss: 0.2520
Epoch: 9178, Loss: 0.2538
Epoch: 9179, Loss: 0.2537
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35701.41it/s]
Epoch: 9180, Loss: 0.2493, Train: 0.9218, Val: 0.9233, test: 0.9168
Epoch: 9181, Loss: 0.2529
Epoch: 9182, Loss: 0.2550
Epoch: 9183, Loss: 0.2508
Epoch: 9184, Loss: 0.2520
Epoch: 9185, Loss: 0.2542
Epoch: 9186, Loss: 0.2524
Epoch: 9187, Loss: 0.2506
Epoch: 9188, Loss: 0.2512
Epoch: 9189, Loss: 0.2517
Epoch: 9190, Loss: 0.2489
Epoch: 9191, Loss: 0.2554
Epoch: 9192, Loss: 0.2555
Epoch: 9193, Loss: 0.2513
Epoch: 9194, Loss: 0.2509
Epoch: 9195, Loss: 0.2536
Epoch: 9196, Loss: 0.2516
Epoch: 9197, Loss: 0.2525
Epoch: 9198, Loss: 0.2501
Epoch: 9199, Loss: 0.2508
Evaluating: 100%|| 64484/64484 [00:02<00:00, 30277.11it/s]
Epoch: 9200, Loss: 0.2504, Train: 0.9224, Val: 0.9218, test: 0.9203
Epoch: 9201, Loss: 0.2522
Epoch: 9202, Loss: 0.2524
Epoch: 9203, Loss: 0.2482
Epoch: 9204, Loss: 0.2497
Epoch: 9205, Loss: 0.2539
Epoch: 9206, Loss: 0.2508
Epoch: 9207, Loss: 0.2501
Epoch: 9208, Loss: 0.2508
Epoch: 9209, Loss: 0.2539
Epoch: 9210, Loss: 0.2537
Epoch: 9211, Loss: 0.2545
Epoch: 9212, Loss: 0.2548
Epoch: 9213, Loss: 0.2531
Epoch: 9214, Loss: 0.2546
Epoch: 9215, Loss: 0.2557
Epoch: 9216, Loss: 0.2541
Epoch: 9217, Loss: 0.2536
Epoch: 9218, Loss: 0.2522
Epoch: 9219, Loss: 0.2545
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35119.40it/s]
Epoch: 9220, Loss: 0.2502, Train: 0.9229, Val: 0.9227, test: 0.9193
Epoch: 9221, Loss: 0.2534
Epoch: 9222, Loss: 0.2530
Epoch: 9223, Loss: 0.2522
Epoch: 9224, Loss: 0.2510
Epoch: 9225, Loss: 0.2541
Epoch: 9226, Loss: 0.2548
Epoch: 9227, Loss: 0.2482
Epoch: 9228, Loss: 0.2510
Epoch: 9229, Loss: 0.2505
Epoch: 9230, Loss: 0.2560
Epoch: 9231, Loss: 0.2538
Epoch: 9232, Loss: 0.2539
Epoch: 9233, Loss: 0.2541
Epoch: 9234, Loss: 0.2530
Epoch: 9235, Loss: 0.2536
Epoch: 9236, Loss: 0.2536
Epoch: 9237, Loss: 0.2551
Epoch: 9238, Loss: 0.2539
Epoch: 9239, Loss: 0.2523
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29359.45it/s]
Epoch: 9240, Loss: 0.2558, Train: 0.9212, Val: 0.9221, test: 0.9181
Epoch: 9241, Loss: 0.2531
Epoch: 9242, Loss: 0.2509
Epoch: 9243, Loss: 0.2498
Epoch: 9244, Loss: 0.2539
Epoch: 9245, Loss: 0.2533
Epoch: 9246, Loss: 0.2534
Epoch: 9247, Loss: 0.2535
Epoch: 9248, Loss: 0.2533
Epoch: 9249, Loss: 0.2548
Epoch: 9250, Loss: 0.2516
Epoch: 9251, Loss: 0.2522
Epoch: 9252, Loss: 0.2520
Epoch: 9253, Loss: 0.2513
Epoch: 9254, Loss: 0.2534
Epoch: 9255, Loss: 0.2499
Epoch: 9256, Loss: 0.2500
Epoch: 9257, Loss: 0.2520
Epoch: 9258, Loss: 0.2514
Epoch: 9259, Loss: 0.2521
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36660.15it/s]
Epoch: 9260, Loss: 0.2534, Train: 0.9224, Val: 0.9230, test: 0.9171
Epoch: 9261, Loss: 0.2543
Epoch: 9262, Loss: 0.2481
Epoch: 9263, Loss: 0.2540
Epoch: 9264, Loss: 0.2529
Epoch: 9265, Loss: 0.2561
Epoch: 9266, Loss: 0.2542
Epoch: 9267, Loss: 0.2520
Epoch: 9268, Loss: 0.2515
Epoch: 9269, Loss: 0.2492
Epoch: 9270, Loss: 0.2515
Epoch: 9271, Loss: 0.2464
Epoch: 9272, Loss: 0.2492
Epoch: 9273, Loss: 0.2517
Epoch: 9274, Loss: 0.2536
Epoch: 9275, Loss: 0.2507
Epoch: 9276, Loss: 0.2501
Epoch: 9277, Loss: 0.2522
Epoch: 9278, Loss: 0.2523
Epoch: 9279, Loss: 0.2512
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37434.06it/s]
Epoch: 9280, Loss: 0.2526, Train: 0.9215, Val: 0.9209, test: 0.9184
Epoch: 9281, Loss: 0.2572
Epoch: 9282, Loss: 0.2532
Epoch: 9283, Loss: 0.2507
Epoch: 9284, Loss: 0.2519
Epoch: 9285, Loss: 0.2555
Epoch: 9286, Loss: 0.2500
Epoch: 9287, Loss: 0.2538
Epoch: 9288, Loss: 0.2522
Epoch: 9289, Loss: 0.2491
Epoch: 9290, Loss: 0.2518
Epoch: 9291, Loss: 0.2583
Epoch: 9292, Loss: 0.2587
Epoch: 9293, Loss: 0.2480
Epoch: 9294, Loss: 0.2505
Epoch: 9295, Loss: 0.2540
Epoch: 9296, Loss: 0.2523
Epoch: 9297, Loss: 0.2507
Epoch: 9298, Loss: 0.2553
Epoch: 9299, Loss: 0.2520
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37708.95it/s]
Epoch: 9300, Loss: 0.2506, Train: 0.9225, Val: 0.9233, test: 0.9171
Epoch: 9301, Loss: 0.2501
Epoch: 9302, Loss: 0.2503
Epoch: 9303, Loss: 0.2488
Epoch: 9304, Loss: 0.2487
Epoch: 9305, Loss: 0.2495
Epoch: 9306, Loss: 0.2493
Epoch: 9307, Loss: 0.2520
Epoch: 9308, Loss: 0.2512
Epoch: 9309, Loss: 0.2488
Epoch: 9310, Loss: 0.2501
Epoch: 9311, Loss: 0.2503
Epoch: 9312, Loss: 0.2514
Epoch: 9313, Loss: 0.2514
Epoch: 9314, Loss: 0.2486
Epoch: 9315, Loss: 0.2488
Epoch: 9316, Loss: 0.2485
Epoch: 9317, Loss: 0.2448
Epoch: 9318, Loss: 0.2509
Epoch: 9319, Loss: 0.2522
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37171.08it/s]
Epoch: 9320, Loss: 0.2478, Train: 0.9225, Val: 0.9233, test: 0.9193
Epoch: 9321, Loss: 0.2464
Epoch: 9322, Loss: 0.2504
Epoch: 9323, Loss: 0.2485
Epoch: 9324, Loss: 0.2517
Epoch: 9325, Loss: 0.2502
Epoch: 9326, Loss: 0.2485
Epoch: 9327, Loss: 0.2474
Epoch: 9328, Loss: 0.2476
Epoch: 9329, Loss: 0.2501
Epoch: 9330, Loss: 0.2506
Epoch: 9331, Loss: 0.2437
Epoch: 9332, Loss: 0.2458
Epoch: 9333, Loss: 0.2452
Epoch: 9334, Loss: 0.2475
Epoch: 9335, Loss: 0.2461
Epoch: 9336, Loss: 0.2442
Epoch: 9337, Loss: 0.2479
Epoch: 9338, Loss: 0.2478
Epoch: 9339, Loss: 0.2483
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36919.17it/s]
Epoch: 9340, Loss: 0.2461, Train: 0.9231, Val: 0.9249, test: 0.9184
Epoch: 9341, Loss: 0.2463
Epoch: 9342, Loss: 0.2473
Epoch: 9343, Loss: 0.2451
Epoch: 9344, Loss: 0.2476
Epoch: 9345, Loss: 0.2455
Epoch: 9346, Loss: 0.2474
Epoch: 9347, Loss: 0.2446
Epoch: 9348, Loss: 0.2473
Epoch: 9349, Loss: 0.2489
Epoch: 9350, Loss: 0.2473
Epoch: 9351, Loss: 0.2446
Epoch: 9352, Loss: 0.2474
Epoch: 9353, Loss: 0.2438
Epoch: 9354, Loss: 0.2485
Epoch: 9355, Loss: 0.2490
Epoch: 9356, Loss: 0.2449
Epoch: 9357, Loss: 0.2476
Epoch: 9358, Loss: 0.2441
Epoch: 9359, Loss: 0.2460
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37512.48it/s]
Epoch: 9360, Loss: 0.2460, Train: 0.9222, Val: 0.9224, test: 0.9175
Epoch: 9361, Loss: 0.2577
Epoch: 9362, Loss: 0.2456
Epoch: 9363, Loss: 0.2476
Epoch: 9364, Loss: 0.2468
Epoch: 9365, Loss: 0.2480
Epoch: 9366, Loss: 0.2457
Epoch: 9367, Loss: 0.2480
Epoch: 9368, Loss: 0.2473
Epoch: 9369, Loss: 0.2505
Epoch: 9370, Loss: 0.2486
Epoch: 9371, Loss: 0.2493
Epoch: 9372, Loss: 0.2515
Epoch: 9373, Loss: 0.2488
Epoch: 9374, Loss: 0.2480
Epoch: 9375, Loss: 0.2519
Epoch: 9376, Loss: 0.2510
Epoch: 9377, Loss: 0.2494
Epoch: 9378, Loss: 0.2509
Epoch: 9379, Loss: 0.2509
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37062.77it/s]
Epoch: 9380, Loss: 0.2545, Train: 0.9196, Val: 0.9227, test: 0.9168
Epoch: 9381, Loss: 0.2504
Epoch: 9382, Loss: 0.2482
Epoch: 9383, Loss: 0.2491
Epoch: 9384, Loss: 0.2510
Epoch: 9385, Loss: 0.2469
Epoch: 9386, Loss: 0.2503
Epoch: 9387, Loss: 0.2491
Epoch: 9388, Loss: 0.2529
Epoch: 9389, Loss: 0.2475
Epoch: 9390, Loss: 0.2561
Epoch: 9391, Loss: 0.2512
Epoch: 9392, Loss: 0.3500
Epoch: 9393, Loss: 0.2515
Epoch: 9394, Loss: 0.2481
Epoch: 9395, Loss: 0.2492
Epoch: 9396, Loss: 0.2500
Epoch: 9397, Loss: 0.2495
Epoch: 9398, Loss: 0.2555
Epoch: 9399, Loss: 0.2564
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37419.70it/s]
Epoch: 9400, Loss: 0.2507, Train: 0.9210, Val: 0.9249, test: 0.9165
Epoch: 9401, Loss: 0.2552
Epoch: 9402, Loss: 0.2497
Epoch: 9403, Loss: 0.2522
Epoch: 9404, Loss: 0.2549
Epoch: 9405, Loss: 0.2528
Epoch: 9406, Loss: 0.2499
Epoch: 9407, Loss: 0.2512
Epoch: 9408, Loss: 0.2496
Epoch: 9409, Loss: 0.2498
Epoch: 9410, Loss: 0.2507
Epoch: 9411, Loss: 0.2467
Epoch: 9412, Loss: 0.2454
Epoch: 9413, Loss: 0.2517
Epoch: 9414, Loss: 0.2504
Epoch: 9415, Loss: 0.2478
Epoch: 9416, Loss: 0.2474
Epoch: 9417, Loss: 0.2474
Epoch: 9418, Loss: 0.2502
Epoch: 9419, Loss: 0.2503
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37300.98it/s]
Epoch: 9420, Loss: 0.2498, Train: 0.9215, Val: 0.9240, test: 0.9168
Epoch: 9421, Loss: 0.2532
Epoch: 9422, Loss: 0.2518
Epoch: 9423, Loss: 0.2471
Epoch: 9424, Loss: 0.2481
Epoch: 9425, Loss: 0.2476
Epoch: 9426, Loss: 0.2486
Epoch: 9427, Loss: 0.2469
Epoch: 9428, Loss: 0.2493
Epoch: 9429, Loss: 0.2471
Epoch: 9430, Loss: 0.2468
Epoch: 9431, Loss: 0.2492
Epoch: 9432, Loss: 0.2454
Epoch: 9433, Loss: 0.2475
Epoch: 9434, Loss: 0.2504
Epoch: 9435, Loss: 0.2536
Epoch: 9436, Loss: 0.2492
Epoch: 9437, Loss: 0.2474
Epoch: 9438, Loss: 0.2526
Epoch: 9439, Loss: 0.2533
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37391.52it/s]
Epoch: 9440, Loss: 0.2455, Train: 0.9219, Val: 0.9227, test: 0.9187
Epoch: 9441, Loss: 0.2521
Epoch: 9442, Loss: 0.2471
Epoch: 9443, Loss: 0.2532
Epoch: 9444, Loss: 0.2522
Epoch: 9445, Loss: 0.2505
Epoch: 9446, Loss: 0.2490
Epoch: 9447, Loss: 0.2446
Epoch: 9448, Loss: 0.2470
Epoch: 9449, Loss: 0.2458
Epoch: 9450, Loss: 0.2445
Epoch: 9451, Loss: 0.2460
Epoch: 9452, Loss: 0.2465
Epoch: 9453, Loss: 0.2500
Epoch: 9454, Loss: 0.2499
Epoch: 9455, Loss: 0.2437
Epoch: 9456, Loss: 0.2454
Epoch: 9457, Loss: 0.2436
Epoch: 9458, Loss: 0.2459
Epoch: 9459, Loss: 0.2444
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37577.79it/s]
Epoch: 9460, Loss: 0.2487, Train: 0.9220, Val: 0.9240, test: 0.9178
Epoch: 9461, Loss: 0.2486
Epoch: 9462, Loss: 0.2475
Epoch: 9463, Loss: 0.2479
Epoch: 9464, Loss: 0.2478
Epoch: 9465, Loss: 0.2442
Epoch: 9466, Loss: 0.2490
Epoch: 9467, Loss: 0.2474
Epoch: 9468, Loss: 0.2513
Epoch: 9469, Loss: 0.2484
Epoch: 9470, Loss: 0.2512
Epoch: 9471, Loss: 0.2519
Epoch: 9472, Loss: 0.2480
Epoch: 9473, Loss: 0.2490
Epoch: 9474, Loss: 0.2528
Epoch: 9475, Loss: 0.2499
Epoch: 9476, Loss: 0.2490
Epoch: 9477, Loss: 0.2486
Epoch: 9478, Loss: 0.2478
Epoch: 9479, Loss: 0.2441
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37386.56it/s]
Epoch: 9480, Loss: 0.2473, Train: 0.9212, Val: 0.9227, test: 0.9175
Epoch: 9481, Loss: 0.2449
Epoch: 9482, Loss: 0.2482
Epoch: 9483, Loss: 0.2534
Epoch: 9484, Loss: 0.2467
Epoch: 9485, Loss: 0.2502
Epoch: 9486, Loss: 0.2501
Epoch: 9487, Loss: 0.2519
Epoch: 9488, Loss: 0.2504
Epoch: 9489, Loss: 0.2525
Epoch: 9490, Loss: 0.2547
Epoch: 9491, Loss: 0.2509
Epoch: 9492, Loss: 0.2489
Epoch: 9493, Loss: 0.2526
Epoch: 9494, Loss: 0.2510
Epoch: 9495, Loss: 0.2516
Epoch: 9496, Loss: 0.2447
Epoch: 9497, Loss: 0.2480
Epoch: 9498, Loss: 0.2473
Epoch: 9499, Loss: 0.2497
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35906.05it/s]
Epoch: 9500, Loss: 0.2499, Train: 0.9220, Val: 0.9246, test: 0.9187
Epoch: 9501, Loss: 0.2486
Epoch: 9502, Loss: 0.2454
Epoch: 9503, Loss: 0.2484
Epoch: 9504, Loss: 0.2489
Epoch: 9505, Loss: 0.2533
Epoch: 9506, Loss: 0.2506
Epoch: 9507, Loss: 0.2484
Epoch: 9508, Loss: 0.2459
Epoch: 9509, Loss: 0.2498
Epoch: 9510, Loss: 0.2470
Epoch: 9511, Loss: 0.2503
Epoch: 9512, Loss: 0.2498
Epoch: 9513, Loss: 0.2478
Epoch: 9514, Loss: 0.2476
Epoch: 9515, Loss: 0.2497
Epoch: 9516, Loss: 0.2492
Epoch: 9517, Loss: 0.2494
Epoch: 9518, Loss: 0.2475
Epoch: 9519, Loss: 0.2485
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35667.19it/s]
Epoch: 9520, Loss: 0.2513, Train: 0.9218, Val: 0.9230, test: 0.9168
Epoch: 9521, Loss: 0.2473
Epoch: 9522, Loss: 0.2486
Epoch: 9523, Loss: 0.2485
Epoch: 9524, Loss: 0.2488
Epoch: 9525, Loss: 0.2472
Epoch: 9526, Loss: 0.2459
Epoch: 9527, Loss: 0.2458
Epoch: 9528, Loss: 0.2491
Epoch: 9529, Loss: 0.2501
Epoch: 9530, Loss: 0.2498
Epoch: 9531, Loss: 0.2484
Epoch: 9532, Loss: 0.2489
Epoch: 9533, Loss: 0.2471
Epoch: 9534, Loss: 0.2513
Epoch: 9535, Loss: 0.2505
Epoch: 9536, Loss: 0.2552
Epoch: 9537, Loss: 0.2506
Epoch: 9538, Loss: 0.2525
Epoch: 9539, Loss: 0.2505
Evaluating: 100%|| 64484/64484 [00:01<00:00, 32308.44it/s]
Epoch: 9540, Loss: 0.2490, Train: 0.9220, Val: 0.9240, test: 0.9187
Epoch: 9541, Loss: 0.2509
Epoch: 9542, Loss: 0.2494
Epoch: 9543, Loss: 0.2508
Epoch: 9544, Loss: 0.2448
Epoch: 9545, Loss: 0.2525
Epoch: 9546, Loss: 0.2539
Epoch: 9547, Loss: 0.2527
Epoch: 9548, Loss: 0.2497
Epoch: 9549, Loss: 0.2485
Epoch: 9550, Loss: 0.2468
Epoch: 9551, Loss: 0.2470
Epoch: 9552, Loss: 0.2457
Epoch: 9553, Loss: 0.2509
Epoch: 9554, Loss: 0.2437
Epoch: 9555, Loss: 0.2452
Epoch: 9556, Loss: 0.2457
Epoch: 9557, Loss: 0.2448
Epoch: 9558, Loss: 0.2439
Epoch: 9559, Loss: 0.2443
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35640.22it/s]
Epoch: 9560, Loss: 0.2421, Train: 0.9231, Val: 0.9252, test: 0.9178
Epoch: 9561, Loss: 0.2489
Epoch: 9562, Loss: 0.2453
Epoch: 9563, Loss: 0.2491
Epoch: 9564, Loss: 0.2473
Epoch: 9565, Loss: 0.2463
Epoch: 9566, Loss: 0.2499
Epoch: 9567, Loss: 0.2442
Epoch: 9568, Loss: 0.2452
Epoch: 9569, Loss: 0.2463
Epoch: 9570, Loss: 0.2475
Epoch: 9571, Loss: 0.2436
Epoch: 9572, Loss: 0.2459
Epoch: 9573, Loss: 0.2484
Epoch: 9574, Loss: 0.2520
Epoch: 9575, Loss: 0.2478
Epoch: 9576, Loss: 0.2448
Epoch: 9577, Loss: 0.2479
Epoch: 9578, Loss: 0.2451
Epoch: 9579, Loss: 0.2503
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35484.11it/s]
Epoch: 9580, Loss: 0.2470, Train: 0.9224, Val: 0.9240, test: 0.9175
Epoch: 9581, Loss: 0.2457
Epoch: 9582, Loss: 0.2443
Epoch: 9583, Loss: 0.2448
Epoch: 9584, Loss: 0.2463
Epoch: 9585, Loss: 0.2513
Epoch: 9586, Loss: 0.2435
Epoch: 9587, Loss: 0.2520
Epoch: 9588, Loss: 0.2494
Epoch: 9589, Loss: 0.2500
Epoch: 9590, Loss: 0.2478
Epoch: 9591, Loss: 0.2500
Epoch: 9592, Loss: 0.2502
Epoch: 9593, Loss: 0.2474
Epoch: 9594, Loss: 0.2453
Epoch: 9595, Loss: 0.2491
Epoch: 9596, Loss: 0.2463
Epoch: 9597, Loss: 0.2491
Epoch: 9598, Loss: 0.2458
Epoch: 9599, Loss: 0.2453
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35217.12it/s]
Epoch: 9600, Loss: 0.2470, Train: 0.9215, Val: 0.9233, test: 0.9159
Epoch: 9601, Loss: 0.2483
Epoch: 9602, Loss: 0.2466
Epoch: 9603, Loss: 0.2457
Epoch: 9604, Loss: 0.2434
Epoch: 9605, Loss: 0.2490
Epoch: 9606, Loss: 0.2426
Epoch: 9607, Loss: 0.2447
Epoch: 9608, Loss: 0.2442
Epoch: 9609, Loss: 0.2474
Epoch: 9610, Loss: 0.2501
Epoch: 9611, Loss: 0.2468
Epoch: 9612, Loss: 0.2433
Epoch: 9613, Loss: 0.2423
Epoch: 9614, Loss: 0.2420
Epoch: 9615, Loss: 0.2454
Epoch: 9616, Loss: 0.2452
Epoch: 9617, Loss: 0.2480
Epoch: 9618, Loss: 0.2481
Epoch: 9619, Loss: 0.2454
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35878.99it/s]
Epoch: 9620, Loss: 0.2473, Train: 0.9220, Val: 0.9246, test: 0.9175
Epoch: 9621, Loss: 0.2450
Epoch: 9622, Loss: 0.2461
Epoch: 9623, Loss: 0.2474
Epoch: 9624, Loss: 0.2440
Epoch: 9625, Loss: 0.2456
Epoch: 9626, Loss: 0.2456
Epoch: 9627, Loss: 0.2485
Epoch: 9628, Loss: 0.2462
Epoch: 9629, Loss: 0.2456
Epoch: 9630, Loss: 0.2437
Epoch: 9631, Loss: 0.2445
Epoch: 9632, Loss: 0.2467
Epoch: 9633, Loss: 0.2489
Epoch: 9634, Loss: 0.2426
Epoch: 9635, Loss: 0.2452
Epoch: 9636, Loss: 0.2473
Epoch: 9637, Loss: 0.2507
Epoch: 9638, Loss: 0.2478
Epoch: 9639, Loss: 0.2454
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37338.74it/s]
Epoch: 9640, Loss: 0.2530, Train: 0.9224, Val: 0.9233, test: 0.9184
Epoch: 9641, Loss: 0.2555
Epoch: 9642, Loss: 0.2518
Epoch: 9643, Loss: 0.2458
Epoch: 9644, Loss: 0.2526
Epoch: 9645, Loss: 0.2582
Epoch: 9646, Loss: 0.2532
Epoch: 9647, Loss: 0.2527
Epoch: 9648, Loss: 0.2514
Epoch: 9649, Loss: 0.2524
Epoch: 9650, Loss: 0.2506
Epoch: 9651, Loss: 0.2517
Epoch: 9652, Loss: 0.2495
Epoch: 9653, Loss: 0.2491
Epoch: 9654, Loss: 0.2476
Epoch: 9655, Loss: 0.2487
Epoch: 9656, Loss: 0.2492
Epoch: 9657, Loss: 0.2473
Epoch: 9658, Loss: 0.2460
Epoch: 9659, Loss: 0.2494
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36466.70it/s]
Epoch: 9660, Loss: 0.2481, Train: 0.9215, Val: 0.9236, test: 0.9165
Epoch: 9661, Loss: 0.2450
Epoch: 9662, Loss: 0.2487
Epoch: 9663, Loss: 0.2507
Epoch: 9664, Loss: 0.2447
Epoch: 9665, Loss: 0.2496
Epoch: 9666, Loss: 0.2476
Epoch: 9667, Loss: 0.2469
Epoch: 9668, Loss: 0.2465
Epoch: 9669, Loss: 0.2465
Epoch: 9670, Loss: 0.2451
Epoch: 9671, Loss: 0.2473
Epoch: 9672, Loss: 0.2454
Epoch: 9673, Loss: 0.2478
Epoch: 9674, Loss: 0.2442
Epoch: 9675, Loss: 0.2431
Epoch: 9676, Loss: 0.2478
Epoch: 9677, Loss: 0.2479
Epoch: 9678, Loss: 0.2469
Epoch: 9679, Loss: 0.2462
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37618.62it/s]
Epoch: 9680, Loss: 0.2470, Train: 0.9214, Val: 0.9221, test: 0.9184
Epoch: 9681, Loss: 0.2451
Epoch: 9682, Loss: 0.2459
Epoch: 9683, Loss: 0.2447
Epoch: 9684, Loss: 0.2456
Epoch: 9685, Loss: 0.2446
Epoch: 9686, Loss: 0.2455
Epoch: 9687, Loss: 0.2435
Epoch: 9688, Loss: 0.2439
Epoch: 9689, Loss: 0.2454
Epoch: 9690, Loss: 0.2437
Epoch: 9691, Loss: 0.2421
Epoch: 9692, Loss: 0.2466
Epoch: 9693, Loss: 0.2488
Epoch: 9694, Loss: 0.2473
Epoch: 9695, Loss: 0.2459
Epoch: 9696, Loss: 0.2413
Epoch: 9697, Loss: 0.2425
Epoch: 9698, Loss: 0.2440
Epoch: 9699, Loss: 0.2451
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37412.62it/s]
Epoch: 9700, Loss: 0.2459, Train: 0.9207, Val: 0.9215, test: 0.9168
Epoch: 9701, Loss: 0.2494
Epoch: 9702, Loss: 0.2473
Epoch: 9703, Loss: 0.2498
Epoch: 9704, Loss: 0.2459
Epoch: 9705, Loss: 0.2474
Epoch: 9706, Loss: 0.2492
Epoch: 9707, Loss: 0.2475
Epoch: 9708, Loss: 0.2464
Epoch: 9709, Loss: 0.2460
Epoch: 9710, Loss: 0.2497
Epoch: 9711, Loss: 0.2470
Epoch: 9712, Loss: 0.2448
Epoch: 9713, Loss: 0.2474
Epoch: 9714, Loss: 0.2440
Epoch: 9715, Loss: 0.2520
Epoch: 9716, Loss: 0.2492
Epoch: 9717, Loss: 0.2448
Epoch: 9718, Loss: 0.2436
Epoch: 9719, Loss: 0.2446
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36446.39it/s]
Epoch: 9720, Loss: 0.2480, Train: 0.9206, Val: 0.9218, test: 0.9168
Epoch: 9721, Loss: 0.2483
Epoch: 9722, Loss: 0.2481
Epoch: 9723, Loss: 0.2441
Epoch: 9724, Loss: 0.2492
Epoch: 9725, Loss: 0.2456
Epoch: 9726, Loss: 0.2424
Epoch: 9727, Loss: 0.2503
Epoch: 9728, Loss: 0.2461
Epoch: 9729, Loss: 0.2484
Epoch: 9730, Loss: 0.2503
Epoch: 9731, Loss: 0.2511
Epoch: 9732, Loss: 0.2492
Epoch: 9733, Loss: 0.2474
Epoch: 9734, Loss: 0.2496
Epoch: 9735, Loss: 0.2461
Epoch: 9736, Loss: 0.2522
Epoch: 9737, Loss: 0.2486
Epoch: 9738, Loss: 0.2476
Epoch: 9739, Loss: 0.2470
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36763.28it/s]
Epoch: 9740, Loss: 0.2458, Train: 0.9196, Val: 0.9227, test: 0.9168
Epoch: 9741, Loss: 0.2430
Epoch: 9742, Loss: 0.2438
Epoch: 9743, Loss: 0.2451
Epoch: 9744, Loss: 0.2464
Epoch: 9745, Loss: 0.2431
Epoch: 9746, Loss: 0.2460
Epoch: 9747, Loss: 0.2521
Epoch: 9748, Loss: 0.2469
Epoch: 9749, Loss: 0.2510
Epoch: 9750, Loss: 0.2526
Epoch: 9751, Loss: 0.2552
Epoch: 9752, Loss: 0.2503
Epoch: 9753, Loss: 0.2530
Epoch: 9754, Loss: 0.2521
Epoch: 9755, Loss: 0.2531
Epoch: 9756, Loss: 0.2524
Epoch: 9757, Loss: 0.2498
Epoch: 9758, Loss: 0.2518
Epoch: 9759, Loss: 0.2506
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36483.61it/s]
Epoch: 9760, Loss: 0.2530, Train: 0.9210, Val: 0.9224, test: 0.9178
Epoch: 9761, Loss: 0.2527
Epoch: 9762, Loss: 0.2505
Epoch: 9763, Loss: 0.2480
Epoch: 9764, Loss: 0.2494
Epoch: 9765, Loss: 0.2464
Epoch: 9766, Loss: 0.2504
Epoch: 9767, Loss: 0.2442
Epoch: 9768, Loss: 0.2443
Epoch: 9769, Loss: 0.2538
Epoch: 9770, Loss: 0.2452
Epoch: 9771, Loss: 0.2424
Epoch: 9772, Loss: 0.2459
Epoch: 9773, Loss: 0.2483
Epoch: 9774, Loss: 0.2506
Epoch: 9775, Loss: 0.2501
Epoch: 9776, Loss: 0.2484
Epoch: 9777, Loss: 0.2498
Epoch: 9778, Loss: 0.2488
Epoch: 9779, Loss: 0.2501
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37320.59it/s]
Epoch: 9780, Loss: 0.2503, Train: 0.9206, Val: 0.9212, test: 0.9184
Epoch: 9781, Loss: 0.2490
Epoch: 9782, Loss: 0.2471
Epoch: 9783, Loss: 0.2475
Epoch: 9784, Loss: 0.2461
Epoch: 9785, Loss: 0.2465
Epoch: 9786, Loss: 0.2491
Epoch: 9787, Loss: 0.2444
Epoch: 9788, Loss: 0.2460
Epoch: 9789, Loss: 0.2444
Epoch: 9790, Loss: 0.2497
Epoch: 9791, Loss: 0.2432
Epoch: 9792, Loss: 0.2491
Epoch: 9793, Loss: 0.2459
Epoch: 9794, Loss: 0.2440
Epoch: 9795, Loss: 0.2463
Epoch: 9796, Loss: 0.2454
Epoch: 9797, Loss: 0.2429
Epoch: 9798, Loss: 0.2464
Epoch: 9799, Loss: 0.2493
Evaluating: 100%|| 64484/64484 [00:01<00:00, 33932.72it/s]
Epoch: 9800, Loss: 0.2465, Train: 0.9204, Val: 0.9230, test: 0.9159
Epoch: 9801, Loss: 0.2459
Epoch: 9802, Loss: 0.2448
Epoch: 9803, Loss: 0.2473
Epoch: 9804, Loss: 0.2462
Epoch: 9805, Loss: 0.2462
Epoch: 9806, Loss: 0.2474
Epoch: 9807, Loss: 0.2442
Epoch: 9808, Loss: 0.2496
Epoch: 9809, Loss: 0.2474
Epoch: 9810, Loss: 0.2463
Epoch: 9811, Loss: 0.2503
Epoch: 9812, Loss: 0.2461
Epoch: 9813, Loss: 0.2461
Epoch: 9814, Loss: 0.2454
Epoch: 9815, Loss: 0.2489
Epoch: 9816, Loss: 0.2529
Epoch: 9817, Loss: 0.2469
Epoch: 9818, Loss: 0.2489
Epoch: 9819, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35031.46it/s]
Epoch: 9820, Loss: 0.2477, Train: 0.9212, Val: 0.9233, test: 0.9181
Epoch: 9821, Loss: 0.2442
Epoch: 9822, Loss: 0.2514
Epoch: 9823, Loss: 0.2521
Epoch: 9824, Loss: 0.2478
Epoch: 9825, Loss: 0.2452
Epoch: 9826, Loss: 0.2476
Epoch: 9827, Loss: 0.2482
Epoch: 9828, Loss: 0.2479
Epoch: 9829, Loss: 0.2471
Epoch: 9830, Loss: 0.2497
Epoch: 9831, Loss: 0.2455
Epoch: 9832, Loss: 0.2468
Epoch: 9833, Loss: 0.2495
Epoch: 9834, Loss: 0.2492
Epoch: 9835, Loss: 0.2453
Epoch: 9836, Loss: 0.2497
Epoch: 9837, Loss: 0.2465
Epoch: 9838, Loss: 0.2474
Epoch: 9839, Loss: 0.2493
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35958.25it/s]
Epoch: 9840, Loss: 0.2485, Train: 0.9217, Val: 0.9209, test: 0.9193
Epoch: 9841, Loss: 0.2529
Epoch: 9842, Loss: 0.2477
Epoch: 9843, Loss: 0.2462
Epoch: 9844, Loss: 0.2490
Epoch: 9845, Loss: 0.2497
Epoch: 9846, Loss: 0.2487
Epoch: 9847, Loss: 0.2485
Epoch: 9848, Loss: 0.2467
Epoch: 9849, Loss: 0.2495
Epoch: 9850, Loss: 0.2538
Epoch: 9851, Loss: 0.2490
Epoch: 9852, Loss: 0.2441
Epoch: 9853, Loss: 0.2461
Epoch: 9854, Loss: 0.2481
Epoch: 9855, Loss: 0.2448
Epoch: 9856, Loss: 0.2441
Epoch: 9857, Loss: 0.2418
Epoch: 9858, Loss: 0.2436
Epoch: 9859, Loss: 0.2435
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35804.94it/s]
Epoch: 9860, Loss: 0.2444, Train: 0.9214, Val: 0.9221, test: 0.9178
Epoch: 9861, Loss: 0.2426
Epoch: 9862, Loss: 0.2516
Epoch: 9863, Loss: 0.2448
Epoch: 9864, Loss: 0.2455
Epoch: 9865, Loss: 0.2490
Epoch: 9866, Loss: 0.2485
Epoch: 9867, Loss: 0.2471
Epoch: 9868, Loss: 0.2506
Epoch: 9869, Loss: 0.2453
Epoch: 9870, Loss: 0.2468
Epoch: 9871, Loss: 0.2478
Epoch: 9872, Loss: 0.2437
Epoch: 9873, Loss: 0.2436
Epoch: 9874, Loss: 0.2453
Epoch: 9875, Loss: 0.2440
Epoch: 9876, Loss: 0.2469
Epoch: 9877, Loss: 0.2457
Epoch: 9878, Loss: 0.2463
Epoch: 9879, Loss: 0.2466
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36083.62it/s]
Epoch: 9880, Loss: 0.2461, Train: 0.9213, Val: 0.9218, test: 0.9181
Epoch: 9881, Loss: 0.2490
Epoch: 9882, Loss: 0.2461
Epoch: 9883, Loss: 0.2465
Epoch: 9884, Loss: 0.2452
Epoch: 9885, Loss: 0.2429
Epoch: 9886, Loss: 0.2453
Epoch: 9887, Loss: 0.2547
Epoch: 9888, Loss: 0.2435
Epoch: 9889, Loss: 0.2460
Epoch: 9890, Loss: 0.2475
Epoch: 9891, Loss: 0.2482
Epoch: 9892, Loss: 0.2467
Epoch: 9893, Loss: 0.2475
Epoch: 9894, Loss: 0.2489
Epoch: 9895, Loss: 0.2453
Epoch: 9896, Loss: 0.2438
Epoch: 9897, Loss: 0.2467
Epoch: 9898, Loss: 0.2484
Epoch: 9899, Loss: 0.2428
Evaluating: 100%|| 64484/64484 [00:01<00:00, 37631.19it/s]
Epoch: 9900, Loss: 0.2451, Train: 0.9200, Val: 0.9209, test: 0.9175
Epoch: 9901, Loss: 0.2471
Epoch: 9902, Loss: 0.3051
Epoch: 9903, Loss: 0.2408
Epoch: 9904, Loss: 0.2460
Epoch: 9905, Loss: 0.2475
Epoch: 9906, Loss: 0.2484
Epoch: 9907, Loss: 0.2479
Epoch: 9908, Loss: 0.2496
Epoch: 9909, Loss: 0.2486
Epoch: 9910, Loss: 0.2514
Epoch: 9911, Loss: 0.2466
Epoch: 9912, Loss: 0.2457
Epoch: 9913, Loss: 0.2473
Epoch: 9914, Loss: 0.2451
Epoch: 9915, Loss: 0.2464
Epoch: 9916, Loss: 0.2468
Epoch: 9917, Loss: 0.2511
Epoch: 9918, Loss: 0.2456
Epoch: 9919, Loss: 0.2478
Evaluating: 100%|| 64484/64484 [00:02<00:00, 29973.02it/s]
Epoch: 9920, Loss: 0.2472, Train: 0.9203, Val: 0.9215, test: 0.9175
Epoch: 9921, Loss: 0.2447
Epoch: 9922, Loss: 0.2465
Epoch: 9923, Loss: 0.2464
Epoch: 9924, Loss: 0.2514
Epoch: 9925, Loss: 0.2463
Epoch: 9926, Loss: 0.2471
Epoch: 9927, Loss: 0.2504
Epoch: 9928, Loss: 0.2524
Epoch: 9929, Loss: 0.2542
Epoch: 9930, Loss: 0.2535
Epoch: 9931, Loss: 0.2493
Epoch: 9932, Loss: 0.2519
Epoch: 9933, Loss: 0.2486
Epoch: 9934, Loss: 0.2508
Epoch: 9935, Loss: 0.2503
Epoch: 9936, Loss: 0.2492
Epoch: 9937, Loss: 0.2483
Epoch: 9938, Loss: 0.2509
Epoch: 9939, Loss: 0.2491
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35712.64it/s]
Epoch: 9940, Loss: 0.2478, Train: 0.9196, Val: 0.9212, test: 0.9149
Epoch: 9941, Loss: 0.2474
Epoch: 9942, Loss: 0.2486
Epoch: 9943, Loss: 0.2442
Epoch: 9944, Loss: 0.2483
Epoch: 9945, Loss: 0.2485
Epoch: 9946, Loss: 0.2467
Epoch: 9947, Loss: 0.2469
Epoch: 9948, Loss: 0.2437
Epoch: 9949, Loss: 0.2480
Epoch: 9950, Loss: 0.2436
Epoch: 9951, Loss: 0.2479
Epoch: 9952, Loss: 0.2463
Epoch: 9953, Loss: 0.2463
Epoch: 9954, Loss: 0.2434
Epoch: 9955, Loss: 0.2454
Epoch: 9956, Loss: 0.2460
Epoch: 9957, Loss: 0.2422
Epoch: 9958, Loss: 0.2476
Epoch: 9959, Loss: 0.2475
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35126.12it/s]
Epoch: 9960, Loss: 0.2425, Train: 0.9213, Val: 0.9209, test: 0.9181
Epoch: 9961, Loss: 0.2452
Epoch: 9962, Loss: 0.2454
Epoch: 9963, Loss: 0.2491
Epoch: 9964, Loss: 0.2463
Epoch: 9965, Loss: 0.2442
Epoch: 9966, Loss: 0.2433
Epoch: 9967, Loss: 0.2455
Epoch: 9968, Loss: 0.2471
Epoch: 9969, Loss: 0.2457
Epoch: 9970, Loss: 0.2449
Epoch: 9971, Loss: 0.2491
Epoch: 9972, Loss: 0.2445
Epoch: 9973, Loss: 0.2446
Epoch: 9974, Loss: 0.2437
Epoch: 9975, Loss: 0.2428
Epoch: 9976, Loss: 0.2426
Epoch: 9977, Loss: 0.2476
Epoch: 9978, Loss: 0.2434
Epoch: 9979, Loss: 0.2436
Evaluating: 100%|| 64484/64484 [00:01<00:00, 36702.18it/s]
Epoch: 9980, Loss: 0.2480, Train: 0.9219, Val: 0.9249, test: 0.9181
Epoch: 9981, Loss: 0.2440
Epoch: 9982, Loss: 0.2485
Epoch: 9983, Loss: 0.2474
Epoch: 9984, Loss: 0.2479
Epoch: 9985, Loss: 0.2475
Epoch: 9986, Loss: 0.2411
Epoch: 9987, Loss: 0.2499
Epoch: 9988, Loss: 0.2444
Epoch: 9989, Loss: 0.2435
Epoch: 9990, Loss: 0.2437
Epoch: 9991, Loss: 0.2490
Epoch: 9992, Loss: 0.2482
Epoch: 9993, Loss: 0.2428
Epoch: 9994, Loss: 0.2439
Epoch: 9995, Loss: 0.2456
Epoch: 9996, Loss: 0.2485
Epoch: 9997, Loss: 0.2445
Epoch: 9998, Loss: 0.2453
Epoch: 9999, Loss: 0.2446
Evaluating: 100%|| 64484/64484 [00:01<00:00, 35477.19it/s]
Epoch: 10000, Loss: 0.2420, Train: 0.9201, Val: 0.9218, test: 0.9156

real	148m33.500s
user	748m14.097s
sys	6m17.735s


SIGN
(twit_env) jgwohlbier@sdh01:~/.../twit$ time ./sign.py
already dl
Epoch: 001, Train: 0.7358, Val: 0.7412, Test: 0.7385
Epoch: 002, Train: 0.7385, Val: 0.7433, Test: 0.7414
Epoch: 003, Train: 0.8332, Val: 0.8389, Test: 0.8296
Epoch: 004, Train: 0.9027, Val: 0.9022, Test: 0.8974
Epoch: 005, Train: 0.3831, Val: 0.9022, Test: 0.8974
Epoch: 006, Train: 0.9019, Val: 0.9022, Test: 0.8974
Epoch: 007, Train: 0.8930, Val: 0.9022, Test: 0.8974
Epoch: 008, Train: 0.8847, Val: 0.9022, Test: 0.8974
Epoch: 009, Train: 0.8821, Val: 0.9022, Test: 0.8974
Epoch: 010, Train: 0.8824, Val: 0.9022, Test: 0.8974
Epoch: 011, Train: 0.8848, Val: 0.9022, Test: 0.8974
Epoch: 012, Train: 0.8896, Val: 0.9022, Test: 0.8974
Epoch: 013, Train: 0.8953, Val: 0.9022, Test: 0.8974
Epoch: 014, Train: 0.9002, Val: 0.9022, Test: 0.8974
Epoch: 015, Train: 0.9020, Val: 0.9022, Test: 0.8974
Epoch: 016, Train: 0.8972, Val: 0.9022, Test: 0.8974
Epoch: 017, Train: 0.8743, Val: 0.9022, Test: 0.8974
Epoch: 018, Train: 0.8382, Val: 0.9022, Test: 0.8974
Epoch: 019, Train: 0.8319, Val: 0.9022, Test: 0.8974
Epoch: 020, Train: 0.8745, Val: 0.9022, Test: 0.8974
Epoch: 021, Train: 0.9022, Val: 0.9029, Test: 0.8970
Epoch: 022, Train: 0.9042, Val: 0.9044, Test: 0.8986
Epoch: 023, Train: 0.9037, Val: 0.9044, Test: 0.8986
Epoch: 024, Train: 0.9038, Val: 0.9044, Test: 0.8986
Epoch: 025, Train: 0.9042, Val: 0.9044, Test: 0.8986
Epoch: 026, Train: 0.9053, Val: 0.9053, Test: 0.9005
Epoch: 027, Train: 0.9047, Val: 0.9053, Test: 0.9005
Epoch: 028, Train: 0.9040, Val: 0.9053, Test: 0.9005
Epoch: 029, Train: 0.9044, Val: 0.9053, Test: 0.9005
Epoch: 030, Train: 0.9047, Val: 0.9053, Test: 0.9005
Epoch: 031, Train: 0.9047, Val: 0.9053, Test: 0.9005
Epoch: 032, Train: 0.9044, Val: 0.9053, Test: 0.9005
Epoch: 033, Train: 0.9057, Val: 0.9053, Test: 0.9005
Epoch: 034, Train: 0.9062, Val: 0.9060, Test: 0.9005
Epoch: 035, Train: 0.9057, Val: 0.9060, Test: 0.9005
Epoch: 036, Train: 0.9056, Val: 0.9060, Test: 0.9005
Epoch: 037, Train: 0.9058, Val: 0.9060, Test: 0.9005
Epoch: 038, Train: 0.9061, Val: 0.9060, Test: 0.9005
Epoch: 039, Train: 0.9065, Val: 0.9063, Test: 0.9011
Epoch: 040, Train: 0.9073, Val: 0.9069, Test: 0.9014
Epoch: 041, Train: 0.9070, Val: 0.9069, Test: 0.9014
Epoch: 042, Train: 0.9070, Val: 0.9069, Test: 0.9014
Epoch: 043, Train: 0.9077, Val: 0.9072, Test: 0.9018
Epoch: 044, Train: 0.9080, Val: 0.9072, Test: 0.9018
Epoch: 045, Train: 0.9068, Val: 0.9072, Test: 0.9018
Epoch: 046, Train: 0.9065, Val: 0.9072, Test: 0.9018
Epoch: 047, Train: 0.9064, Val: 0.9072, Test: 0.9018
Epoch: 048, Train: 0.9067, Val: 0.9072, Test: 0.9018
Epoch: 049, Train: 0.9069, Val: 0.9072, Test: 0.9018
Epoch: 050, Train: 0.9082, Val: 0.9072, Test: 0.9018
Epoch: 051, Train: 0.9088, Val: 0.9078, Test: 0.9024
Epoch: 052, Train: 0.9088, Val: 0.9078, Test: 0.9024
Epoch: 053, Train: 0.9088, Val: 0.9078, Test: 0.9024
Epoch: 054, Train: 0.9086, Val: 0.9078, Test: 0.9024
Epoch: 055, Train: 0.9078, Val: 0.9078, Test: 0.9024
Epoch: 056, Train: 0.9072, Val: 0.9078, Test: 0.9024
Epoch: 057, Train: 0.9078, Val: 0.9078, Test: 0.9024
Epoch: 058, Train: 0.9079, Val: 0.9078, Test: 0.9024
Epoch: 059, Train: 0.9085, Val: 0.9078, Test: 0.9024
Epoch: 060, Train: 0.9093, Val: 0.9078, Test: 0.9024
Epoch: 061, Train: 0.9092, Val: 0.9078, Test: 0.9024
Epoch: 062, Train: 0.9094, Val: 0.9084, Test: 0.9021
Epoch: 063, Train: 0.9095, Val: 0.9084, Test: 0.9021
Epoch: 064, Train: 0.9101, Val: 0.9084, Test: 0.9021
Epoch: 065, Train: 0.9097, Val: 0.9084, Test: 0.9021
Epoch: 066, Train: 0.9093, Val: 0.9084, Test: 0.9021
Epoch: 067, Train: 0.9091, Val: 0.9084, Test: 0.9021
Epoch: 068, Train: 0.9093, Val: 0.9084, Test: 0.9021
Epoch: 069, Train: 0.9098, Val: 0.9084, Test: 0.9021
Epoch: 070, Train: 0.9102, Val: 0.9084, Test: 0.9021
Epoch: 071, Train: 0.9106, Val: 0.9088, Test: 0.9027
Epoch: 072, Train: 0.9108, Val: 0.9091, Test: 0.9030
Epoch: 073, Train: 0.9106, Val: 0.9091, Test: 0.9030
Epoch: 074, Train: 0.9106, Val: 0.9091, Test: 0.9030
Epoch: 075, Train: 0.9104, Val: 0.9091, Test: 0.9030
Epoch: 076, Train: 0.9104, Val: 0.9091, Test: 0.9030
Epoch: 077, Train: 0.9104, Val: 0.9091, Test: 0.9030
Epoch: 078, Train: 0.9106, Val: 0.9091, Test: 0.9030
Epoch: 079, Train: 0.9107, Val: 0.9094, Test: 0.9030
Epoch: 080, Train: 0.9110, Val: 0.9094, Test: 0.9030
Epoch: 081, Train: 0.9111, Val: 0.9094, Test: 0.9030
Epoch: 082, Train: 0.9110, Val: 0.9094, Test: 0.9030
Epoch: 083, Train: 0.9109, Val: 0.9094, Test: 0.9030
Epoch: 084, Train: 0.9108, Val: 0.9094, Test: 0.9030
Epoch: 085, Train: 0.9109, Val: 0.9094, Test: 0.9030
Epoch: 086, Train: 0.9110, Val: 0.9094, Test: 0.9030
Epoch: 087, Train: 0.9112, Val: 0.9094, Test: 0.9030
Epoch: 088, Train: 0.9113, Val: 0.9094, Test: 0.9030
Epoch: 089, Train: 0.9112, Val: 0.9094, Test: 0.9030
Epoch: 090, Train: 0.9114, Val: 0.9094, Test: 0.9030
Epoch: 091, Train: 0.9112, Val: 0.9094, Test: 0.9030
Epoch: 092, Train: 0.9111, Val: 0.9094, Test: 0.9030
Epoch: 093, Train: 0.9111, Val: 0.9094, Test: 0.9030
Epoch: 094, Train: 0.9115, Val: 0.9094, Test: 0.9030
Epoch: 095, Train: 0.9116, Val: 0.9094, Test: 0.9030
Epoch: 096, Train: 0.9116, Val: 0.9094, Test: 0.9030
Epoch: 097, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 098, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 099, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 100, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 101, Train: 0.9118, Val: 0.9094, Test: 0.9030
Epoch: 102, Train: 0.9118, Val: 0.9094, Test: 0.9030
Epoch: 103, Train: 0.9118, Val: 0.9094, Test: 0.9030
Epoch: 104, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 105, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 106, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 107, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 108, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 109, Train: 0.9117, Val: 0.9094, Test: 0.9030
Epoch: 110, Train: 0.9119, Val: 0.9094, Test: 0.9030
Epoch: 111, Train: 0.9121, Val: 0.9094, Test: 0.9030
Epoch: 112, Train: 0.9121, Val: 0.9094, Test: 0.9030
Epoch: 113, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 114, Train: 0.9121, Val: 0.9094, Test: 0.9030
Epoch: 115, Train: 0.9121, Val: 0.9094, Test: 0.9030
Epoch: 116, Train: 0.9121, Val: 0.9094, Test: 0.9030
Epoch: 117, Train: 0.9123, Val: 0.9094, Test: 0.9030
Epoch: 118, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 119, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 120, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 121, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 122, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 123, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 124, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 125, Train: 0.9123, Val: 0.9094, Test: 0.9030
Epoch: 126, Train: 0.9123, Val: 0.9094, Test: 0.9030
Epoch: 127, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 128, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 129, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 130, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 131, Train: 0.9122, Val: 0.9094, Test: 0.9030
Epoch: 132, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 133, Train: 0.9123, Val: 0.9094, Test: 0.9030
Epoch: 134, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 135, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 136, Train: 0.9124, Val: 0.9094, Test: 0.9030
Epoch: 137, Train: 0.9125, Val: 0.9094, Test: 0.9030
Epoch: 138, Train: 0.9125, Val: 0.9094, Test: 0.9030
Epoch: 139, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 140, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 141, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 142, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 143, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 144, Train: 0.9127, Val: 0.9094, Test: 0.9030
Epoch: 145, Train: 0.9129, Val: 0.9094, Test: 0.9030
Epoch: 146, Train: 0.9129, Val: 0.9094, Test: 0.9030
Epoch: 147, Train: 0.9130, Val: 0.9097, Test: 0.9087
Epoch: 148, Train: 0.9130, Val: 0.9097, Test: 0.9087
Epoch: 149, Train: 0.9129, Val: 0.9097, Test: 0.9087
Epoch: 150, Train: 0.9128, Val: 0.9097, Test: 0.9087
Epoch: 151, Train: 0.9129, Val: 0.9097, Test: 0.9087
Epoch: 152, Train: 0.9129, Val: 0.9097, Test: 0.9087
Epoch: 153, Train: 0.9133, Val: 0.9097, Test: 0.9087
Epoch: 154, Train: 0.9130, Val: 0.9097, Test: 0.9087
Epoch: 155, Train: 0.9130, Val: 0.9097, Test: 0.9087
Epoch: 156, Train: 0.9129, Val: 0.9097, Test: 0.9087
Epoch: 157, Train: 0.9130, Val: 0.9097, Test: 0.9087
Epoch: 158, Train: 0.9131, Val: 0.9097, Test: 0.9087
Epoch: 159, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 160, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 161, Train: 0.9131, Val: 0.9097, Test: 0.9087
Epoch: 162, Train: 0.9131, Val: 0.9097, Test: 0.9087
Epoch: 163, Train: 0.9133, Val: 0.9097, Test: 0.9087
Epoch: 164, Train: 0.9133, Val: 0.9097, Test: 0.9087
Epoch: 165, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 166, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 167, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 168, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 169, Train: 0.9134, Val: 0.9097, Test: 0.9087
Epoch: 170, Train: 0.9135, Val: 0.9097, Test: 0.9087
Epoch: 171, Train: 0.9136, Val: 0.9097, Test: 0.9087
Epoch: 172, Train: 0.9137, Val: 0.9097, Test: 0.9087
Epoch: 173, Train: 0.9137, Val: 0.9097, Test: 0.9087
Epoch: 174, Train: 0.9135, Val: 0.9097, Test: 0.9087
Epoch: 175, Train: 0.9136, Val: 0.9097, Test: 0.9087
Epoch: 176, Train: 0.9136, Val: 0.9097, Test: 0.9087
Epoch: 177, Train: 0.9137, Val: 0.9097, Test: 0.9087
Epoch: 178, Train: 0.9136, Val: 0.9097, Test: 0.9087
Epoch: 179, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 180, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 181, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 182, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 183, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 184, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 185, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 186, Train: 0.9131, Val: 0.9100, Test: 0.9090
Epoch: 187, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 188, Train: 0.9135, Val: 0.9100, Test: 0.9090
Epoch: 189, Train: 0.9136, Val: 0.9100, Test: 0.9090
Epoch: 190, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 191, Train: 0.9134, Val: 0.9100, Test: 0.9090
Epoch: 192, Train: 0.9137, Val: 0.9100, Test: 0.9090
Epoch: 193, Train: 0.9139, Val: 0.9100, Test: 0.9090
Epoch: 194, Train: 0.9139, Val: 0.9100, Test: 0.9090
Epoch: 195, Train: 0.9138, Val: 0.9100, Test: 0.9090
Epoch: 196, Train: 0.9139, Val: 0.9100, Test: 0.9090
Epoch: 197, Train: 0.9138, Val: 0.9100, Test: 0.9090
Epoch: 198, Train: 0.9139, Val: 0.9100, Test: 0.9090
Epoch: 199, Train: 0.9138, Val: 0.9100, Test: 0.9090
Epoch: 200, Train: 0.9138, Val: 0.9100, Test: 0.9090

real	3m29.571s
user	58m28.257s
sys	50m51.805s
